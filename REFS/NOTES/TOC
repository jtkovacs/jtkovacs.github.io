
# MS Access objects

An Access database is made up of different objects:

- **Tables:**
    - Autosaves when inputting records; need to save changes to DB structure though
    - Database Tools > Relationships; drag & drop interface
    - **Control input:**
        - Check “Enforce referential integrity”, “Cascade update” and “Cascade delete” to prevent orphan records
        - Can make a field required
        - Specify formatting: Design View > choose field > Input Mask > e.g. (xxx) xxx-xxxx
        - Default value: Design View > choose field > Default Value > … > =”United States”
        - Design View > choose field > Validation Rule > … > =Date(), =”MasterCard” OR “Visa”
        - Design View > choose field > Validation Text > The credit card you’ve entered has expired!
- **Queries:**
    - **Different types:** Create > Query Design > Totals, Select, Update, Make Table, Delete, Append, Crosstab > Run
        - Deletion: Query Design > join Table and Query affecting Table > Property Sheet > set Unique Records to Yes > Delete > Run
        - Append is a permanent combination of two tables; union is a temporary combination
        - Crosstab is similar to totals, but presents the results in matrix form
    - Choose join type: double click on UML associations in Query Design view
- **Forms** give a user-friendly interface for tables and queries:
    - Choose table > Create > Form (or Form Wizard, or Form Design)
    - Basing forms on queries (rather than tables) makes future modifications easier 
    - Dashboard: Create > Navigation; drag & drop forms onto this page
    - [Tips for designing Access Forms](http://www.opengatesw.net/ms-access-tutorials/Access-Articles/MS-Access-Forms.htm)
    - Add a combo box to the header of a form for easier navigation (e.g. by employee name)
- **Reports,** unlike forms, are printer-friendly:
    - Create > Report (or Report Design or Report Wizard)
    - Design > Group & Sort, Totals
    - Format > Conditional Formatting
- **Macros** automate operations, making forms more capable:
    - Create > Macro
    - Button macros: Design > create a button --> Property Sheet > Events > On Click > … > ApplyFilter
    - Remove all filters: ApplyFilter > Where Condition = true
    - Data macros: use to make logs that capture editing. create a log table; Table > After Update > … 
    - Name macro autoexec to run it on launch
- **Modules** are for Visual Basic code.

Objects might not be immediately visible; right click on “All Access Objects”, choose “Navigation Options” from the resulting menu, and check “Show Hidden Objects”.

# Data types

- Short text, long text
    - Store numbers as text if you don’t need to manipulate them mathematically, e.g. phone numbers
- Autonumber/keys 
- Int
- Float 
- Binary
- Attachment 
- Hyperlink 
- Calculated field
- Currency
- Lookup Wizard: Design View > choose field > Data Type > Lookup Wizard
    - Lookup list is dynamic, based on a query --> leave the “Hide key column” check box selected
    - Lookup value list is static, good for a small number of relatively fixed values



    
    
    
    
    
    
# Administration

Optimize: 

- Database Tools > Access Database (splits database into front-end=queries+forms+reports and back-end=tables)
- Design View > choose field > Field Size
- Indexing

Troubleshooting: 

- External Data > Linked Table Manager
- Database Tools > Compact & Repair (run periodically)

Get information:

- Database Tools > Object Dependencies
- Database Tools > Database Documenter 



# Improving navigability

- Create custom groups to replace the default Tables/Forms/Queries in the lefthand navigation pane
- Design View of tables/queries/etc. lets you add descriptions and tool tips for fields
- Build a custom ribbon with frequently-used commands & macros: Options > Customize Ribbon
- Define startup actions, default views, and limited views: Options > Current Database
    - Name macro autoexec to run it on launch
    - Bypass by holding shift key while opening database

    
    
    
    
# SQL dialect novelties

- Date delimiter: BETWEEN #00/00/0000# AND #00/00/0000#



# Sharing an Access database

- [https://support.office.com/en-us/article/Ways-to-share-an-Access-desktop-database-03822632-da43-4d8f-ba2a-68da245a0446](https://support.office.com/en-us/article/Ways-to-share-an-Access-desktop-database-03822632-da43-4d8f-ba2a-68da245a0446)
- [How to Make MS Access Database Executable](https://www.youtube.com/watch?v=aylQNvVDPsg)

## Splitting a database

Why split a database? Database performance can be improved by splitting the database: tables in one file, all other objects (like queries, forms, and reports) in another. 

- [http://www.techrepublic.com/blog/10-things/10-plus-reasons-to-split-an-access-database/](http://www.techrepublic.com/blog/10-things/10-plus-reasons-to-split-an-access-database/)
- [https://support.office.com/en-us/article/Split-an-Access-database-3015ad18-a3a1-4e9c-a7f3-51b1d73498cc?ui=en-US&rs=en-US&ad=US](https://support.office.com/en-us/article/Split-an-Access-database-3015ad18-a3a1-4e9c-a7f3-51b1d73498cc?ui=en-US&rs=en-US&ad=US)

How? Database Tools > Access Database > Database Splitter wizard. 

- Naming conventions: for mydb.accdb, the backend can be named mydb_be.accdb or mybd_datafile.accdb
- Tables in mydatabase.accdb are linked tables
- Backend goes on server
- What about .mdb?? 
    - https://support.microsoft.com/en-us/kb/118609
    - https://support.office.com/en-us/article/Which-Access-file-format-should-I-use-012d9ab3-d14c-479e-b617-be66f9070b41
- Prepare frontend
  - Consider compiling the frontend file to limit functionality: Database Tools > Make ACCDE > Save as mydb.accde or mydd.mde
  - Hide menus: Access Options > deselect Display Navigation Pane
- Prepare backend
  - Backup plan?
  - https://support.microsoft.com/en-us/kb/162522: "When you distribute your application, instruct a network or system administrator to run the Setup program for your back-end database on the network file server first ... After Setup install the appropriate files on the file server, instruct all users to run the setup program for your front-end database. This setup program sets up your main application and all of the Microsoft Access run-time files on each user's computer."

## Migrating to Microsoft SQL Server

- [Migrating Access Databases to SQL Server](https://www.lynda.com/Access-tutorials/Migrating-Access-Databases-SQL-Server/397389-2.html)
  
  
  
  
  

 




# Sources

## References

- [http://allenbrowne.com/tips.html](http://allenbrowne.com/tips.html)
# Bash basics


## What is Bash?

Through a (virtual) terminal you can send commands to the shell, which passes commands along to the OS. You could also write scripts for the shell to run. The terminology around this is muddled because of the way computer architecture has evolved and because the same words are used differently across operating systems and computing subfields ([1](http://stackoverflow.com/questions/21464073/shells-vs-command-interpreters-vs-command-line), [2](http://askubuntu.com/questions/506510/what-is-the-difference-between-terminal-console-shell-and-command-line#comment683259_506510), [3](http://askubuntu.com/questions/506510/what-is-the-difference-between-terminal-console-shell-and-command-line), [4](http://stackoverflow.com/questions/21014344/terminal-or-console-or-shell-or-command-prompt), [5](http://linuxcommand.org/lts0010.php)): 

- A __terminal__ is any environment for text I/O. __Console__ is an old word for a physical terminal. Many terminals are now [virtual](http://askubuntu.com/questions/14284/why-is-a-virtual-terminal-virtual-and-what-why-where-is-the-real-terminal), also called terminal emulators. In Unix, "everything is a file"; a terminal is controlled by a [tty device file](http://www.linusakesson.net/programming/tty/index.php).
- A __shell__ is a very general term meaning any program that controls and runs other programs; however, a command line shell (also called a __CLI__, command line interface) is often called 'the shell' for short.  The term comes from [Unix architecture](https://en.wikipedia.org/wiki/Unix_architecture): shell wraps around kernel which wraps around hardware. 
- __Bash__ is the default CLI for Linux and Mac. Windows has Command Prompt; to get Bash for Windows, install [Cygwin](https://cygwin.com/install.html).

In addition to commands that are typed and entered, Bash responds to hotkeys (`^` means `ctrl`): 

```Bash
# tab to autocomplete
# ^L to clear the screen
# ^d sends EOF
# ^c interrupts/quits
# ^k to kill (cut)
# ^y to yank (paste)
# q to quit (sometimes)
```

## Set up Bash

Bash lets you control your environment by adding messages, defining aliases, setting the values of environment variables, etc. 

- Defining an alias means giving a new name to an existing command or giving a name to a whole sequence of existing commands. Define aliases by adding them to `.bash_profile` or, to make them persist across sessions, to `.bashrc`.
- Environment variables are used across commands, sessions, and programs:
  - `$HOME`: path of home directory
  - `$PATH`: directories containing command line scripts
  - `$PS1`: defines style of command prompt

```Bash
nano ~/.bash_profile  # create/edit preferences file
source ~/.bash_profile  # activates newly edited profile for current session

echo message_string  # add new session greeting message to .bash_profile

alias  # displays all current aliases 
alias pd="pwd"  # rename an existing command
alias nwnano = "touch file1 && nano file2"  # make new command from sequence of existing commands
unalias pd  # removes alias

env  # view all environment variables
env | grep REGEX  # filter and view environment variables
export USER = "user_name"  # type in .bash_profile to create environment variable
echo $USER  # print environment variable
```

## Manage system

```Bash
df  # check space on disk, in KB
df -h  # " in MB/GB
watch -n 5 [command]  # repeat a command or script even 5 seconds
watch -n 10 free -m  # show memory usage in MB every 10 seconds

top  # list top processes
ps aux  # list all running processes
ps aux | grep regex_pattern  # search running processes
kill processID  # get PID from ps aux's output
kill -9 processID  # kill process immediately

sudo -i #   pushes terminal into root until you type “exit”

sudo apt-get update  # fetch list of available packages
sudo apt-get upgrade  # updates packages according to apt-get update's list
sudo apt-get dist-upgrade  # like apt-get upgrade, but handles dependencies better

# analyze boot speed
cat /proc/sys/vm/swappiness
systemd-analyze-blame
Df -h, checks disk space and partition usage
```

### Get more information
 
```Bash
lsb_release -a  # Linux version
nano /etc/issue  # Linux version
uname -a  # information about system: Linux or Unix, 32- or 64-bit, etc.
hostname  # computer's network name
groups  # list of all groups your username is in
host domain  # DNS lookup on URL, IPv4, IPv6

history  # view commands history
!hnum  # re-run a command using its history number
history -c  # clear history

help command_name
type command_name
which command_name
file path/command_name
man command_name  # documentation; type 'q' to exit
man -k searchstring  # search in mans' short descriptions for searchstring
apropos command_name  # find appropriate man page 
whatis command_name  # one-line man page summary
# ^R to 'reverse-i-search' the command history
```

## Navigate filesystem

The __Linux filesystem__ is a tree of directories (folders) containing files. Unlike Windows, the Linux filesystem has a single root directory. Typically, in addition to users' home directories, root will contain these directories:

- __bin__: binary files/shell scripts available to all programs
- __opt__: commercial software installations
- __temp, var__: files that change a lot or are frequently deleted, e.g. logs

__Directory and file names__ in Linux are case sensitive, but Linux does not require you to specify file extensions. Names can include any character except /, although to do so some characters (`$, <, >, &, |, ;, ", ', \`) must be escaped `\` or contained by quotes. A directory or file will be hidden if its name begins with a period. 

In addition to a name, every directory and file has an __address__ (or path) that can be given in absolute terms (starting from the root: `/dir1/dir2/filename`) or in relative terms (starting from the current location: `dir2/filename`, if `dir1` is your current location). When specifying an address, there are shortcut symbols denoting different parts of the filesystem:

```Bash
pwd  # print working directory (current location)

cd path/directory_name  # move to a new directory
cd ../../dirname  # go two directories up, then down
# /, the root
# ~, the user's home directory
# ~username, a specified user's home directory
# ., the current directory
# .., the current directory's parent directory 
# –, the prior working directory

ls  # list files in the working directory 
ls -a  # list hidden files too
ls -t  # list by time-last-modified instead of alphabetically
ls -r  # list content in reverse order
ls -art  # options can be chained; equivalently, ls -a -l -t

ls -R  # list all content in working directory, including content in subdirectories 
find  # list all files contained in the working directory and its subdirectories; same as ls -aR
```

### Manipulate filesystem

Based on [Bruce Barnett's summary](http://www.grymoire.com/Unix/Inodes.html), Unix/Linux files consist of data and inode metadata; and a Unix/Linux directory is a table associating file names with inodes. Try to visualize the operations below in these terms; for example (based on a [response in an Arch Linux forum](https://bbs.archlinux.org/viewtopic.php?id=53484)), consider the difference between

- __copying a file__: you have two files with different inodes; if you edit or delete one file, the other is unchanged
- __creating a hard link to a file__: you have one file with one inode with multiple names; if you edit under one name, the file changes for them all, but if you delete one, the others are unaffected
- __creating a soft link (or shortcut, or symbolic link, or pointer) to a file__: you have one file or directory that has one inode, but you can get to it via shortcuts which have their own inodes; if you delete the file, the shortcuts break, but if you delete the shortcuts, the file is fine

```Bash
mkdir dir_name  # create directory
touch ~/dirname/file.txt  # create file
curl http://url  # downloads file from URL
unzip file.zip

cp fromfile tofile  # move contents of one file into another; will overwrite existing tofile
cp file1 file2 todir  # copy two files to specified directory
cp bash_pattern todir  # copy all files matching pattern to directory
cp -R fromdir1 todir2  # copy dir1 into dir2

ls -i  # show inode #s as well as names
ln file hardlink  # create hard link
ln -s file softlink  # create soft link

mv file1 file2 todir  # move files or directories to directory
mv -i file todir  # adds confirmation step to prevent overwrites
mv filename new_filename  # rename a file or directory

rm file1 file2  # delete file/s 
rmdir dirname  # deletes directory IF empty
rm -R dirname  # delete directory and its contents
```

## Manage permissions

[A detailed view of a directory's contents](http://linuxsurvival.com/linux-file-security-permissions-part-2/) includes, for each item in this order, the item's permissions; the number of hard links to it; its owner; its group; its size (as number of bytes); the date and time of its last modification; and its name. [Permission are expressed in a 9 digit-long code](http://linuxsurvival.com/linux-file-security-permissions-part-3/), where the first three digits encode the owner's permissions; the next three digits encode the owner's group's permissions; and the last three encode everyone else's. Then, each three digit cluster encodes read-write-execute permissions as yes (`r`, `w`, `x`) or no (`-`). Examples: `rwx------`, `rw-rw-r--`, `rwxrwxr-x`.

```Bash
ls -l  # list in long form
chmod ugo+rwx dirname  # give user, group, and others read, write and execute permission
chmod o-w filename  # revoke others' write permission
chown <new_username>:<new_groupname> filename  # change ownership
```

## Chain and redirect

Rather than typing and entering one at a time, commands can be **chained**: entered as a sequence separated by `;` or `&&`, then executed. In the semicolon method, if a command fails, Bash still tries to execute commands that follow it. In the ampersand method, if a command fails then the remaining ones simply aren't run. Chaining is meant to be a time-saver.

[**Redirection** goes beyond chaining](http://linuxcommand.org/lc3_lts0070.php). Each command has standard input (`stdin`), standard error (`stderr`), and standard output (`stdout`); by way of special characters (`>, >>, <, |`), any of these can be passed to and used by another command, file, or program. This means we don't need to create variables for storing this data.

```Bash
cat > file  # create file by rdct cat's stdin to file; exit with ctrl+d
cat >> file  # create file by rdct cat's stdin, appends to file (if exists)
echo "hello" > file  # create file by rdct echo's stdout from terminal to file

cat file1 > file2  # copy file by rdct cat's stdout from terminal to file2

cat < file  # display file by rdct to cat's stdin; same as >> cat file
cat file | less  # display large files by rdct cat's stdout to less's stdin
```



# Bash recipes

## Write, read, print

```Bash
touch ~/dirname/file  # create file, or update its modification timestamp if file already exists
nano file  # open file in text editor
# Hotkeys for nano:
# ^a jumps to beginning of line
# ^e jumps to end of line

head file  # show first 10 lines of file
tail -n 20 file  # show last 20 lines of file
more file  # open long file in special reading view
less file  # open long file in special reading view
# Hotkeys for less:
# D to go forward one page
# U to go up one page
# > to go to end of document
# < to go to top of document
# /regex to search, then n and N to page up and down through results
# q to quit

cat file  # show file contents on screen 
cat file1 file2  # shows concatenated files
tac file  # show content of file in reverse order
clear  # clear the screen by printing many blank lines

lpr file  # send file to default printer
lpr -P printer file  # send file to specified printer
lpq  # display print queue
lprm jobnum  # delete specified job from print queue
lprm -  # delete all jobs from print queue, subject to user permissions
```

## Find and regex

```Bash
find ... # http://ss64.com/bash/find.html, http://do.co/2hrRPfN

# Some Git commands (grep, less, sed, awk, etc.) take regex expressions:
# http://tldp.org/LDP/Bash-Beginners-Guide/html/chap_04.html
# http://www.regular-expressions.info/reference.html
# http://www.regular-expressions.info/refquick.html
grep REGEX file.txt  # searches for regex pattern in filenames
grep -i REGEX file.txt  # option makes grep case-insensitive
grep -R REGEX path/dir/  # searches directory and outputs filename:line of hits
grep -Rl REGEX path/dir/  # searches directory and outputs filename of hits

# In general (ls, cp, etc.), Git uses its own syntaxes for pattern matching and globbing:
# http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_04_03.html
# http://wiki.bash-hackers.org/syntax/pattern
# http://wiki.bash-hackers.org/syntax/expansion/globs
ls \*{jpg,png}  # list anything ending with either 'jpg' or 'png'
cp \*html dirname  # copy anything ending with 'html' to specified directory
cp [abd]\*.txt dirname  # copy all files beginning with 'a', 'b', or 'c' and ending with '.txt'
cp [!0-9]\*.txt dirname  # copy all files  ending with '.txt', NOT beginning with a number between 0-9
cp [[:upper:]]\* dirname  # copy all files that begin with a capital letter; 
# see also [[:lower:]], [[:digit:]], [[:alpha:]], [[:alnum:]]
cp b?t.txt dirname  # copy all files with exactly one character between 'b' and 't.txt'
cp b??t.txt dirname  # copy all files with exactly two characters between 'b' and 't.txt'
```

## Calculate and analyze

```Bash
cal  # displays month calendar
cal -y  # displays calendar for current year
cal 1995  # displays calendar for 1995
date  # displays UTC date & time

expr 2 + 2  # displays sum; spaces are important
bc  # simple calculator program

wc file  # display text file's #lines, #words, #bytes
wc -L file  # display num_characters in longest line
cat file1 | wc | cat > file2  # create/overwrite file2 with #lines, #words, #bytes of file1

sort file  # sort lines alphabetically
sort -n file  # sort numbers
sort -t: -k4 file  # sort 4th column of semicolon-delineated text file
sort file > new_file  # save sort to file
cat file | sort > sorted-file  # sort items in a file, and save to a new file
sort file1 | uniq > file2  # creates file2, an alphabetized set of file1
# The uniq command filters out adjacent duplicate lines in a file, so always call sort first

sed 's/search_string/replace_string' file.txt  # find and replace 1st occurrence
sed 's/search_string/replace_string/g' file.txt  # find and replace all occurrences
diff file1 file2  # get differences between files
```

## Combine and split files

```Bash
cat file1 file2 file3 > newfile  # creates or overwrites newfile
cat file2 >> file1  # appends file2 to file1

split [options] filename prefix
# options:
# -l linenumber
# -b bytes
``` 


# Sources

## References

- Command line cheatsheets - [Mac](http://ss64.com/osx/), [Linux Bash](http://ss64.com/bash/), [Windows PowerShell](http://ss64.com/ps/)
- Software Carpentry’s [Unix shell cheatsheet](http://swcarpentry.github.io/shell-novice/reference/)
- [Bash keyboard shortcuts for maximum productivity](http://www.skorks.com/2009/09/bash-shortcuts-for-maximum-productivity/)
- [Bash reserved variables](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_03_02.html#table_03_02)

## Read

- [Codecademy - Command Line](https://www.codecademy.com/learn/learn-the-command-line)
- [Udacity - Linux Command Line Basics](https://www.udacity.com/course/linux-command-line-basics--ud595)
- [Udemy - Linux command line basics](https://www.udemy.com/linux-command-line-volume1/)
- [Linux Survival - Command line tutorial](http://linuxsurvival.com/linux-tutorial-introduction/)
- [The importance of command line literacy](http://www.linux-mag.com/id/7096/)

## Unread
- [Data science at the command line](http://datascienceatthecommandline.com/): book, webcast, VM 
- [How to spy on your programs with strace](http://jvns.ca/strace-zine-unfolded.pdf) (pdf)
- [Talks by Julia Evans](http://jvns.ca/talks/)



# BI architectures


# Sources

## References

## Read

- [UW IT - EDW 101](http://itconnect.uw.edu/work/data/training/workshops/#EDW101)
- [MapReduce - Simplified Data Processing on Large Clusters](http://research.google.com/archive/mapreduce.html)
- [Parallel MapReduce in Python in 10 Minutes](https://mikecvet.wordpress.com/2010/07/02/parallel-mapreduce-in-python/)
- [Big Data, n. A kind of black magic](http://www.talyarkoni.org/blog/2014/05/19/big-data-n-a-kind-of-black-magic/)


## Unread

- _The Data Warehouse Lifecycle Toolkit_
- _The Data Warehouse Toolkit_
- _The Data Warehouse ETL Toolkit_
- [Demystifying data warehouses, lakes, and marts](https://www.sisense.com/blog/demystifying-data-warehouses-data-lakes-data-marts/)
- [Difference between database, data warehouse, data lake, and data cube?](https://www.quora.com/What-are-the-differences-between-a-database-data-mart-data-warehouse-a-data-lake-and-a-cube)
- [Data lake versus data warehouse](http://www.kdnuggets.com/2015/09/data-lake-vs-data-warehouse-key-differences.html)
- [No, Hadoop isn't going to replace your data warehouse](http://timoelliott.com/blog/2014/04/no-hadoop-isnt-going-to-replace-your-data-warehouse.html)
- [From data lakes to data swamps](http://timoelliott.com/blog/2014/12/from-data-lakes-to-data-swamps.html)
- [Business 104 - Information Systems and Computer Applications](http://study.com/academy/course/information-systems-and-computer-applications.html)
- [MapReduce](http://wiki.apache.org/hadoop/HadoopMapReduce)
- [Writing a Hadoop MapReduce Job In Python](http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/)
- [What MapReduce can’t do](http://www.analyticbridge.com/profiles/blogs/what-mapreduce-can-t-do)
- [Practical illustration of Map-Reduce (Hadoop-style), on real data](http://www.ap-institute.com/big-data-articles/big-data-what-is-spark-an-explanation-for-anyone.aspx)
- [What is Spark?](http://www.ap-institute.com/big-data-articles/big-data-what-is-spark-an-explanation-for-anyone.aspx)
- [What is Hadoop?](http://www.ap-institute.com/big-data-articles/big-data-what-is-hadoop-%E2%80%93-an-explanation-for-absolutely-anyone.aspx)
- [Spark or Hadoop — Which is the best big data framework?](http://www.forbes.com/sites/bernardmarr/2015/06/22/spark-or-hadoop-which-is-the-best-big-data-framework/#5928d0cd532c)
- [Hadoop Fundamentals](http://www.lynda.com/Hadoop-tutorials/Hadoop-Fundamentals/191942-2.html)
- [AWS](http://www.lynda.com/Amazon-Web-Services-tutorials/Amazon-Web-Services-Data-Services/383048-2.html)
- [Modern Enterprise Data Environment](http://www.lynda.com/Data-management-tutorials/Building-Modern-Enterprise-Data-Roadmap/420016-2.html?srchtrk=index%3a1%0alinktypeid%3a2%0aq%3amodern+enterprise+data+environment%0apage%3a1%0as%3arelevance%0asa%3atrue%0aproducttypeid%3a2)
- [Hadoop Fundamentals](http://bigdatauniversity.com/courses/hadoop-course/)
- [Hadoop Reporting &amp; Analysis](http://bigdatauniversity.com/courses/hadoop-reporting-and-analysis/)
- [Integrated Analytics: Platforms and Principles for Centralizing Your Data](https://drive.google.com/open?id=0B6XYyy1UbJ3XU0psbHFkd1ZCdXc)
- [Big data analysis with Revolution R Enterprise](https://www.datacamp.com/community/open-courses/big-data-revolution-r-enterprise-tutorial#gs.O46fnBg)
- [When data flows faster than it can be processed](http://www.bigdatanews.com/profiles/blogs/when-data-flows-faster-than-it-can-be-processed)
- [Making sense of stream processing](https://www.oreilly.com/learning/making-sense-of-stream-processing)
- [Why Python is slow](https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/)
- [Fast clustering algorithms for massive datasets](http://www.bigdatanews.com/profiles/blogs/fast-clustering-algorithms-for-massive-datasets)
- [Why and how you should build a data dictionary for big datasets](http://www.analyticbridge.com/profiles/blogs/why-and-how-you-should-build-a-data-dictionary-for-big-data-sets)
- [Database key terms](http://www.kdnuggets.com/2016/07/database-key-terms-explained.html)
- [Data governance](https://en.wikipedia.org/wiki/Data_governance)
- [Data steward](https://en.wikipedia.org/wiki/Data_steward)
- [Data custodian](https://en.wikipedia.org/wiki/Data_custodian)
- [Governed data discovery](https://www.betterbuys.com/bi/governed-data-discovery/)
- [In Search of Database Nirvana](https://drive.google.com/open?id=0B6XYyy1UbJ3XeXlnOW11bzVqc2c)
- [Data stack at Slack](https://slack.engineering/data-wrangling-at-slack-f2e0ff633b69?imm_mid=0eb8e0#.wyh01fwh4)
- [Data stack at Blue Apron](https://bytes.blueapron.com/bigquery-delivers-for-blue-apron-9acef1c1b417#.jbicbta5v)
- [What is ETL?](http://www.webopedia.com/TERM/E/ETL.html)


# What is civic tech?

# What is civic education?

# What is open data?

# How does knowledge translate into policy?

# How does mediation apply to public disputes?


# Sources

## References

- [Open Society Foundations (Soros)](https://www.opensocietyfoundations.org/about)
- [Sunlight Foundation](http://sunlightfoundation.com/)
- [Public Accountability Initiative](http://public-accountability.org/) - Little Sis
- [Public Conversations Project](http://www.publicconversations.org/)
- [National Coalition for Dialogue & Deliberation](http://ncdd.org/rc/)
- [GovernmentIsGood.com](http://governmentisgood.com/)
- [Center for Deliberative Democracy](http://cdd.stanford.edu/what-is-deliberative-polling/) - Deliberative polling
- [Training for Change](https://www.trainingforchange.org/workshops)
- [U.S. Institute for Environmental Conflict Resolution](http://www.udall.gov/OurPrograms/Institute/Institute.aspx)

## Read
 
- _[Unearthing seeds of fire: The idea of Highlander](http://www.goodreads.com/book/show/825838.Unearthing_Seeds_of_Fire)_
- [Yotam Marom's "Undoing the politics of powerlessness"](https://medium.com/@YotamMarom/undoing-the-politics-of-powerlessness-72931fee5bda#.b956ibevc) on the necessity of leadership in social movements
- Brian Keegan's [Jupyter notebook](http://nbviewer.jupyter.org/github/brianckeegan/Bechdel/blob/master/Bechdel_test.ipynb) arguing that data journalism needs to adhere to scientific standards of replicability
- Jonathan Stray: [How does a country get to open data?](http://www.niemanlab.org/2013/04/how-does-a-country-get-to-open-data-what-taiwan-can-teach-us-about-the-evolution-of-access/) (short history of open data in the US)
- [The Guardian view on data sharing: the privacy of citizens is being eroded](https://www.theguardian.com/commentisfree/2016/oct/16/the-guardian-view-on-data-sharing-the-privacy-of-citizens-is-being-eroded)
- Podziba, S. (2013). _Civic fusion._
- Carpenter, S. L. & Kennedy, W. D. J. (2001). _Managing public disputes: A practical guide for professionals in government, business and citizen's groups._

## Unread

- [Puget Sound’s best civic tech projects of 2015](http://crosscut.com/2015/12/puget-sounds-best-civic-tech-efforts-in-2015/)
- [How to open up data](http://opendatahandbook.org/guide/en/how-to-open-up-data/)
- [10 ways to democratize the economy](http://www.truth-out.org/opinion/item/18908-what-then-can-i-do-ten-steps-toward-transforming-the-system)
- [The "understandable economics" project](http://www.ecnmy.org/learn/)
- [infed: resources on community and informal learning](http://infed.org/mobi/main-index/)
- [Wiki - Online deliberation](https://en.wikipedia.org/wiki/Online_deliberation)
- [On democratic theories](https://philosophynow.org/issues/101/On_Democratic_Theories)
- [Marshall Ganz on making social movements matter](http://billmoyers.com/segment/marshall-ganz-on-making-social-movements-matter/)
- [How Citizen Mapmakers are Changing the Story of our Cities](http://thisbigcity.net/how-citizen-mapmakers-are-changing-the-story-of-our-cities/)
- [Crowd-Sourcing & Gamification for City 2.0](http://thisbigcity.net/future-city-crowd-sourcing-gamification-city-2-0/)
- [Libraries Struggle to Close the ‘Digital Divide’](http://www.nytimes.com/roomfordebate/2012/12/27/do-we-still-need-libraries/libraries-struggle-to-close-the-digital-divide)
- [Isolated scholars: making bricks, not shaping policy](http://www.chronicle.com/article/Isolated-Scholars-Making/151707/)
- [How economists came to dominate the conversation](http://www.nytimes.com/2015/01/24/upshot/how-economists-came-to-dominate-the-conversation.html?_r=2)
- Schon, D. A. (1995). _Frame reflection: Toward the resolution of intractable policy controversies._


# What is conflict? 

## What are primary drivers of conflict? 

## How do people operate in conflict situations?



# What communication skills help cope with conflict?


# What is the alternative dispute resolution (ADR) spectrum?

## What is the facilitative mediation process?

## What standards govern mediation in Washington State?

## What are major applications of mediation?



# Persuasion

## What makes people change their minds?

## How are people irrational?

## What are major rhetorical appeals?

## What helps people change their behavior?



# Sources

## References

- [http://www.au.af.mil/au/awc/awcgate/va/mediation/speaking.htm](http://www.au.af.mil/au/awc/awcgate/va/mediation/speaking.htm)
- [Lakoff's Cognitive Policy Institute](http://www.cognitivepolicyworks.com/about-us/)
- [Cognitive bias cheat sheet](https://betterhumans.coach.me/cognitive-bias-cheat-sheet-55a472476b18#.n5m7q0xpk)

## Read

Baruch Bush, R. A. & Folger, J. P. (1994). _The promise of mediation: Responding to conflict through empowerment and recognition._

Fisher, R., Ury, W. L., & Patton, B. (1991). _Getting to yes._

## Unread

Schulman, S. (2016). _Conflict is not abuse._

Hoffmann, E. A. (2012). _Co-operative workplace dispute resolution: Organizational structure, ownership, and ideology._

Ellickson, R. (1994). _Order without law: How neighbors settle disputes._

Patterson, K. Grenny, J., McMillan, R, & Switzler, A. (2011). _Crucial conversations: Tools for talking when stakes are high._

- [How to change someone's mind, according to science](https://www.washingtonpost.com/news/wonk/wp/2016/02/10/how-to-change-someones-mind-according-to-science/)
- [Why I defend scoundrels](http://squid314.livejournal.com/333353.html)
- [Persuasion: empirical evidence](https://ideas.repec.org/a/anr/reveco/v2y2010p643-669.html)
- [I asked my mom why she didn't vaccinate me](https://www.buzzfeed.com/virginiahughes/why-my-mom-didnt-vaccinate?utm_term=.trAX57OBd#.wfKMbJ8EV)
- [Study proves that politics and math are incompatible](http://www.salon.com/2013/09/05/study_proves_that_politics_and_math_are_incompatible/)
- [The terrible horrible no good very bad challenge: Changing patient and provider behaviour](http://theincidentaleconomist.com/wordpress/the-terrible-horrible-no-good-very-bad-challenge-changing-patient-and-provider-behaviour/)


# Cheatsheet

- Put Chrombook into recovery mode: `esc + refresh + power`
- Enter dev mode: `ctrl + D`
- [Download crouton](https://goo.gl/fd3zc)
- Launch crosh: `crtl + alt + T`
- Enter shell: `shell`
- View possible targets: `sh ~/Downloads/crouton -t help`
- Launch chroot with name and chosen target: `sudo sh ~/Downloads/crouton -n my_chroot_name -t xfce,xiwi,cli-extra`
    - The `sh` command creates a new shell
    - View current targets: `sudo edit-chroot -l my_chroot_name`
    - Add a target to existing chroot: `sudo sh ~/Downloads/crouton -u -n my_chroot_name -t core`  
        - I installed extension, core, cli-extra, xfce
- Target-specific start commands:
    - Command line: `sudo enter-chroot -n my_chroot_name`
    - XFCE window: `sudo enter-chroot -n my_chroot_name startxfce4`
        - Toggle between Chrome OS and chroot: `crtl + alt + shift + forward/back`
        - Open in browser with [Crouton extension](https://chrome.google.com/webstore/detail/crouton-integration/gcpneefbbnfalgjniomfjknbcgkbijom): xiwi or extension targets must be installed
- Delete the chroot: `sudo delete-chroot my_chroot_name`


# Sources

- [GitHub repo for crouton](https://github.com/dnschneid/crouton)
- [Author's cheatsheet](https://github.com/dnschneid/crouton/wiki/Crouton-Command-Cheat-Sheet)
- From HowToGeek:
  - [How to install](http://www.howtogeek.com/162120/how-to-install-ubuntu-linux-on-your-chromebook-with-crouton/)
  - [How to run in browser](http://www.howtogeek.com/208368/how-to-run-a-full-linux-desktop-in-a-browser-tab-on-your-chromebook/)
  - [How to manage](http://www.howtogeek.com/210047/how-to-manage-the-crouton-linux-system-on-your-chromebook/)

_These notes are heavily influenced by Dr. Simon Wu-Ping Wang's slides as well as Connolly and Begg (2015)._



# What is a database?

A **database** is a either (1) collection of data that's structured according to a **data model** (usually relational, as [discussed below;](#the-relational-data-model) see [notes on information structures](information-architecture.html#information-structures) for other major data models); or (2) this structured data plus a database management system (DBMS). 

A [DBMS](DBMS.html) is either a **database engine** for interacting with the database plus a **database frontend** for user interaction, per definition (1) above; or these two things plus a database, per definition (2) above. A DMBS may be desktop-based (Access, FileMaker Pro) or server-based (SQL Server, Oracle, DB2, MySQL, PostgreSQL). Typical DBMS offer:

- A convenient language with two data sublanguages:
    - Data definition language **(DDL)** expresses the schema; 
    - Data manipulation language **(DML),**
        - enables **CRUD operations** (create, read, update, delete);
        - includes the capacity to query/retrieve/read the data and is therefore often called a query language, though this is technically incorrect (because partial);
        - may be procedural or, as with SQL, nonprocedural/declarative;
- Access to database metadata (includes schema) through the system catalog AKA data directory, data dictionary;
- Support for concurrent access (data sharing);
- Enforcement of access authorization (security);
- Enforcement of constraints on data input (integrity);
- Backup and recovery;
- Views that are customized to the needs of specific audiences.

Finally, a **database system** is a database/DBMS plus database applications (any applications that source from or feed data to the database). This term denotes only the technical environment of a database; the full **database environment** encompasses hardware, software, data, procedures, and people.


## The database system lifecycle

<table>
<tr><th colspan="2">Requirements Analysis</th></tr>
<tr><th rowspan=3>Design</th><td>Conceptual</td></tr>
<tr><td>Logical</td></tr>
<tr><td>Physical</td></tr>
<tr><th rowspan="2">Development</th><td>Implementation</td></tr>
<tr><td>Testing</td></tr>
<tr><th rowspan="2">Administration</th><td>Rollout</td></tr>
<tr><td>Support</td></tr>
</table>



## History of databases

The first computerized information systems (IBM, c. 1950) imitated **hierarchical** paper filing systems. The more semantically powerful **graph** AKA network data model followed shortly thereafter (1960s, also IBM). In early systems, file and data formats were specific to an application or language; applications were specific to a department. This arrangement led to

- ***redundancy:*** the same data collected (\$\$) and stored (\$\$) by multiple departments;
- ***inconsistency:*** redundant data that *should be the same* is not updated consistently;
- ***inaccessibility:*** software developers were needed to write queries and integrate data across applications; no ad hoc queries or data processing.

Hierarchies and graphs were superseded by Edgar Codd's **relational** data model, proven in the 1970s and implemented in the 1980s. A relational database addresses the aforementioned problems because it is ***centralized*** (reducing redundancy, improving consistency, enabling data integration) and ***abstracted*** (available as a black box to interface with many different applications; offering an accessible language for ad hoc queries). It also:

- Scales to terabytes with excellent performance for processing transactions
- Is platform-independent due to ANSI standards, allowing transfer of code across different products
- Is comparatively cheap to purchase and maintain, and widely available
- Provides a [programming/query language](SQL.html) that is easy to learn and execute, expediting data retrieval
- Minimizes data redundancy, conserving storage and safeguarding data quality (although some redundancy is still required to establish relationships)
- Can capture complex relationships (important for enforcing business rules)
- [Preserves data integrity](#integrity)

**Object** databases introduced features like encapsulation and polymorphism c. 1990, but never became popular or standardized. With the advent of Big Data, [NoSQL databases](#NoSQL-databases) (an umbrella term for non-relational database with SQL-like interface) have become popular because they beat relational DBs at quick search; however, relational databases are still better at maintaining data integrity (via transaction management with ACID properties).

### ANSI-SPARC architecture

This is one way of thinking about database abstraction/separation, which, in general, makes the database easier to change and maintain by providing **data independence:**

- External views for different users (i.e., subsets and derivations of the data) are described by subschemas. Views are **logically independent** from the conceptual level.
- The structure of the entire database---all entities, attributes, relationships, and constraints---is described by a conceptual schema. This conceptual layer is **physically independent** from implementation details.
- Implementation details, including storage allocation, compression, and encryption, are described by an internal schema.

Then, the DBMS creates mappings (also called intensions; a realization of a schema is called the extension or state of the database) between schemas.




## Types of databases

### The relational data model

In a relational database, the data model is of **tables** AKA relations, which can be clustered tables or heaps [depending on their indexing.](#indexing-and-performance) Tables have **rows** (AKA tuples, records) and **columns** (AKA attributes, fields). The order of rows and columns is insignificant (unless an [index](#indexing-and-performance) is created).

Representing reality in terms of entities, attributes and relationships occurs during the [conceptual design phase](#conceptual-design) of database development. Per Ullman (2006), **many different relational schemas could be used to model any given reality;** the best designs will suit the underlying business processes and be [in a normal form.](#normalization) 

#### Relationships between entities

The relationship between two entites has several characteristics. The **participation** of a relationship is mandatory or optional; the **cardinality** AKA modality of a relationships may be one-to-one, one-to-many, or many-to-many; see [ERDs](modeling.html#erds-for-databases) for notation. A relationship is **identifying** if the PK of a parent entity appears in the PK of a child entity, denoted with a solid line; nonidentifying relationships are denoted with dashed lines.

##### Associative entities

Many-to-many relationships must be resolved with an associative entity (AKA junction table) that has a combined primary key (PK), both of which are foreign keys (FK). For example, consider a taxi company that owns cars; employs drivers; randomly assigns each driver a car for their shift; and wants to maintain a record for liability purposes. Entities CAR and DRIVER have a many-to-many relationship, since a driver will be assigned to multiple cars over the course of their employment and a car will likewise be driven by many different drivers. To capture the necessary data, SHIFTS is created as an associative entity with attributes driver ID, car ID, and shift date.

##### Superclasses and subclasses

Entities may be classified as superclasses and subclasses; this provides more semantic meaning to an ER model, makes the ER model more readable, and (depending on implementation) can reduce the number of NULLs in the database. There are several implementation options:

- Subclass entity contains superclass PK along with its own unique attributes;
- Subclass entity contains all superclass attributes along with its own unique attributes;
- The ERD includes a superclass and subclass(es) but they are implemented as a single table.


#### Types of attributes

Per Sunderraman (2012) and the Database Management Wikia (n.d.), an attribute is:

- **Composite** if it can be decomposed into **atomic** attributes (which, per first normal form, it should be).
- **Derived** if its value can be calculated from (an)other attribute(s) (which, per third normal form, it shouldn't be).
- **Multi-valued** AKA set-valued if, for a single entity, the attribute could/should store multiple values (one-to-many relationship); in this case, the multi-valued attribute should be moved to a new table and linked back to the main entity via the entity's primary key.  


#### Relationships between attributes

Different sorts of relationships AKA dependencies exist between attributes; this is not a modeling decision, it is a feature of the real world. Dependencies are important for understanding [normalization;](#normalization) normalization is a process of allocating attributes to entities to achieve a certain configuration of dependencies within each entity. Dependencies are also used (somehow?) in DB compression and query optimization.

##### Functional dependencies and keys

A functional dependency `A → B` exists when the same A (for our purposes, an attribute value called the **determinant)** is linked to a single B (another attribute value, called the **dependent).** The reverse is not necessarily true. Note that this is like a mathematical function, where each x, a function input, must correspond to exactly one y, a function output, but a single value of y might correspond to multiple different values of x:

| This is a Function | This is Not a Function | 
| --- | --- |
| ![](../ILLOS/fcn.png) | ![](../ILLOS/not-fcn.png) |

Written in predicate logic with tuples denoted t and u, attributes denoted A and B, a functional dependency exists if, for `∀ t,u ∈ R, t.A = u.A ⇒ t.B = u.B`; this generalizes to multiple attributes, such that a determinant is best (i.e., worst) defined as (an) attribute(s) whose value(s) determine(s) the value(s) of a second (set of) attribute(s). There are a few special cases:

- Trivial: `A → B & B ⊆ A`
- Nontrivial: `A → B & B ⊈ A`
- Completely nontrivial: `A → B & A ∩ B = ∅` 
- Technically transitive dependencies are functional dependencies

The set of attributes that are functionally dependent on a determinant is called the determinant's **closure, ** `{A}*.` A closure can obviously be as small as a single attribute. Per displayName (2015), a determinant whose closure is the *entire table* is a **candidate key** AKA identity value; one candidate key is chosen as the table's sole **primary key (PK).** [This is an excellent demonstration of identifying (super) keys.](http://stackoverflow.com/questions/2718420/candidate-keys-from-functional-dependencie)

In general, keys may be **natural,** i.e. present in the data, or **synthetic** AKA surrogate, automatically generated by the database for internal use. Keys may also be **composite** AKA concatenated, meaning that several attributes taken together (a set) are a determinant whose closure is the entire table. For primary keys specifically, they are most often an integer (the narrowest suitable field); immutable; and mandatory.

##### Transitive dependencies

If a functional dependency exists between X and Y, and a functional dependency exists between Y and Z, then a transitive dependency exists between X and Z: `A → B & B → C ⇒ A → C.` As an example, consider a table (perhaps in a bookstore database) with three attributes: ISBN, TITLE, AUTHOR, PHONE NUMBER. ISBN is the primary key; TITLE and AUTHOR are functionally dependent on it; but PHONE NUMBER is functionally dependent on AUTHOR, not on ISBN. Therefore a transitive dependency exists between PHONE NUMBER and ISBN.

##### Multivalued dependencies

A multivalued dependency `A ↠ B` exists if all tuples share their A attributes; tuple v shares B attributes with t, and its remaining attributes with u; tuple w shares A attributes with u, and its remaining attributes with t. In predicate logic: `if ∀ t,u∈R | t.A = u.A then ∃ v∈R | v.A=t.A and v.B=t.B and v.rest=u.rest.` Furthermore, `∃ w∈R | w.A=t.A and w.B=u.B and w.rest=t.rest`. MVDs matter for 4NF; [examples and details here.](http://infolab.stanford.edu/~ullman/fcdb/aut07/slides/mvds.pdf)


#### Integrity

Data must have integrity to be useful and trustworthy. Data integrity [tends to erode,](wrangling.html#dirty-data-typologies) and a database has mechanisms for maintaining it in its various forms:

- **Entity integrity** is the assurance that entities in the real world are represented in each applicable table of the database by a single record (row). The constraint that rows be unique is enforced through the primary key.     
- **Domain integrity** is the assurance that attributes have meaningful (as in possible, if not necessarily accurate) values. This is enforced by column data types and custom domain restrictions.
- **Referential integrity** is the assurance that, once one-to-many or many-to-many relationships are separated into different tables, the data is still kept in sync despite updates and deletions. Specifically, when a row is added to the latter table, its FK value should come from the former table’s PK or it should be NULL.

![](../ILLOS/integrity.png)

Additionally, database designs are normalized to preserve integrity and minimize redundancy (by limiting storage costs). 

##### Normalization

Normalization is a process of allocating attributes to entities to achieve a certain configuration of [dependencies](#relationships-between-attributes) within each entity; [Bill Kent](http://www.bkent.net/Doc/simple5.htm) does a good job talking about this in terms of which attributes provide "facts" about other attributes. There are five but actually maybe six levels of normalization, with normalization to third normal form the most frequent target. The first normal form is how Codd articulated his relational data model in the 1970s, with the other forms progressive refinements of the basic relational model:

- **1NF:** Rows are unique (i.e., there is a primary key), columns have a datatype, and all attributes are atomic. These requirements reduce redundancy.
- **2NF:** All columns in a table must be related via [FDs;](functional-dependencies-and-keys) i.e., each column must be either a determinant or a dependent. This may require the creation of new entities to resolve one-to-many relationships through PK/FK pairs. If so, modification anomalies are prevented. 
- **3NF:** Remove [TDs](#transitive-dependencies) and derived attributes, preventing update and deletion anomalies.
- **BCNF:** [Extreme version of 3NF](http://psoug.org/reference/normalization.html) where, for all FDs `A → B,` A is the PK.
- **4NF:** Remove [MVDs,](#multivalued-dependencies) somehow increasing efficiency because there are B+C vs. B\*C tuples??
- **5NF:** ???

This [example from ThoughtCo](https://www.thoughtco.com/transitive-dependency-1019760) shows how normal forms prevent data anomalies. In this case there are two FDs `(Book → Author, Author → Author_Nationality)` and one TD `(Book → Author_Nationality),` not to mention a violation of 1NF:

| Author | Book | Author_Nationality | 
| --- | --- | --- |
| Orson Scott Card | Ender's Game | United States | 
| Orson Scott Card | Children of the Mind | United States | 
| Margaret Atwood | The Handmaid's Tale | Canada |

Note the redundancy (caused by the transitive dependency) the liabilities it creates: 

- If you deleted Card's two books, you would remove _him_ as an entity from the database. This is a **deletion anomaly.**
- You must add an author to add a book, and vice versa; this is an **insertion anomaly.**
- If an attribute value changes, you'd need to find and update every occurrence to maintain database accuracy---but you might not. This is an **update anomaly.**

#### Why not normalize?

Per Chapple (2016): 

- Normalization means more tables; more tables mean more [JOINs;](SQL.html) JOINs are slow.
- Normalization is a complex, time-consuming process and developer time is valuable, so operate in the spirit of 'quick and dirty'.




### NoSQL databases

NoSQL databases use non-relational data models ... 

- **Key-value model,** e.g. Dynamo, Riak, Basho: _[Key|Value|Timestamp]._ Provides easy and fast storage for simple data.
- **Columnar model,** e.g. Google’s Bigtable, Apache’s HBase (part of Hadoop): _[Row Key|Value|Timestamp|Column Family|Column Name]._ Good for retaining relationships (since columns can be grouped into families). 
- **Document model,** e.g. MongoDB, JSON, XML. Good for storing complex hierarchical relationships.
- **Graph/triple model,** e.g. Neo4j. Good for capturing a web of relationships.

... plus some of these other features ...

- Open source & less costly hardware
- Distributed storage and processing rather than client/server architecture
- Memory cache
- Batch processing (Google Map Reduce) or interactive AKA stream processing (Apache Tez Framework, Apache Spark, Facebook Presto)
- Proprietary and/or (for Presto, Hive QL, Pig, Cassandra Query Language (CQL), Cosmos/Scope) SQL-like interfaces  
- Analytics integration (Hive, Amazon’s Redshift, Facebook’s Presto, Airbnb’s Airpal)

... to store Big Data ...  

- large **volume** (petabytes rather than terabytes)
- wide **variety** (structured and unstructured)
- high **velocity**

... achieving greater speed by: 

- appending rather than updating records, and 
- denormalizing data upon input. 





# Database development

A good design process minimizes redundancy; reduces errors by automating or imposing constraints on data entry; permits multiple analyses by replacing multipart fields with atomic ones; avoids data conflicts by reserving calculation to the analysis phase, rather than storing results; and ensures complete information by requiring it during input. **Best practices for DB design:**

- Clearly identify scope of database
- Follow a database design methodology, e.g. SDLC, Agile
- Use a professional data modeling tool, e.g. MS Visio
- Use a source control system, e.g. GitHub

Note that databases are often developed in parallel with the applications that will use them. Also, DB development may use CASE (computer-aided software engineering) tools that help with standardization, integration, consistency, and automation. For databases, CASE tools may provide forward engineering (generating database-creating code based on ERD) and reverse engineering (generating ERD from existing database; an efficient way of producing documentation).


## Business requirements

- Conduct a **cost-benefit analysis** for the proposed database;
- Write a **mission statement and objectives;**
- Gather and formulate **business requirements** by analyzing the business processes, documents, workflows, etc. that will be replaced or supported by the database:
    - Within the scope of the objectives, identify key **actors;**
    - Interview actors to understand **tasks** that actors execute;
    - Identify pertinent **business rules,** i.e., database design constraints that arise from the business processes being modeled, not from requirements of the data model.


## Design phases

All stages of design are beholden to the underlying data model. Conceptual design is broader, mostly focused on grouping attributes into tables; logical design is more granular, mostly focused on properties and constraints of each attribute. Lastly, physical design is focused on specifying the database and its interfaces, etc. according to a particular DBMS.

### Conceptual design

In the conceptual design stage of database development, there are two competing approaches: 

- The **top-down approach** (AKA design by decomposition) begins with identifying entities and relationships in the domain to be modeled, then filling in attributes. Entity relationship diagrams are often used. ERDs can be done in [ER or UML notation;](modeling.html#erds-for-databases) MS Visio offers both. 
- The **bottom-up approach** begins with identifying attributes, then grouping them until entities and relationships emerge. Connolly and Begg (2015) suggest that a bottom-up approach is manageable only for smaller databases. For a larger, more complex database, a top-down approach may be necessary so that the database designer doesn’t get overwhelmed by numerous attributes.

Regardless, the end goal is a schema that is [normalized](#normalization) to avoid anomalies. 

In addition to constructing tables via a top-down or bottom-up approach, a conceptual design should:

- Identify relationships verbally;
- Identify cardinality (max#) and optionality (min#) associated with each relationship;
- Identify entity subtypes/subclasses;
- Specify lookup tables;
- Identify [primary keys.](#types-of-attributes)

### Logical design
    
Proceed table by table, field by field:

- Choose naming conventions (avoid special characters and reserved words for your DBMS);
- Choose data types, which vary by DBMS;
    - Store numbers as text if you don’t need to manipulate them mathematically, e.g. phone numbers
- Resolve many-to-many relationships with [associative entities;](#associative-entities)
- Apply [integrity constraints:](#integrity)
    - With a lookup table; 
    - With a referential integrity constraint to prevent orphaned records; 
    - Through a check constraint. 
- Denote required fields.

### Physical design
    
Physical design depends on DBMS-specific features; see [notes on DBMS software.](DBMS.html) The goal of this stage is to provide all the information necessary to build a database that takes advantage of features from the chosen platform.

#### Indexing and performance

##### Why to index

Various kinds of indexes are created to accelerate queries (retrieval of rows from pages) at the expense of write speed `(INSERT, UPDATE, and DELETE operations).` (Per Sheldon (2014), not _all_ indexes improve performance for _all_ queries; more complex queries that involve grouping and sorting can suffer from a clustered index.) Because of this read/write tradeoff, indexes are most useful in [reporting databases versus transactional databases.](information-systems.html#what-are-mis?) Alternatively, an index may be erased when loading a very large dataset into the database, then subsequently restored. 

##### What to index

The PK is indexed by default, and commonly searched fields may be indexed as well; many DBMS offer a **query optimizer** that identifies statistically when indexing would be beneficial. Often indexing a PK/FK pair will improve JOIN performance (and JOINs are very costly).

##### How different types of indexes work

This discussion is based on MS SQL Server, which stores table data (rows) in uniformly-sized pages AKA blocks:

![](../ILLOS/SQLDataPage.png)

A table is either a **heap** or, if it has a clustered index, a **clustered table.** A heap is simply unsorted data pages; the order of its contents (i.e., how its rows are allocated across data pages) will be determined initially by data entry and then by DBMS-initiated changes (for efficiency's sake). A **clustered index,** on the other hand, introduces sorting that is implemented at the level of pages through row offset arrays AKA slot arrays; see Sheffield (2012). For this reason, there can be only one clustered index per table (PK by default).

However, to facilitate specific queries, both heaps and clustered tables may have multiple **non-clustered indexes** that provide alternate sort orders "very much like the index at the end of a book: it occupies its own space, it is highly redundant, and it refers to the actual information stored in a different place"  (Winand, n.d.).  

- When a subset of rows are indexed, this is called a **filtered index.** 
- When multiple fields are included in a single non-clustered index, this is called a **covering index** because it could "cover" all the fields retrieved in a stored query.  

Just as heaps and clustered tables store their rows in data pages, non-clustered indexes store their **leaf nodes** in data pages. Via pointers, [leaves are doubly connected](http://use-the-index-luke.com/sql/anatomy/the-leaf-nodes) to each other (to maintain sort order as rows are added and deleted) and also refer to rows in the heap/clustered table (thereby making the index useful):

![](../ILLOS/nonclustered-index.png)

For heap pages, clustered indexes, and non-clustered indexes alike, a **B-tree** AKA balance tree structure with [root and intermediary nodes](http://use-the-index-luke.com/sql/anatomy/the-tree) is used to make page search more efficient:

<img src="../ILLOS/B-tree.png" style="padding-top: 5px;" width="500px">

Finally, while heaps, clustered indexes, and non-clustered indexes use a rowstore structure (Sheldon, 2013), a **columnstore index** (useful for read-heavy databases with star or snowflake schemas, i.e. BI warehouses) searches only relevant columns: 

![](../ILLOS/columnstore.png)

##### Fragmentation

Index fragmentation is inevitable, especially in OLTP environments:

- INSERT and UPDATE operations lead to page splits **(logical fragmentation?)**
- DELETE operations lead to partially-filled pages **(internal fragmentation)**
- Large rows **(extent fragmentation?)**

Fragmentation can be detected with a DBMS tool, then repaired: 

- Clustered indexes:
    - <1000 pages long or <5% fragmentation, do nothing
    - 5% < logical fragmentation < 30%: **reorganize**
    - 30% < logical fragmentation: **rebuild**
- Extent fragmentation of a heap table (non-indexed) can be reduced by creating then dropping a clustered index

| Characteristic | Reorganize | Rebuild |
| --------------- | ---- | -------- |
| Online or offline | Online | Offline as default; online as option |
| Internal fragmentation | Yes | Yes |
| Logical fragmentation | Yes | Yes |
| Transaction atomicity | Small discrete transactions | Single atomic transaction |
| Rebuild statistics automatically | No | Yes |
| Parallel execution | No | Yes |
| Untangle indexes that are interleaved with the data file | No | Yes |
| Transaction log space used | Less | More |
| Additional free space required in the data file | No | Yes |




# Database administration

## Files and filegroups

A DBMS records _actions_ in its **log file** (.LDF) )and data in its **data file** (holding pages; see [discussion on indexing](#how-different-types-of-indexes-work) and [SQL Server files.)](https://docs.microsoft.com/en-us/sql/relational-databases/databases/database-files-and-filegroups) During backup, the local log file is wiped but the data files are unchanged.

If the main data file (.MDF) exceeds its initially allocated space, there are several options:

- Specify a new size limit
- Specify a growth rate
- Move the data file to a larger drive
- Create multiple data files (.NDF)
    - Create filegroups to manage multiple data files as one object

## Database architectures

![](../ILLOS/db-arch1.png)

![](../ILLOS/db-arch2.png)

![](../ILLOS/db-arch3.png)

![](../ILLOS/db-arch4.png)

## Transaction management

- ACID
- https://www.thoughtco.com/abandoning-acid-in-favor-of-base-1019674

## Security

### Audits

Check database logs to identify security problems, or conduct a more extensive audit. Common security threats may be categorized by human vectors or by system targets:

<table class="bullets">
<tr><th>Users</th> <th>Developers</th> <th>Administrators</th></tr>
<tr>
<td style="text-align: left;">
- Use of person's means of access
- Inappropriate data sharing
- Inadequate training
- Blackmail
- Unwitting conduit for viruses or hacks
</td> 
<td style="text-align: left;">
- Creation of trapdoor
- Staff shortages leading to bad code
- Lack of security training and procedures
</td> 
<td style="text-align: left; width=50px;">
- Inadequate policies and procedures
</td> 
</tr>
</table>

<table class="bullets">
<tr><th>Hardware</th> <th>Networks</th> <th>DBMS & Applications</th> <th>Database</th></tr>
<tr>
<td style="text-align: left;">
- Fire, flood, bombs
- Power loss or surge
- Electromagnetic interference and radiation
- Hardware failure of security mechanisms
- Equipment theft
</td> 
<td style="text-align: left;">
- Wire tapping
- Cable breakage or disconnection
- Electromagnetic interference and radiation
</td> 
<td style="text-align: left;">
- Software failure of security mechanisms
- Program alteration
- Program theft
</td> 
<td style="text-align: left;">
- Unauthorized data I/O
</td> 
</tr>
</table>

### Encryption (TDE)

- Data encryption standard (DES) uses bit manipulation (substitution and permutation) and blocks of 64 bits
- Advanced Encryption Standards (AES) uses block size of 128/192/256 bits
- Public key encryption (Diffie & Hellman, 1976); later RSA public key (1978)
- [https://blogs.msdn.microsoft.com/plankytronixx/2010/10/22/crypto-primer-understanding-encryption-publicprivate-key-signatures-and-certificates/](https://blogs.msdn.microsoft.com/plankytronixx/2010/10/22/crypto-primer-understanding-encryption-publicprivate-key-signatures-and-certificates/)

### Authentication and authorization

- Role-based access control (RBAC)
- Mandatory access control (MAC)
- Principles, permissions, securables

#### Digital signatures

Digital signatures are based on Public Key techniques; are different for each use; are commonly used for online transactions

#### Ownership chaining

#### Contained database

### Preventing SQL injections

### Backup





# Sources

## Cited

Chapple, M. (2016, November 29). Should I denormalize my database? ThoughtCo. Retrieved from [https://www.thoughtco.com/should-i-normalize-my-database-1019730](https://www.thoughtco.com/should-i-normalize-my-database-1019730)

displayName. (2015, December 3). Are determinants and candidate keys same or different things? [Comment]. Stack Overflow. Message posted to [https://stackoverflow.com/questions/16706637/are-determinants-and-candidate-keys-same-or-different-things](https://stackoverflow.com/questions/16706637/are-determinants-and-candidate-keys-same-or-different-things)

E/R model: types of attributes. (n.d.). Retrieved from the Database Management Wiki: [http://databasemanagement.wikia.com/wiki/E/R_Model:_Type_of_Attributes](http://databasemanagement.wikia.com/wiki/E/R_Model:_Type_of_Attributes)

Connolly, T. & Begg, C. (2015). _Database systems: A practical approach to design, implementation, and management_ (6th ed.). New York City, NY: Pearson Education.

Sheffield, W. (2012, October 12). Does a clustered index really physically store the rows in key order? [http://blog.waynesheffield.com/wayne/archive/2012/10/does-a-clustered-index-really-physically-store-the-rows-in-key-order/](http://blog.waynesheffield.com/wayne/archive/2012/10/does-a-clustered-index-really-physically-store-the-rows-in-key-order/)

Sheldon, R. (2013, July 30). Columnstore indexes in SQL Server 2012. _Simple Talk._ Retrieved from [https://www.simple-talk.com/sql/database-administration/columnstore-indexes-in-sql-server-2012/](https://www.simple-talk.com/sql/database-administration/columnstore-indexes-in-sql-server-2012/)

Sheldon, R. (2014, March 25). 14 SQL Server indexing questions you were too shy to ask. _Simple Talk._ Retrieved from [https://www.simple-talk.com/sql/performance/14-sql-server-indexing-questions-you-were-too-shy-to-ask/](https://www.simple-talk.com/sql/performance/14-sql-server-indexing-questions-you-were-too-shy-to-ask/)

Sunderraman, R. (2012). Entity-relationship (ER) model. Retrieved from [http://tinman.cs.gsu.edu/~raj/4340/sp12/er.html](http://tinman.cs.gsu.edu/~raj/4340/sp12/er.html)

Ullman, R. D. (2006). Relational database design. Retrieved from [http://infolab.stanford.edu/~ullman/fcdb/jw-notes06/reldesign.html](http://infolab.stanford.edu/~ullman/fcdb/jw-notes06/reldesign.html)

Watt, A. (n.d.). Functional dependencies. In _Database design._ Retrieved from [https://opentextbc.ca/dbdesign/chapter/chapter-11-functional-dependencies/](https://opentextbc.ca/dbdesign/chapter/chapter-11-functional-dependencies/)

Winand, M. (n.d.). Anatomy of a SQL index. Retrieved from [http://use-the-index-luke.com/sql/anatomy](http://use-the-index-luke.com/sql/anatomy)


## References

- [Database Management Wikia](http://databasemanagement.wikia.com/wiki/Main_Page)
- [Use the index, Luke: A guide to database performance for developers](http://use-the-index-luke.com/sql/table-of-contents)

## Read

- [Intro. to Relational Databases](https://lagunita.stanford.edu/courses/DB/RDB/SelfPaced/about)
- [Relational Algebra](https://lagunita.stanford.edu/courses/DB/RA/SelfPaced/about)
- [Relational Design Theory](https://lagunita.stanford.edu/courses/DB/RD/SelfPaced/about)
- [Relational Database Fundamentals](http://www.lynda.com/Access-tutorials/Relational-Database-Fundamentals/145932-2.html)
- [ORM is an offensive anti-pattern](http://www.yegor256.com/2014/12/01/orm-offensive-anti-pattern.html)
- [Access 2016](http://www.lynda.com/Access-tutorials/Access-2016-Essential-Training/367064-2.html)

## Unread

- [Overview of SQL RDBMS](https://www.codecademy.com/articles/sql-rdbms)
- [How does a relational database work?](http://coding-geek.com/how-databases-work/)
- [Indexes &amp; Transactions](https://lagunita.stanford.edu/courses/DB/Indexes/SelfPaced/about)
- [Constraints &amp; Triggers](https://lagunita.stanford.edu/courses/DB/Constraints/SelfPaced/about)
- [Views &amp; Authorization](https://lagunita.stanford.edu/courses/DB/Views/SelfPaced/about)
- [OLAP](https://lagunita.stanford.edu/courses/DB/OLAP/SelfPaced/about)
- [Recursion](https://lagunita.stanford.edu/courses/DB/Recursion/SelfPaced/about)
- [Database dependency](http://databases.about.com/od/specificproducts/a/Database-Dependency.htm)
- [CodeSchool - NoSQL with MongoDB](https://www.codeschool.com/courses/the-magical-marvels-of-mongodb)
- [From relational to graph databases](https://neo4j.com/developer/graph-db-vs-rdbms/)
- [Blockchain](https://en.wikipedia.org/wiki/Blockchain_(database))
- [Is Git a blockchain?](https://news.ycombinator.com/item?id=9436847)
- [SQL vs NoSQL](http://dataconomy.com/sql-vs-nosql-need-know/)
- [Beyond Relational](http://faculty.washington.edu/blabob/bob/eBooks/Beyond%20Relational%20(WP%20MarkLogic%202015).pdf)
- [Why NoSQL](http://faculty.washington.edu/blabob/bob/eBooks/Why%20NoSQL%20(WP%20IBM).PDF)
- [7 steps to understanding NoSQL databases](http://www.kdnuggets.com/2016/07/seven-steps-understanding-nosql-databases.html)
- [Mapping your SQL thinking to NoSQL](http://apigee.com/about/blog/technology/nosql-noproblem-mapping-your-sql-thinking-nosql)
-[BigSQL on Hadoop](https://bigdatauniversity.com/courses/sql-access-on-hadoop-big-sql-v4/)
- [Enterprise NoSQL for Dummies](http://faculty.washington.edu/blabob/bob/eBooks/Enterprise%20NoSQL%20for%20Dummies%20(eBook).pdf)
- [Graph Databases](http://graphdatabases.com/)

# What is data science?

## What is Big Data?

## Data science lifecycle & skills

Via Mason and Wiggins (2010):

<table class="bullets">
<tr>
<th>OSEMN Model</th>
<td>Obtain</td>
<td>Scrub</td>
<td>Explore</td>
<td>Model</td>
<td>iNterpret</td>
</tr>

<tr>
<th rowspan=2>Alt Terms</th>
<td>Acquire</td>
<td>Clean</td>
<td colspan=2 rowspan=2>Analyze</td>
<td rowspan=2>Apply</td>
</tr>

<tr>
<td colspan=2>Wrangle</td>
</tr>

<tr>
<th>Skills & Tools</th>
<td style="text-align: left;">
- Plain text
- CSV
- JSON
- XML/HTML
- Query DB
- Query API
- REST
- Encoding
</td>

<td style="text-align: left;">
- Filter data
- Extract data
- Extract values
- Replace values
- Handle NULL, missing data
- Convert formats
</td>

<td style="text-align: left;" colspan=2>
- Summary stats
- Visualization
- Clustering
- Classification
- Regression
- Dimension reduction
</td>

<td style="text-align: left;">
- Conclusion
- Implications
- Communication
</td>

</tr>
</table>

### Obtain 

**Describe the concepts here. Code can be embedded or via a link, depending on how much there is.**

- What are major risks in web scraping?
- When would you scrape versus using an API?
- How do you parse scraped web data (HTML, JSON, XML)?
- How is authorization implemented in Google APIs?

### Scrub

- [http://radar.oreilly.com/2012/07/data-jujitsu.html](http://radar.oreilly.com/2012/07/data-jujitsu.html)
- What are major steps in data cleaning?
- What's the best way to filter data?
- What's the best way to aggregate data?

### Explore

See [notes on data visualization.](data-viz.html)

[https://medium.com/@eytanadar/banning-exploration-in-my-infovis-class-9578676a4705](https://medium.com/@eytanadar/banning-exploration-in-my-infovis-class-9578676a4705)

### Model

See [notes on models,](models.html) [statistics,](statistics.html) [machine learning,](machine-learning.html) and [text analytics.](text-analytics.html)

### iNterpret

- What are best practices for making Excel sheets auditable?
- [_Thinking with data_](http://shop.oreilly.com/product/0636920029182.do)





## Data science tools

R, Python, Bash, SQL on MySQL, Spark, Excel, Tableau are most common; see [2016 Data Science Salary Survey](http://www.oreilly.com/data/free/files/2016-data-science-salary-survey.pdf?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=revue) and [2016 Stack Overflow Developer Survey.](https://stackoverflow.com/insights/survey/2016)

### Why command line for data science?

Per Janssens (2015):

- **Agile:** supports faster iteration through a read-eval-print loop (REPL) versus an edit-compile-run-debug loop.
- **Augmenting:** amplifies rather than replaces existing tools
- **Scalable:** ability to automate commands means they're repeatable, supporting scalable analytic workflows
- **Extensible:** because command line tools are language-agnostic
- **Ubiquitous** (via Linux/Unix) 






# Workflow management

- [https://jakevdp.github.io/blog/2017/03/03/reproducible-data-analysis-in-jupyter/](https://jakevdp.github.io/blog/2017/03/03/reproducible-data-analysis-in-jupyter/)
- [https://swcarpentry.github.io/good-enough-practices-in-scientific-computing/](https://swcarpentry.github.io/good-enough-practices-in-scientific-computing/)





# Sources

Janssens, J. (2015). _Data science at the command line: Facing the future with time-tested tools._ Sebastopol, CA: O'Reilly.

Mason, H. & Wiggins, C. (2010). A taxonomy of data science [blog post]. _dataists._ Retrieved from [http://www.dataists.com/2010/09/a-taxonomy-of-data-science/](http://www.dataists.com/2010/09/a-taxonomy-of-data-science/)

## References

- [Google APIs Explorer](https://developers.google.com/apis-explorer/#p/)
- [Google Developers](https://developers.google.com/)
- [Handy Python libraries for data cleaning](https://blog.modeanalytics.com/python-data-cleaning-libraries/)
- [The definitive guide to doing data science for good](http://www.google.com/url?q=http%3A%2F%2Fblog.datalook.io%2Fdefinitive-guide-data-science-good%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFEeDt8cmAXAGdxIEkBkfvfpFaj0Q)
- [Inspiring graphics representing the power of Big Data](http://www.google.com/url?q=http%3A%2F%2Fbigdatapix.tumblr.com%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEehYvPPRU2P5aV9UdWZkxXQ4sGXQ)
- [A gallery of interesting IPython notebooks](https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks)
- [Payscale.com](http://www.google.com/url?q=http%3A%2F%2FPayscale.com&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGimDEmMBGkQckho0pfxd2Apq2pog)

## Read

- [Treehouse -&nbsp;CSV &amp; JSON&nbsp;in Python](https://www.google.com/url?q=https%3A%2F%2Fteamtreehouse.com%2Flibrary%2Fcsv-and-json-in-python&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNH4m-fFGcDa-fuIpEHKxk28kRxlqg)
- _Bad data handbook._
- [Essentials of Excel](http://www.lynda.com/Excel-tutorials/Excel-2016-Essential-Training/376985-2.html)
- [2012 Big Data Trends](http://www.google.com/url?q=http%3A%2F%2Fnewvantage.com%2Fwp-content%2Fuploads%2F2012%2F12%2FNVP-Big-Data-Survey-Themes-Trends.pdf&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHRoT4kKvY4DlkGXKQGbYza3sms_Q)
- [Non-Geek’s Big Data Playbook](https://drive.google.com/file/d/0B6XYyy1UbJ3XNHZTNnd5VHJlU1lFTy14X21yakpRbkp1aXY0/view?usp=sharing)
- [What is data science?](http://www.google.com/url?q=http%3A%2F%2Fwww.harlan.harris.name%2F2011%2F09%2Fdata-science-moores-law-and-moneyball%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFFUXewbEeU8TMiZNRGDjapcpXbQg)
- [Overview of data science skills](https://www.google.com/url?q=https%3A%2F%2Fhail-data.quora.com%2FHow-to-acquire-the-Essential-Skill-Set-the-Self-Starter-way&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFKlrTrypOXNz0wAef67v-A-uQrsQ)
- [Curriculum for undergrad data science class](https://drive.google.com/open?id=0B6XYyy1UbJ3XTGJ3UU9oSnVhNEk)
- [Become a type A data scientist](http://www.google.com/url?q=http%3A%2F%2Fwww.kdnuggets.com%2F2016%2F08%2Fbecome-type-a-data-scientist.html&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFH66SnPEMDR4tnCqIIxeJ1uKuhoQ)
- [2015 Data Science Salary Survey](https://drive.google.com/open?id=0B6XYyy1UbJ3XbG41SmpRNE5MY00)
- [2016 Data Science Salary Survey](http://www.google.com/url?q=http%3A%2F%2Fwww.oreilly.com%2Fdata%2Ffree%2Ffiles%2F2016-data-science-salary-survey.pdf%3Futm_campaign%3DRevue%2520newsletter%26utm_medium%3DNewsletter%26utm_source%3Drevue&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHoClnoMNA-S7SxTpbK-wfKIPIvvA)
- [Beginner’s Guide to Getting Your First Data Science Job](https://drive.google.com/file/d/0B6XYyy1UbJ3XZnl1S2d3aUlOMWc/view?usp=sharing)
- [One year as a data scientist at Stack Overflow](http://www.google.com/url?q=http%3A%2F%2Fvarianceexplained.org%2Fr%2Fyear_data_scientist%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFsYHVOtqKTMjBOpf9Cjnr2d42hWg)
- [The evolution of data science at Airbnb](http://www.google.com/url?q=http%3A%2F%2Fblog.kaggle.com%2F2016%2F09%2F06%2Fbuilding-a-team-from-the-inside-out-alok-gupta-on-the-evolution-of-data-science-at-airbnb%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHq0epTyxnQywNKzIra7o4rnPOj6Q)

## Unread

- [Formulas](http://www.lynda.com/Excel-tutorials/Excel-2016-Advanced-Formulas-Functions/431188-2.html)
- [What is a modern, SaaS-based BI stack?](https://blog.fishtownanalytics.com/what-are-the-steps-tools-in-setting-up-a-modern-saas-based-bi-infrastructure-281e0860f9a9#.bm4b1vblj)
- [What's a modern BI stack?](https://blog.fishtownanalytics.com/what-are-the-steps-tools-in-setting-up-a-modern-saas-based-bi-infrastructure-281e0860f9a9#.bm4b1vblj)
- [What we know about spreadsheet errors](http://panko.shidler.hawaii.edu/SSR/Mypapers/whatknow.htm)
- [Eight (no, nine!) problems with Big Data](https://www.nytimes.com/2014/04/07/opinion/eight-no-nine-problems-with-big-data.html)
- [What is the difference between Data Analytics, Data Analysis,](http://www.quora.com/What-is-the-difference-between-Data-Analytics-Data-Analysis-Data-Mining-Data-Science-Machine-Learning-and-Big-Data-1)
- [Deep learning vs machine learning vs pattern recognition](http://www.datasciencecentral.com/profiles/blogs/deep-learning-vs-machine-learning-vs-pattern-recognition)
- [Big data: the four layers that everyone must know](http://www.ap-institute.com/big-data-articles/big-data-the-4-layers-everyone-must-know.aspx)
- [The curse of big data](http://www.analyticbridge.com/profiles/blogs/the-curse-of-big-data)
- [The parable of Google flu: traps in Big Data analysis](http://gking.harvard.edu/files/gking/files/0314policyforumff.pdf)
- [The Cardinal Sin of Data Mining](http://www.kdnuggets.com/2014/06/cardinal-sin-data-mining-data-science.html)
- [Approaching (Almost) Any Machine Learning Problem&nbsp;](http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/)
- [Techniques of Big Data](http://www.lynda.com/Hadoop-tutorials/Techniques-Concepts-Big-Data/158656-2.html)
- [Top 3 algorithms in plain English](http://dataconomy.com/top-3-algorithms-plain-english/)
- [Decision tree algorithm](http://www.codechannels.com/video/edureka/data-science/understanding-decision-tree-algorithm-edureka/)
- [association rule mining](http://www.codechannels.com/video/edureka/data-science/association-rule-mining-data-science-edureka/)
- [random forest classifier](http://www.edureka.co/blog/random-forest-classifier/)
- [How to detect a pattern: problem and solution](http://www.analyticbridge.com/profiles/blogs/how-to-detect-a-pattern-problem-and-solution)
- [Big data acronyms and abbreviations](http://jethro.io/blog/big-data-acronyms-and-abbreviations)
- [Where predictive analytics is having the biggest impact](https://hbr.org/2016/05/where-predictive-analytics-is-having-the-biggest-impact?)
- [21 data science systems used by Amazon to operate its business](http://www.datasciencecentral.com/profiles/blogs/20-data-science-systems-used-by-amazon-to-operate-its-business)
- [How is big data used in practice?](http://www.ap-institute.com/big-data-articles/how-is-big-data-used-in-practice-10-use-cases-everyone-should-read.aspx)
- [24 Uses of Statistical Modeling](http://www.datasciencecentral.com/profiles/blogs/top-20-uses-of-statistical-modeling)
- [Machine Learning Becomes Mainstream: How to Increase Your Competitive Advantage](http://www.datasciencecentral.com/profiles/blogs/machine-learning-becomes-mainstream-how-to-increase-your)
- [How Companies are Using Machine Learning](https://hbr.org/2016/05/how-companies-are-using-machine-learning-to-get-faster-and-more-efficient?__s=1sug2edwwzuepsbhzhoz)
- [Improving operations using data analytics](https://www.oreilly.com/ideas/improving-operations-using-data-analytics)
- [Text analysis of Trump's tweets confirms he writes only the (angrier) Android half](http://varianceexplained.org/r/trump-tweets/)
- [Facebook V: Predicting Check Ins](https://ttvand.github.io/Winning-approach-of-the-Facebook-V-Kaggle-competition/)
- [EdX: The Analytics Edge](https://courses.edx.org/courses/course-v1:MITx+15.071x_2a+2T2015/2891f8bf120945b9aa12e6601739c3e6/)
- [Of prediction and policy: Applying algorithms to public policy](http://www.economist.com/news/finance-and-economics/21705329-governments-have-much-gain-applying-algorithms-public-policy)
- [Beginner’s Guide to the History of Data Science](http://www.google.com/url?q=http%3A%2F%2Fdataconomy.com%2Fbeginners-guide-history-data-science%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNH_qcKxjcd-DAAvXg_zWX7kzUKA7w)
- [A very short history of data science](http://www.google.com/url?q=http%3A%2F%2Fwww.forbes.com%2Fsites%2Fgilpress%2F2013%2F05%2F28%2Fa-very-short-history-of-data-science%2F%2376ae778569fd&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEpft2sPmMQw1O1wCxORjUQnG0HRg)
- [Data science: the end of statistics?](https://www.google.com/url?q=https%3A%2F%2Fnormaldeviate.wordpress.com%2F2013%2F04%2F13%2Fdata-science-the-end-of-statistics%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEX9FQWmpXnP-59VF-tKf4kvDgkAg)
- [What statisticians think about data scientists](http://www.google.com/url?q=http%3A%2F%2Fwww.datasciencecentral.com%2Fprofiles%2Fblogs%2Fwhat-statisticians-think-about-data-scientists&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGm3a3YP_b1OiE_1ABY1YNbEEEWTQ)
- [Data Science Compared to 16 Analytic Disciplines](http://www.google.com/url?q=http%3A%2F%2Fwww.datasciencecentral.com%2Fprofiles%2Fblogs%2F17-analytic-disciplines-compared&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEjtsf0c0S7M3g9pJ4XrRPEM4iaPQ)
- [High versus low-level data science](http://www.google.com/url?q=http%3A%2F%2Fwww.datasciencecentral.com%2Fprofiles%2Fblogs%2Fhigh-level-versus-low-level-data-science&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEe74Dh1pID4GB4LuN3Xv1n5o5Qpw)
- [Data science cheat sheet](http://www.google.com/url?q=http%3A%2F%2Fwww.datasciencecentral.com%2Fprofiles%2Fblogs%2Fdata-science-cheat-sheet&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFNiLDxwomw2tMc50hiQZDQsOhS7A)
- [Skills checklist for first data science job](http://www.google.com/url?q=http%3A%2F%2F1onjea25cyhx3uvxgs4vu325.wpengine.netdna-cdn.com%2Fwp-content%2Fuploads%2F2014%2F12%2FUdacityUltimateSkillChecklistForYourFirstDataAnalystJob.pdf&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNH746Let7xxAFLenUAyhAi7uZgdSA)
- [38 seminal papers all data scientists should read](http://www.google.com/url?q=http%3A%2F%2Fwww.datasciencecentral.com%2Fgroup%2Fresources%2Fforum%2Ftopics%2F38-seminal-articles-every-data-scientist-should-read&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNELg0Qvmdfnkf12GmRnPi1nTw9ONg)
- [What statistics concepts are needed for excelling at data science?](http://www.google.com/url?q=http%3A%2F%2Fwww.kdnuggets.com%2F2016%2F08%2Fstatistics-topics-needed-excelling-data-science.html&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHpdLWvFlUWKh2cGq8ILiFnrU-kMA)
- [4 easy steps to becoming a data scientist](http://www.google.com/url?q=http%3A%2F%2Fwww.datasciencecentral.com%2Fprofiles%2Fblogs%2F4-easy-steps-to-becoming-a-data-scientist&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNE4o9n0pcJ2lGNRppqlmgA1dVnOEw)
- [Top Algorithms and Methods Used by Data Scientists](http://www.google.com/url?q=http%3A%2F%2Fwww.kdnuggets.com%2F2016%2F09%2Fpoll-algorithms-used-data-scientists.html&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHoySpJiVoWzXDLNefqUQReXOD8qw)
- DS curriculum:[Pluralsight](https://www.google.com/url?q=https%3A%2F%2Fwww.pluralsight.com%2Fblog%2Fdata-professional%2Flearning-path-data-analyst&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHUEG4Ar1WINfWvjL0JNpxID7NtvA), [General Assembly](https://www.google.com/url?q=https%3A%2F%2Fgeneralassemb.ly%2Feducation%2Flearn-data-analysis-online&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEI9Cf0yv4BiPnzXOyzoaV8Z9P47g), [DataQuest](https://www.google.com/url?q=https%3A%2F%2Fwww.dataquest.io%2Flearn&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGJ3Omy1LNWaMp1sHo_ArS6mrn30Q), [Udacity](https://www.google.com/url?q=https%3A%2F%2Fwww.udacity.com%2Fcourse%2Fdata-analyst-nanodegree--nd002&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNH_-llcEXW6MF4UCI9uFTY1sXZDEw), [Data School](http://www.google.com/url?q=http%3A%2F%2Fwww.dataschool.io%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNG6zDzdbXOeZYZGxYMmDSFKCOIdxg), [Open Source Data Science masters](http://www.google.com/url?q=http%3A%2F%2Fdatasciencemasters.org%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFvvYsmrIetQCBoBeWqSu5RIf494Q)
- [Getting started with data science](http://www.google.com/url?q=http%3A%2F%2Fpartiallyderivative.com%2Fresources%2F2015%2F2%2F17%2Fgetting-started-with-data-science&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGR3oBQckqsvW32JsvNzOLqjU5JMg)
- [Data science projects for data science apprentices](http://www.google.com/url?q=http%3A%2F%2Fwww.datasciencecentral.com%2Fgroup%2Fdsa-projects%2Fforum%2Ftopics%2Fdata-science-projects-for-dsa-candidates&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGnb-P4SLbKYWXac6ojs3WCfS5zZg)
- [New coder tutorials](http://www.google.com/url?q=http%3A%2F%2Fnewcoder.io%2Ftutorials%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHhbzT658qzztWhgBomka73ampRRQ)
- [Confronting jargon](https://www.google.com/url?q=https%3A%2F%2Fmedium.com%2F%40duretti%2Fconfronting-jargon-7d39c8dd9353%23.re1tlrs5p&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFTx6ZMIsYsFvlqT2SIOMq7rWS8tA)
- [My data science journey](http://www.google.com/url?q=http%3A%2F%2Fwww.datasciencecentral.com%2Fprofiles%2Fblogs%2Fmy-data-science-journey&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFSgql1qJDyre2oxRWhZDML0PxTbA)
- [The data science industry: Who does what?](https://www.google.com/url?q=https%3A%2F%2Fwww.datacamp.com%2Fcommunity%2Ftutorials%2Fdata-science-industry-infographic&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGeGN7P11RfyE17iRG0p2PC1eMXYg)
- [Data science career paths: Different roles in the industry](https://www.google.com/url?q=https%3A%2F%2Fwww.springboard.com%2Fblog%2Fdata-science-career-paths-different-roles-industry%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFF_f0cUUWEfsR83PjdZLCIPUSoGw)
- [9 types of data scientists](http://www.google.com/url?q=http%3A%2F%2Fwww.datasciencecentral.com%2Fprofiles%2Fblogs%2Fsix-categories-of-data-scientists%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGFlYWvvrFMh4L8ulF5JlcmtUJcAQ)
- Data science interview prep: [1](http://www.google.com/url?q=http%3A%2F%2Fblog.udacity.com%2F2015%2F04%2Fdata-science-interview-questions.html&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNF5QW4TXA3OpccPjVoRXzmQfQZGlA), [2](https://www.google.com/url?q=https%3A%2F%2Fwww.dezyre.com%2Farticle%2F100-data-science-interview-questions-and-answers-general-%2F184&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHGsvpYO9UiMzqMxFo1mrjZ1kT8vw), [3](http://www.google.com/url?q=http%3A%2F%2Fsteve-yegge.blogspot.com%2F2008%2F03%2Fget-that-job-at-google.html&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHC4Ev98jT0GfuuV00TTlJRdIEKBQ), [4](https://www.google.com/url?q=https%3A%2F%2Fwww.quora.com%2FHow-does-Airbnb-hire-data-scientists%3F__s%3D1sug2edwwzuepsbhzhoz&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFxkqN6uz2zG8hJe0rTbzzRkHsx5w), [5](http://www.google.com/url?q=http%3A%2F%2Fwww.kdnuggets.com%2F2016%2F02%2F21-data-science-interview-questions-answers.html%3F__s%3D1sug2edwwzuepsbhzhoz&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEFlANuWeAasXz3uMqREKvtaqWgOg),[6](http://www.google.com/url?q=http%3A%2F%2Fwww.kdnuggets.com%2F2016%2F01%2F20-questions-to-detect-fake-data-scientists.html&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGA8jrF_4AaM_EzAgGUrrJ5ZK9MEw), [7](http://www.google.com/url?q=http%3A%2F%2Fwww.fastcompany.com%2F3062158%2Fhit-the-ground-running%2Fhow-to-ace-a-data-science-interview&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEtAlE_Jojv3cQuK4mgC4T5nW6BKw), [8](http://www.google.com/url?q=http%3A%2F%2Fwww.datasciencecentral.com%2Fprofiles%2Fblogs%2F66-job-interview-questions-for-data-scientists&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHR5DgeLtwbEwlF15wMN5M2kB_RPw), [9](http://www.google.com/url?q=http%3A%2F%2Fwww.datasciencecentral.com%2Fprofiles%2Fblogs%2F25-questions-to-detect-fake-data-scientists&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNE_wRib5RRCDq8u7ElRg9T4TEDGyg), [10](http://www.google.com/url?q=http%3A%2F%2Fwww.fastcompany.com%2F3062713%2Fhow-to-be-a-success-at-everything%2Fi-hire-engineers-at-google-heres-what-i-look-for-and-why&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNE4ftaiEVCc-CDKsvyMqBlfEb0ktA), [11](http://www.google.com/url?q=http%3A%2F%2Fshop.oreilly.com%2Fproduct%2F0636920039259.do%3Fintcmp%3Dil-data-books-videos-product-na_20150815_new_site_common_questions_in_data_science_interviews_video_post_note_link&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNH2KhH15AX9K5DUzVuBRWP63oUh5A), [12](https://www.google.com/url?q=https%3A%2F%2Fwww.fastcompany.com%2F3063167%2Fevery-data-science-interview-boiled-down-to-five-basic-questions&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFNkhEWZ40BySqRFt_wgTUhwAKCsQ), [13](https://www.google.com/url?q=https%3A%2F%2Fwww.springboard.com%2Fblog%2Fdata-science-interviews-lessons%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGLM_SAxd-INeCXpUEBNWCkDEIROg), [14](http://www.google.com/url?q=http%3A%2F%2Fanalyticscosm.com%2Fmachine-learning-interview-questions-for-data-scientist-interview%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNF3PMoqnZmpdRBR56_nPJl1Dd5X4Q)
- [5 secrets for writing the perfect data science resume](https://www.google.com/url?q=https%3A%2F%2Fwww.oreilly.com%2Fideas%2F5-secrets-for-writing-the-perfect-data-scientist-resume%3Fimm_mid%3D0e581a%26cmp%3Dem-data-na-na-newsltr_20160706%26__s%3D1sug2edwwzuepsbhzhoz&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGdhAN5YxS2z9xbyq_e-jfhfLgO9w)
- [80,000 Hours Career Guide](https://www.google.com/url?q=https%3A%2F%2F80000hours.org%2Fcareer-guide%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGlMjbiH7gq5Nw5Imcl32gRnb1MZw)
- [What factors can increase your data science salary?](https://www.google.com/url?q=https%3A%2F%2Fwww.springboard.com%2Fblog%2Fhighest-data-scientist-salary-possible%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGfXBBQe8f3lzukZqOuZxXHXIW6sg)
- [Data Science for IoT vs Classic Data Science: 10 Differences](http://www.datasciencecentral.com/profiles/blogs/data-science-for-iot-vs-classic-data-science-10-differences)
- _Data wrangling with Python._
- [Coursera - Using Python to Access Web Data](https://www.coursera.org/learn/python-network-data)
- [Scraping the web](http://schoolofdata.org/2013/11/25/scraping-the-web/)
- [Prevent web scraping](https://blog.hartleybrody.com/prevent-scrapers/)
- [How to prevent getting blacklisted while scraping](https://www.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/)
- [Some traps to know and avoid in web scraping](https://www.promptcloud.com/blog/some-traps-to-avoid-in-web-scraping/)
- [HTML scraping with requests and lmxl](http://docs.python-guide.org/en/latest/scenarios/scrape/)
- [Don’t parse HTML with regex](http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454)
- [Automate the Boring Stuff with Python: Web Scraping](https://automatetheboringstuff.com/chapter11/)
- [The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets](http://www.joelonsoftware.com/articles/Unicode.html)
- [The Data Scrub](https://statswithcats.wordpress.com/2010/10/17/the-data-scrub-3/)
- [Why data preparation should not be overlooked](http://www.datasciencecentral.com/profiles/blogs/why-data-preparation-should-not-be-overlooked)
# Types of data visualizations

Source: Eckerson, W., & Hammond, M. (2011). Visual reporting and analysis. TDWI Best Practices Report. TDWI, Chatsworth. Retrieved from http://www.smartanalytics.com.au/pdf/Advizor-TDWI_VisualReportingandAnalysisReport.pdf

![](../ILLOS/viz-types.png)

# What are Tufte's design principles?

Trust the eye as a tool that extracts patterns from complex data. Provide viewers with dense information in high-resolution; maximize information, minimize clutter.


# Data viz in Python

- [http://nbviewer.jupyter.org/github/plotly/python-user-guide/blob/master/Index.ipynb](http://nbviewer.jupyter.org/github/plotly/python-user-guide/blob/master/Index.ipynb)
- [http://nbviewer.jupyter.org/gist/darribas/4121857](http://nbviewer.jupyter.org/gist/darribas/4121857)


```Python
import matplotlib.pyplot as plt

# convert to int: 
my_data = list(map(int, data_in)))

# linecharts/plots:
fig = plt.figure()
ax = fig.add_subplot(1,1,1)
x_axis_ticks = list(range(len(my_data)))
ax.plot(x_axis_ticks, my_data, linewidth=2)
ax.set_title(my_title)
ax.set_xlim([0, len(sample)])
ax.set_xlabel(‘Axis name’)
ax.set_ylabel(‘Axis name’)
fig.save_fig(my_filename)

# table: see also tablib
from prettytable import PrettyTable
my_data_header = my_data[0]
x = PrettyTable(my_data_header)
x.add_row(my_data[1])

# plot all bars as barchart:
X = numpy.arange(len(my_data))
width = 0.25
plt.bar(X+width, prices, width)
plt.xlim(0, 5055)

# plot buckets:
from collections import Counter
def group_data_by_range(my_data):
talley = Counter()
for em in data:
bucket = 0
if em >=0 and em < 10:
bucket = 1
elif em >= 10 and em < 20:
bucket = 2
talley[bucket] += 1
return talley
fig = plt.figure()
ax = fig.add_subplot(1,1,1)
plt.style.use(‘ggplot’)
colors = plt.rcParams[‘axes.color_cycle’]
for group in my_grouped_data:
ax.bar(group, my_grouped_data[group], color=colors[groups[group%len(my_grouped_data)])
labels = [‘Group 1’, ‘Group 2’ … ]
ax.legend(labels)
ax.set_title(‘Title’)
ax.set_xlabel(‘Axis name’)
ax.set_xticklabels(labels, ha=’left’)
ax.set_xticks(range(1, len(my_grouped_data)+1))
ax.set_ylabel(‘Axis name’)
plt.grid(True)
```


# Data viz in R

- library for complex graphs: [http://ggplot2.org/](http://ggplot2.org/)

```R
p <- seq(0, 1, 0.01)

# scatterplot: 
plot(my_df$name1, my_df$name2)

# line: 
plot(... type=”l”)

plot(var1 ~ var2))

# univariate boxplot: 
boxplot(my_df$var_name)
# multivariate boxplot: 
boxplot(var1 ~ var2)

# histogram: 
hist(data, breaks=)

# frequencies: 
table()
# multivariable: 
table(my_df$var1, my_df$var2) 
mosaicplot(table(my_df$var1, my_df$var2) )
mosaicplot(var1 ~ var2)

# relative frequencies: 
table(my_df$my_var)/length(my_df$my_var)

barplot(table())

# plot in three rows: 
par(mfrow = c(3, 1))
xlimits <- range(data1)
hist( … xlim=xlimits)
plot_ss(x = mlb11$at_bats, y = mlb11$runs,  x1, y1, x2, y2)
showSquares=T/F
leastSquares=T/F

# OLS best-fit: 
lm(y ~ x, my_df)
summary(lm(...)

# line: 
abline()
abline(lm(...))
qqnorm(m1$residuals)
qqline(m1$residuals)
hist(m1$residuals)

# account for overlapping data point: 
plot(jitter(x), y)
```



# Sources

## References

- [Timdream's HTML5 wordcloud generator](https://timdream.org/wordcloud/)

## Read

- UW IT - [Tableau Intro](http://itconnect.uw.edu/work/data/training/workshops/#TableauDesktopIntroduction), [Tableau Fundamentals](http://itconnect.uw.edu/work/data/training/workshops/#TableauDesktopFundamentals)
- [Tableau - Which chart?](https://drive.google.com/file/d/0B6XYyy1UbJ3XOVJxVTFJOURpVWc/view?usp=drive_web)
- [Tufte - Presenting data & information](https://www.edwardtufte.com/tufte/courses)
- [Coursera - Infographic Design](https://www.coursera.org/learn/infographic-design)
- [Data visualization as a first step . . . and also an intermediate step and a last step](http://andrewgelman.com/2009/05/25/data_visualizat_1/)
- [Old-school NYT death infographics were depressing](http://mentalfloss.com/article/61828/old-school-new-york-times-death-infographics-were-depressing)
- [3D dataviz taxonomy](http://www.datavizualization.com/blog/taxonomy-of-3d-dataviz)

## Unread

- [Word clouds considered harmful](http://www.niemanlab.org/2011/10/word-clouds-considered-harmful/)
- [Tableau - Training videos](http://www.tableau.com/learn/training)
- [Lynda - Interactive Tableau Dashboards](https://www.lynda.com/Tableau-tutorials/Creating-Interactive-Dashboards-Tableau/417094-2.html)
- [Design a better dashboard](https://pages.sisense.com/dashboard-design-video.html) (Sisense workshop)
- [Design better data tables](https://medium.com/mission-log/design-better-data-tables-430a30a00d8c#.w1siia9bf)
- [Comparison among graph types](https://faculty.washington.edu/wijsman/GRAPHS3.pdf) (p. 24) [PDF]
- [DATA + DESIGN ebook](https://infoactive.co/data-design)
- [Youtube - Infographics & Data Visualization Course](https://www.youtube.com/watch?v=fZswD5RC1G8&list=PLa4VFIBUKrgLao-DalwedOCiq9RV6MPk9)
- [OpenLearn - Effective Ways of Displaying Information](http://www.open.edu/openlearn/science-maths-technology/computing-and-ict/information-and-communication-technologies/effective-ways-displaying-information/content-section-0)
- [Information Visualization MOOC](http://ivmooc.cns.iu.edu/index.html)
- _The Visual Display of Quantitative Information_
- _Visual Explanations_
- _Beautiful Evidence_
- _Envisioning Explanations_
- [DataCamp - ggvis, ggplot 1, ggplot 2, R Markdown](https://www.datacamp.com/courses/)
- [FlowingData - Tutorials](http://flowingdata.com/category/tutorials/) (mostly in R)
- [Tufte in R](http://motioninsocial.com/tufte/)
- [Data visualization: modern approaches](https://www.smashingmagazine.com/2007/08/data-visualization-modern-approaches/)
- [Data visualization and infographics](https://www.smashingmagazine.com/2008/01/monday-inspiration-data-visualization-and-infographics/)


# Sources

## References

## Read

- [Orwell's "Politics and the English language"](http://www.npr.org/blogs/ombudsman/Politics_and_the_English_Language-1.pdf) [pdf]
- [Paul Graham's "The age of the essay"](http://paulgraham.com/essay.html)
- [Paul Graham's "On writing, briefly"](http://www.paulgraham.com/writing44.html)

## Unread
# What is microeconomics?

# What is macroeconomics? 

## Macroeconomic metrics 

How are key economic metrics calculated, how did they originate, and what are their limitations?

# What is econometrics?





# Sources

## References

## Read

- [Relatively deprived: How poor is poor?](http://www.newyorker.com/magazine/2006/04/03/relatively-deprived)

## Unread

- [Stop using the unemployment rate](http://evansoltas.com/2012/08/13/stop-using-unemployment-rate/)
- [Is child poverty really less of a problem than we thought?](http://cepr.net/blogs/cepr-blog/the-supplemental-poverty-measure-does-it-paint-a-more-accurate-picture-of-poverty)
- [Alternative data sources for women's work in agriculture<br>](http://www.fao.org/docrep/x0188e/x0188e.htm)

# What is accounting?

## What is the fundamental accounting identity?

## What is the accounting process?

# What is finance? 

## What good is it?

## What is sustainable finance?


# Sources

## References

- [Investopedia's investing tutorials<br>](http://www.investopedia.com/university/all/basics/)

## Read


## Unread

- [World's top firms cause $2.2tn of environmental damage, report estimates](https://www.theguardian.com/environment/2010/feb/18/worlds-top-firms-environmental-damage)
- [The Differences Among Socially Responsible, Impact, and Common Good Investing](http://www.huffingtonpost.com/terry-mollner/the-differences-among-soc_b_4221293.html)
- [Why does financial sector growth crowd out real growth?](http://www.bis.org/publ/work490.htm)

# What is Git?

Git is a versioning tool tailored to the needs of programmers. Like all versioning tools, Git records a series of changes, letting you restore an earlier version if something goes wrong with the current one. In particular, Git is a versioning tool that works offline; lets you choose your file editing programs; saves manually, not automatically (reflecting the stance that old versions are useful only if they are complete, coherent, functioning); saves multiple documents at a time, not just one; and allows branching.

There are three general workflows in Git: solo, collaborative (local-remote), and GitHub (local-remote-collaborator). Obviously workflows should be customized to suit the circumstances (number of collaborators, purpose of branches, purpose of master).

# Solo workflow

Cloning or initializing Git in a directory creates a staging area and repository with one branch, the __master__. Master should be production-quality code that always runs, so work should be done in __branches__, which are in most cases intended to be temporary. Branches are important for sharing code and for compartmentalizing your own work. 

Git tracks which commit you're 'on' by means of the __HEAD__ pointer. HEAD can be pointed to a commit or to a branch that, in turn, points to its __'tip'__ (its most recent commit). Each commit points to its parent commit, making most earlier commits __reachable__. 'Checking out' means moving the HEAD pointer to a new commit; checking out a commit that's not the tip of a branch puts you in _detached HEAD state_, and any commits you make from here will be unreachable. Git periodically runs a garbage collector process that deletes unreachable commits; until the garbage collector runs, all commits are [accessible via their SHA](http://blog.thoughtram.io/git/2014/11/18/the-anatomy-of-a-git-commit.html) if you happen to know it.

So, the workflow: 

1. Check out a branch, and edit files in your working directory---as many as needed to implement a single logical change. 
2. As you finish editing individual files, add them to the staging area. 
3. When all edited files are in the staging area, commit them to the specified branch in the repository.
4. When this branch is functional, merge it into the master (this might require manual resolution of conflicts).

<img src="../ILLOS/solo-workflow.png" width="700px">

# Local-remote workflow

For collaborative work via remotes, you retain your tripartite solo working environment but add new steps:

1. To start with someone else's files, clone their repository (which copies the current state and commit history of their directory, and adds a remote called 'origin'); otherwise, create your own repository and add another repository to yours as a remote. 
2. Create a branch for your personal work, and commit your work to this branch. 
3. Pull (fetch and merge) changes from the remote into your master. 
4. Merge your branch with your updated master. 
5. Push your master up to the remote for review.

<img src="../ILLOS/local-remote-workflow.png" width="700px">

# Local-remote-collaborator workflow

For collaborative work via GitHub, you retain your tripartite solo working environment and, from your command line, work with your GitHub repository like a remote repository (remember to [cache your GitHub login](https://help.github.com/articles/caching-your-github-password-in-git/)). GitHub has additional functionality, though, like issues, wikis, forking (the ability to clone someone else's GitHub projects to your GitHub account). Every GitHub repository has three special files by default: __README.md__, a description of the project; __CONTRIBUTING.md__, instructions for how to contribute to the project; and __ISSUE_TEMPLATE.md__, a template for raising issues with the project. 

A sample workflow:

1. Create a directory on GitHub, sometimes by forking another repository. 
2. Clone this directory locally so you can edit it. 
3. Create and checkout a new branch to modify and commit to your local repository. 
4. Push the local branch to your personal GitHub remote; no need to pull from your GitHub remote first. 
5. On GitHub, from the new branch, make a pull request to merge the branch into your personal GitHub master or the master you forked from. 
6. Other users can see your pull request, discuss it, eventually accept it and merge the branch into the master.

<img src="../ILLOS/local-remote-collaborator-workflow.png" width="700px">



# CODE

__CREATE__: _CONFIGURE, INIT, CLONE, BRANCH, CHECKOUT, ADD, COMMIT, MERGE_

```Bash
git --version
git config --global color.ui auto  # make diff output colored
git config --global merge.conflictStyle diff3  # make merge files show content of original

git init  # create .git project in working directory
git clone remote_loc [clone_name]  # copy repository to local drive; remote_loc can be path, HTTP, SSH
# to start with someone else's files, clone their repository 
# cloning copies the current state and commit history of their directory

git branch branchname  # create new branch
git checkout branchname  # switch branches
git checkout -b branchname  # create and switch to new branch

git add file1 file2 ...  # add newly edited file/s to staging area
git add -A # add all edits and deletions to staging area
git commit -m "Your message here"  # commit to repository
# Change editor for longer commit messages: https://www.youtube.com/watch?v=s_eFuGauy6k
# Style guide for commit messages: http://udacity.github.io/git-styleguide/
# title can include a tag like 'feat', 'fix', 'docs', 'style', 'refactor', 'test', 'chore' 
# title should be in imperative present tense, and should be &lt;50 chars 
# body lines &lt;72 chars; should address 'what' and 'why', not 'how' (should be evident in the code)
# include relevant issue #s in footer if applicable

git merge branchname  # call from branch you want to merge into
# If there is a merge conflict, discrepancies will show up inside the documents in the working directory; 
# undo the merge (see below) or edit, add, and commit the affected documents to complete the merge.
```

__UNDO__: _CHECKOUT, RESET, ABORT, AMEND, REVERT_

```Bash
git rm  filename '*pattern.txt'  # deletes files and stages deletions

git checkout shortSHA  # discard changes to working dir by checking out specified commit
git checkout -- filename.ext  # restore file to version from previous commit
git checkout HEAD [files]  # (partially) discard changes to working dir by checking out most recent commit
git checkout file1, file2 ..  # partially discard changes to working dir by restoring from staging area
# 5 or 7 of 40-character SHA should be enough to identify a commit
# CHECKOUT usually moves the HEAD pointer; exception, when it's applied at the file level

git reset --soft shortSHA  # move branch tip to an earlier commit, abandoning intervening commits
git reset --mixed shortSHA  # move branch tip to earlier commit, reset staging area to that commit
git reset --hard shortSHA  # reset staging area, working dir to earlier commit and move branch tip to same
git reset  # branch tip unchanged; resets staging area to HEAD
git reset file1 file2 ...  # partially resets staging area from HEAD; options above don't apply
# RESET moves a branch pointer; it should be used to undo changes on a private branch

git branch -d branchname  # delete branch after successful merge
git merge --abort  # to abandon a merge, run this after git reset HEAD

git commit --amend  # 'undo' most recent commit by making new commit of parent commit
git revert HEAD-2  # recycle commit-from-two-commits-ago as a new commit, extending the branch tip
# REVERT should be used to undo changes on a public branch 
  
git gc  # run garbage collector
```

__MONITOR__: _STATUS, DIFF, LOG, SHOW_

```Bash
git status # check status

git diff -u file  # see differences working directory and staging area
git diff --staged  # see differences between staging area and repository
git diff shortSHA  # see differences between working directory and specified commit
git diff shortSHA1 shortSHA2  # see differences between specified commits
# Type 'q' to exit diff mode

git log  # view commits history
git log -n 10  # view 10 most recent commits
git log --stat  # esp helpful view for multi-file commits

git branch  # list all branches and indicate working branch
git log --graph --oneline branch1 branch2  # visualize changes between branches

git show HEAD  # show most recent commit
git show shortSHA  # diff a commit with its parent
```

__COLLABORATE__: _CLONE, REMOTE, FETCH, PULL, PUSH, MERGE_

```Bash
git clone remote_loc [clone_name]  # copy remote repository to local drive

git init
git remote  # check local for existing remotes; -v option makes verbose
git remote add remotename remote_loc  # give remote a convenient name 
git remote rename oldname newname
git remote rm remotename  # remove convenient name
# Remote branches are prefixed by their remote's name so you don’t mix them up with local branches

git fetch remotename [branchname]  # check for changes in remote directory without merging 
git merge remotename/branchname  # merge remote branch into local's active branch
# https://git-scm.com/docs/git-merge#_how_conflicts_are_presented
git pull remotename [branchname]  # fetch & merge content into local's active branch

git push -u remotename branchname  # FIRST TIME: push content from local's specified branch to remote
git push  # subsequent times, if no change to remote or branch
```





# Questions

## Git-style version control

*How could having easy access to the entire history of a file make you a more efficient programmer in the long term?*

- Would be able to diagnose past mistakes: where did this go wrong? 
- Not lose work from machine failures.

*What do you think are the pros and cons of manually choosing when to create a commit,like you do in Git, vs having versions automatically saved, like Google Docs does?*

- Save manually: -WILL forget, +sensible, intelligible versions, +reasonable num of versions
- Save automatically: +can't forget, +/-more data, which may or may not be useful

*Why do you think some version control systems, like Git, allow saving multiple files in one commit, while others, like Google Docs, treat each file separately?*

- Git recognizes that files in coding are more likely to be interdependent, affected by each other's updates.

## Major components of git

*What happens when you initialize a repository? Why do you need to do it?*

- You create a hidden .git file within the working directory, to store metadata for the files.
- This is the first step, followed by adding specific files to be tracked.

*How is the staging area different from the working directory and the repository? What value do you think it offers?*

- The staging area lets you decide which files from the working directory should be part of a commit, reducing the size of each commit: making them more logically distinct and therefore meaningful and navigable. Without a staging area, all files in the wd would get committed together irrespective of whether they'd changed.

*How can you use the staging area to make sure you have one commit per logical change?*

- ...along with git diff and get diff --staged. 

## Branching, diffing, and merging

*What are some situations when branches would be helpful in keeping your history organized? How would branches help?*

- Branches might be helpful when making a major but experimental change, or a deliberately different 'flavor' of the code'.

*How do the diagrams help you visualize the branch structure?*

- They are literally visualizations of a branch structure, unlike the output of git log--which, without the --graph option, is linear.

*What is the result of merging two branches together? Why do we represent it in the diagram the way we do?*

- Branches are not adirectionally merged 'together'; one branch is merged INTO another branch. The branch being merged is no longer needed,because the branched merged INTO inherits all its commits. Both branches are parents of the new commit, but we can still safely delete the branch being merged. 

*What are the pros and cons of Git’s automatic merging vs. always doing merges manually?*

- Always doing merges manually means you need to be really familiar with the purpose and content of each branch as well as the code, so that you can merge intelligently. It would be nearly impossible for auto merge to match the accuracy of an informed programmer.

*How did viewing a diff between two versions of a file help you see the bug that was introduced?*

- It drastically reduced the number of lines I had to squint at.

## Remotes & GitHub-based collaboration

*When would you want to use a remote repository rather than keeping all your work local?*

- When you expect to be using different computers, or to be collaborating with others.


*Why might you want to always pull changes manually rather than having Git automatically stay up-to-date with your remote repository?*

- You don't want things being overwritten, or incomprehensibly fragmented, by automatic pull changes.

*Describe the differences between forks, clones, and branches. When would you use one instead of another?*

- TO CLONE is to copy a directory either remote or local, such that its commit history comes along with it. Clone when you want to make your own branch of someone's work.
- TO BRANCH is to explore something before merging it back into the master code.
- TO FORK is to clone someone else's GitHub project via GitHub; it is to make a remote-remote connection.

*What is the benefit of having a copy of the last known state of the remote stored locally?*

- ... someone else with access to the remote might commit to it and change it while you're working on the same code 

*How would you collaborate without using Git or GitHub? What would be easier, and what would be harder?*

- I think it depends on the scale of the project. But definitely, the larger the project, the more benefit in handling it with these tools.

*When would you want to make changes in a separate branch rather than directly in master? What benefits does each approach have*

- Because you're trying to preserve the master for display/production. 



# Sources

## REFERENCES

- [Share code quickly with a Gist](https://gist.github.com/)
- [Official GitHub cheatsheet](https://services.github.com/on-demand/downloads/github-git-cheat-sheet.pdf) [pdf]
- [Visual Git Guide](https://marklodato.github.io/visual-git-guide/index-en.html)
- [NDP Software's Interactive VIsual Git Cheat Sheet](http://ndpsoftware.com/git-cheatsheet.html)

## Read

- [Udacity - How to use Git and GitHub](https://www.udacity.com/course/how-to-use-git-and-github--ud775)
- Codecademy - [Learn Git](https://www.codecademy.com/learn/learn-git), [Push to GitHub](https://www.codecademy.com/articles/push-to-github)
- [Git for grownups](https://24ways.org/2013/git-for-grownups/)
- [Mastering GithHub Markdown](https://guides.github.com/features/mastering-markdown/)
- [Caching your GitHub password in Git](https://help.github.com/articles/caching-your-github-password-in-git/)
- Checkout, reset, revert: [1](https://makandracards.com/makandra/11485-git-basics-checkout-vs-reset), [2](https://www.atlassian.com/git/tutorials/resetting-checking-out-and-reverting/), [3](https://git-scm.com/blog/2011/07/11/reset.html)
- [Try Git](https://try.github.io

## Unread

- GitHub as a portfolio: [1](https://www.epicodus.com/blog/sprucing-up-github), [2](http://pydanny.blogspot.com/2011/08/github-is-my-resume.html)
- Merge vs rebase [1](http://gitforteams.com/resources/merge-rebase.html), [2](https://www.atlassian.com/git/tutorials/merging-vs-rebasing/)
- Treehouse - [Git Basics](https://teamtreehouse.com/library/git-basics), [GitHub Basics](https://teamtreehouse.com/library/github-basics), [GitHub Desktop](https://teamtreehouse.com/library/share-your-projects-with-github-desktop)
- [CodeSchool - Git](https://www.codeschool.com/learn/git)
- [Markdown tutorial](https://daringfireball.net/projects/markdown/)

# Computer hardware: Information storage & processing

https://fossbytes.com/wp-content/uploads/2016/04/latest-computer-chart.jpg, https://fossbytes.com/wp-content/uploads/2016/04/the-complete-computer-hardware-chart.jpg, http://eent3.lsbu.ac.uk/units/compsys/1%20Network%20Computer%20HardwareSlides.htm

High-level languages must be translated into machine code. This translation is hardware-specific, not portable. Translation can be done once (when code is complied into an executable) or on-the-fly by an interpreter.
http://stackoverflow.com/questions/30156349/is-bash-an-interpreted-language

![Layers](../ILLOS/layers.jpg)
![Block diagram of computer](../ILLOS/block-diagram-of-computer.jpg)
![Case](../ILLOS/case.jpg)
![Ports](../ILLOS/ports.jpg)


# Networks: Information representation and transfer

__Bits__, binary digits, are the 'atoms' of computerized information. Bits can be represented and transmitted by any two-state medium plus a consistent bits-per-unit-time clock (since states may be consecutive). The _bandwidth_ of a system, its capacity to transmit information, is also called its _bitrate_. _Latency_ is also important: that's the time it take for a bit to travel from receiver to sender. 

When bits flow between machines and the machines have __protocols__ to interpret the bits, you have a __network__: devices that can communicate with each other. Most modern networks are packet-switched networks. Instead of sending a constant bitstream, devices on the network send packets of bits plus metadata. The __router__ takes these packets and routes them efficiently but robustly to a destination device that reassembles the packets. 

"Computer networks differ in the transmission medium used to carry their signals, the communications protocols to organize network traffic, the network's size, topology and organizational intent" [[1](https://en.wikipedia.org/wiki/Computer_network)]: 

- Cheap copper cables transmit electricity over comparatively short distances.
- Expensive fiber optic cables transmit light much faster, over much longer distances with less signal decay. 
- Like copper cables, radio waves (wi-fi) experience signal decay over distance. Radio waves are analog, so multiple translations are needed. Antennae & tower.

## Home networks

[Networks can be many different sizes and scopes](https://en.wikipedia.org/wiki/Template:Area_networks): LAN, PAN, WAN, MAN, etc. A P2P network (most home networks) lacks a central computer or server; a LAN might include Ethernet cables, wi-fi/wireless radio signals, or powerline network adaptors connect each device to a switch or hub.

In data communication, a physical network node may either be a [data communication equipment](https://en.wikipedia.org/wiki/Data_communication_equipment) (DCE) such as a [modem](https://en.wikipedia.org/wiki/Modem), [hub](https://en.wikipedia.org/wiki/Network_hub), [bridge](https://en.wikipedia.org/wiki/Bridging_(networking) or [switch](https://en.wikipedia.org/wiki/Network_switch); or a [data terminal equipment](https://en.wikipedia.org/wiki/Data_terminal_equipment) (DTE) such as a digital telephone handset, a printer or a [host computer](https://en.wikipedia.org/wiki/Host_computer) (router, a workstation or a server).

- client: 
- server:  provide resources to clients on the network. 
- NIC: A network interface controller (NIC, also known as a network interface card, network adapter, LAN adapter or physical network interface, and by similar terms) is a computer hardware component that connects a computer to a computer network.
  - Ethernet is a specific kind of NIC
- a switch knows devices apart and can route communications selectively
- a hub sends all communication to all connected devices (use case: surveillance of employee behavior)
- an access point/base station/wireless router connects wi-fi devices to network
- A wireless repeater, which can also be known as a wireless range extender, is a device that takes a signal from an existing access point and rebroadcasts it as a second network. 
- Bluetooth: radio signals between devices
- a router: can be ISP-provided (even bundled together with a modem in one device); connects devices, incl. modem. 
  - _A router is a device that sends packets of data between different networks._ [[2](http://whatismyipaddress.com/router-modem)]
  - _Routers ... send packets between LANs, while also assigning IP addresses, acting as a switch and protecting your LAN [like with a hardware firewall]._ [[2](http://whatismyipaddress.com/router-modem)]
  - _When your computer asks to browse a website, it’s the router’s job to send requests out to that website, then direct the replies back to the appropriate device on your network. Your router will have also have a public IP address, through which Internet services and websites will know where to send their data back to your house, at which point the router examines the data packet and says, “Oh, this was meant for that PC in the bedroom, I’ll send it there.”_ [[2](http://whatismyipaddress.com/router-modem)]
  - directs packets flowing from modem to LAN back to the specific devices that requested them
- a modem: provided by ISP, takes signals from router, converts them, sends them to ISP.
  - _modem turns the information from our network into information manageable by the telephone infrastructure and vice versa._ [[2](http://whatismyipaddress.com/router-modem)]
  - tapped into phone system: dial-up, DSL, ADSL (ADSL splitter/filter: separates voice and computer frequencies in phoneline (instead of modem))
  - own lines: coaxial cable, fiber optic cable
  - wireless broadband: antenna to radio tower
  - radio to mobile devices
  - satellite radio
- Bridge: connects wired devices to something else via wi-fi (opposite of access point)
- firewall: can be hardware (in router) or software (on device); works by closing network ports

## The Internet

The Internet is a network of networks with special protocols and entities. Internet means "interconnected networks". Internet is not WWW: _the Internet is a technical infrastructure which allows billions of computers to be connected all together. Among those computers, some computers (called Web servers [or HTTP servers]) can send messages intelligible to web browsers. The Internet is an infrastructure, whereas the Web is a service built on top of the infrastructure. It is worth noting there are several other services built on top of the Internet, such as email and IRC. // The Internet carries an extensive range of information resources and services, such as the inter-linked hypertext documents and applications of the World Wide Web (WWW), electronic mail, voice over IP telephony, and peer-to-peer networks for file sharing._ [[3](https://en.wikipedia.org/wiki/World_Wide_Web)]. The Web is a graphical interface for some of the content that's available through the Internet.

An __ISP__ is a company that manages some special routers that link all together and can also access other ISPs' routers. So the message from our network is carried through the network of ISP networks to the destination network. [[5](https://www.reddit.com/r/explainlikeimfive/comments/3kgehf/eli5how_does_my_internet_service_provider_isp/)] 

- web server provides the Web---in the form of webpages and resources, clustered into websites with domain names---to a web client's browser. The browser renders web pages.
- _The browser goes to the DNS server and finds the real address of the server that the website lives on (you find the address of the shop). The browser sends an HTTP request message to the server asking it to send a copy of the website to the client (you go to the shop and order your goods). This message, and all other data sent between the client and the server, is sent across your internet connection using TCP/IP. Provided the server approves the client's request, the server sends the client a "200 OK" message, which means "Of course you can look at that website! Here it is", and then starts sending the website's files to the browser as a series of small chunks called data packets (the shop gives you your goods, and you bring them back to your house). The browser assembles the small chunks into a complete website and displays it to you (the goods arrive at your door — new stuff, awesome!)._ 
- web server is a computer hosting one or more websites. "Hosting" means that all the webpages and their supporting files are available on that computer. The web server will send any webpage from the website it is hosting to any user's browser, per user request.
  - HTTP server
  - file storage
  - A server is any program that responds to requests from another program (client). Servers are run on host machines/host computers. Server=software, host=hardware.

ISPs can connect to each other as peers via [IXPs](https://en.wikipedia.org/wiki/List_of_Internet_exchange_points_by_size) or to upstream providers: Just as their customers pay them for Internet access, ISPs themselves pay upstream ISPs for Internet access. An upstream ISP usually has a larger network than the contracting ISP or is able to provide the contracting ISP with access to parts of the Internet the contracting ISP by itself has no access to (https://en.wikipedia.org/wiki/Internet_service_provider)

Under the Internet Protocol (__IPv4, IPv6__), all devices on the Internet have a unique IP address, and bits travel in IP packets (rather than directly from one machine to another). Routers receive these packets and---following transmission control protocol (TCP), user datagram protocol (UDP), or some other packet directing protocol---send them along different routes from sender to receiver, depending on traffic and outages. A router also take inventory of packets when they arrive and tells the sending server to resend any that were incomplete or missing. each packet carries the intended IP address, a number to determine where the packet fits back into the data sent, how many packets to expect, as well as your IP address

The domain name system (__DNS__) associates IP addresses with more human-friendly URLs. The DNS system consists of many DNS servers, connected to each other in a distributed hierarchy. DNS servers are sometimes spoofed: malicious IPs are associated with a domain that people might navigate to. Type URL into browser; DNS converts to IP?; browser talks to server located at IP ...:

- URL: http://www.ascii-code.com/, http://ascii.cl/
  - `<scheme>://<username>:<password>@<host>:<port>/<path>;<parameters>?<query>#<fragment>`
    - `<host> is <subdomain>.<domain>.<second-level domain>.<top-level domain> or <IP address>`
- in hypertext transfer protocol (HTTP). Browser-server communication consists of:
  - GET requests from the browser, and web files (a combination of text, HTML, and CSS; or images, or videos) in response from the server.
  - POST requests from the browser, sending information to the server
  - COOKIES from the server to the browser and browser to the server; how the server remembers who you are
- Or, you might communicate with a server via the HTTPS protocol. The server offers a digital certificate that your browser verifies with a certification authority. Then, communication between you is protected by secure sockets layer (SSL) or transport layer security (TLS).
  - Encrypt-key-decrypt
  - Caesar cypher: shift every letter by the same number
  - Shared key: symmetric encryption. Impossible for computers via Internet. 
  - Public key cryptography: 
    - N-digit keys: shift every n letters by n numbers: the idea behind 256-bit encryption
    - So, asymmetric encryption: the server offers its public key for encrypting messages to it, and private keys are ...
- Or ftp, pop, imap, mailto, etc.; https://en.wikipedia.org/wiki/Lists_of_network_protocols

_When you type an address such as www.codecademy.com in your browser, you are commanding it to open a TCP channel to the server that responds to that URL ... Once the TCP connection is established, the client sends an HTTP request to the server ..._

![URL](../ILLOS/complex_url.png)
![OSI model layers](../ILLOS/OSI-model-layers.png)








# Sources 

## References

- [Glossary of computer jargon](http://www.computerhope.com/jargon.htm)
- [Internet jargon](http://www.computerhope.com/jargon/internet.htm)
- [Network jargon](http://www.computerhope.com/jargon/network.htm)


## Read

- Mozilla - [How the Internet works](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/How_does_the_Internet_work), [What is a web server?](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_is_a_web_server), [HTTP](https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview)
- Wikipedia - [Outline of the Internet](https://en.wikipedia.org/wiki/Outline_of_the_Internet), [Computer network](https://en.wikipedia.org/wiki/Computer_network), [Internet protocol suite](https://en.wikipedia.org/wiki/Internet_protocol_suite), [OSI model](https://en.wikipedia.org/wiki/OSI_model#Description_of_OSI_layers), [Internet Exchange Point](https://en.wikipedia.org/wiki/Internet_exchange_point)
- Computer Hope - [broadband](http://www.computerhope.com/jargon/b/broadban.htm), [DNS](http://www.computerhope.com/jargon/d/dns.htm), [reverse lookup](http://www.computerhope.com/jargon/r/rdns.htm), [domain](http://www.computerhope.com/jargon/d/domain.htm), [webhost](http://www.computerhope.com/jargon/w/webhost.htm), [http server](http://www.computerhope.com/jargon/h/http-server.htm), [modem](http://www.computerhope.com/jargon/m/modem.htm), [P2P network](http://www.computerhope.com/jargon/p/ptpnet.htm), [WWW](http://www.computerhope.com/jargon/w/www.htm), [access point](http://www.computerhope.com/jargon/a/accepoin.htm), [ethernet](http://www.computerhope.com/jargon/e/ethernet.htm), [protocol](http://www.computerhope.com/jargon/p/protocol.htm), [firewall](http://www.computerhope.com/jargon/f/firewall.htm), [port](http://www.computerhope.com/jargon/p/port.htm), [home network](http://www.computerhope.com/jargon/h/homenetw.htm), [host computer](http://www.computerhope.com/jargon/h/hostcomp.htm), [server](http://www.computerhope.com/jargon/s/server.htm), [ISP](http://www.computerhope.com/jargon/i/isp.htm), [OSI](http://www.computerhope.com/jargon/o/osi.htm), [LAN](http://www.computerhope.com/jargon/l/lan.htm), [NOS](http://www.computerhope.com/jargon/n/nos.htm), [REST](http://www.computerhope.com/jargon/r/rest.htm), [router](http://www.computerhope.com/jargon/r/router.htm), [TCP/IP](http://www.computerhope.com/jargon/t/tcpip.htm)
- MakeUseOf - [What is home networking](http://www.makeuseof.com/tag/everything-need-know-home-networking/), [Types of Internet access technologies](http://www.makeuseof.com/tag/types-of-internet-access-technologies-explained-and-what-you-should-expect/), [How the Internet works](http://www.makeuseof.com/tag/technology-explained-how-the-internet-works/), [What’s the difference between routers, hubs, &amp; switches?](http://www.makeuseof.com/tag/whats-difference-routers-hubs-switches/), [How does a router work?](http://www.makeuseof.com/tag/technology-explained-how-does-a-router-work/), [Wireless networking terms](http://www.makeuseof.com/tag/wireless-networking-simplified-the-terms-you-should-know/), [10 wrong ways to set up your wireless network](http://www.makeuseof.com/tag/10-wrong-ways-set-wireless-network/), [How to Optimize Your House For Best Wi-Fi Reception](http://www.makeuseof.com/tag/wireless-feng-shui-optimize-house-wifi-reception/)
- [How networks send data](http://pluto.ksi.edu/~cyh/cis370/ebook/ch03c.htm), [Does all LAN traffic travel through a router](http://superuser.com/questions/295528/does-all-lan-traffic-travel-through-a-router), [Anatomy of a URL](https://doepud.co.uk/blog.php/anatomy-of-a-url), [URLs](https://www.cs.tut.fi/~jkorpela/HTML3.2/3.5.html), [What every dev should know about URLs](http://www.skorks.com/2010/05/what-every-developer-should-know-about-urls/), [URL definitions](https://www.mattcutts.com/blog/seo-glossary-url-definitions/), [HTTP Requests](https://www.codecademy.com/articles/http-requests)

## Unread

- [Intro. to ICT](http://openbookproject.net/courses/intro2ict/index.html)
- Security Now [videos] - [Let’s design a computer](https://twit.tv/shows/security-now/episodes/233?autostart=false), [Machine language](https://twit.tv/shows/security-now/episodes/235?autostart=false), [Pointers](https://twit.tv/shows/security-now/episodes/237?autostart=false), [Stacks, registers, and recursion](https://twit.tv/shows/security-now/episodes/239?autostart=false), [Hardware interrupts](https://twit.tv/shows/security-now/episodes/241?autostart=false) 
- Wikiversity - [Intro. to Computers](https://en.wikiversity.org/wiki/Introduction_to_Computers), [IT Fundamentals](https://en.wikiversity.org/wiki/IT_Fundamentals), [IC3](https://en.wikiversity.org/wiki/IC3), [Intro. Computer Science](https://en.wikiversity.org/wiki/Introduction_to_Computer_Science)
- [How computers work](http://www.explainthatstuff.com/howcomputerswork.html), [Computer parts and what they do](http://explainlikeakid.blogspot.com/2011/10/computer-parts-and-what-they-do.html), [Why your computer gets slow over time](http://explainlikeakid.blogspot.com/2011/09/why-your-computer-gets-slow-overtime.html), [How computers work](http://www.carnegiecyberacademy.com/facultyPages/computer/computers.html#OS), [Memory](http://www.linfo.org/memory.html)
- [Wiki - Internet protocol suite](https://en.wikipedia.org/wiki/Internet_protocol_suite#Layer_names_and_number_of_layers_in_the_literature)
- [The network effect](http://networkeffect.io/)
- [How the internet works](http://pyvideo.org/pycon-us-2013/how-the-internet-works.html) [video] 
- [Microsoft Imagine Academy - Networking Fundamentals](http://www.spl.org/library-collection/articles-and-research/microsoft-it-academy)


# What is information retrieval?

IR occurs when a user retrieves information objects AKA content objects (documents and metadata) from an information retrieval system (e.g. libraries, archives, repositories/portals, websites, databases). [IR requires IA.](#what-is-information-architecture?)


## IR systems

- Catalogs, search engines, ??? 
- Svenonius (2005): "Systems that do not employ vocabulary control may be characterized in terms of their indexing: natural language, derived, keyword, or title word indexing; or, in terms of the type of searching they allow: free-text searching or full-text searching. It is not a foregone conclusion that a system with vocabulary control is better than one without it."


## Major activities in IR

### Cataloging & indexing

Information professionals work to ensure that IR (1) has good **precision,** AKA 'satisfies the requirement for general survey' or 'satisfies the collocating requirement' or attains 'representational predictability'; (2) has **good recall;** and (3) is [possible across multiple IR systems.](#interoperability) They do this by (1) [creating search algorithms](search-engines.html) or by (2) ingesting information objects into a catalog, which includes:

- **CATALOGING** the object, i.e. describing/representing an information object with metadata, including subject headings and subheadings. Subject headings and subheadings can be combined from the beginning (precoordination) or combined by users while searching (postcoordination)

    - Assigning subject headings (called "subject cataloging") entails **CLASSIFYING** objects. "In LIS, the term 'classification' is used to refer to three distinct but related concepts: a system of classes, ordered according to a predetermined set of principles and used to organize a set of entities; a group or class in a classification system; and the process of assigning entities to classes in a classification system" (Jacob, 2004, p.522). 
    - The first sense of classification given here (constructing classes) may also be called **CATEGORIZATION,** although categories are perhaps less strict than classes. "Categorization is the process of dividing the world into groups of entities whose members are in some way similar to each ... By reducing the load on memory and facilitating the efficient storage and retrieval of information, categorization serves as the fundamental cognitive mechanism that simplifies the individual’s experience of the environment" (Jacob, 2004, p. 518).
    
- **INDEXING** the object, i.e. mapping out the contents of an information object, perhaps using terms from a CV (at minimum, for named entities, an authority file should be consulted). 

    - Although **TAGGING** or keywording is occasionally used as a synonym for indexing, it is more often used to denote the opposite of indexing, in which keywords emerge from an information itself rather than from an indexing language. This allows for a grassroots view of what something is about.
    
    - Indexing may be performed by machines when it is **necessary** (there are many documents; content changes quickly; speed matters) or when it is **suitable** (documents are inherently structured; documents fall within a small domain; documents are only text).
        - Automatic indexing with a CV (AKA auto-tagging, -classification, -categorization) may be rules-based (e.g., using regex) or machine learning-based (Bayesian, support vector machines, neural networks). Regardless, performance is better with pre-coordinated terms. 
        - Automatic indexing that doesn't use a CV is called information extraction, which includes**entity extraction** AKA entity recognition, entity identification.
<br/><br/>
    - Indexing may be performed by humans (professionals or social media users) when it is **necessary** (accuracy matters more than speed; must index nontext objects; content lacks inherent structure; content varies greatly) or **feasible** (few documents). 
        - Human professional indexing consists of 
            - analyzing content; 
            - assigning terms; and possibly 
            - assigning relevancy scores/weights to the terms.
        - Human social indexing produced a **folksonomy,** different from the anthropological term "folk taxonomy" that describes informal categorization schemes.
    
<table style="padding-left: 90px;">
<tr><th></th><th></th>
<th colspan="3">INDEXING AGENT</th></tr>

<tr><th></th><th></th>
<th>Taxonomist</th>
<th>Users</th>
<th>Machine</th></tr>

<tr><th rowspan="2">TAXONOMY</th>
<th>Yes</th>
<td>Closed</td>
<td>n/a</td>
<td>Auto-Tagging, Auto-Classification, Auto-Categorization</td></tr>

<tr><th>No</th>
<td>Open</td>
<td>Tagging</td>
<td>Information Extraction</td></tr>
</table>


### Searching, browsing, & navigating
    
To retrieve information, users engage with information systems by:

- **SEARCHING** for a specific piece of information; this is the concern of back-end IA. Simultaneous search of multiple information systems is called metasearching, broadcast searching, cross-database searching, federated searching, and parallel searching.
    
- **BROWSING** a collection of information; this is the concern of front-end IA, with the goal of intuitive user interfaces and navigational structures. Browsing is "quick examination of the relevance of a number of objects which may or may not lead to a closer examination or acquisition/selection of (some of) these objects" Hjørland (2011); "visually scanning through organized collections of representations of content objects" (ANSI/NISO Z39.19-2005, p. 157). 

    - **NAVIGATION** is related to browsing, but has implications of (1) traversing a website (2) via "pre-established links or relationships" (ANSI/NISO Z39.19-2005, p. 162).


    
    
# What is information architecture?

Rosenfeld, Morville, and Arango (2015) say that IA aims to facilitate both the finding and understanding of information. The "finding" dimension of IA is information retrieval, the legacy of library science; the "understanding" dimension comes via Richard Saul Wurman, who focused on making complex systems intelligible through visualization. 

## Must information be organized?

Per Abrahamson and Freedman (2008), order (like all things) has both benefit and cost. Even setting feasibility aside, it's probably never the case that 100% order is _optimal._ At the same time, because individuals' organization strategies and preferences vary so much, shared information spaces must (?) be organized according to intelligible principles that prospective users of the space can access and learn.

### Anatomy of mess

Per Abrahamson and Freedman (2008):

<img src="../ILLOS/mess.jpg" width="600px">

### Bases of organization

Hedden (2016) offers a typology in which taxonomies (i.e., knowledge organization structures) have different bases/origins. KOSs may be (1) objective, with obvious empirical basis; (2) socially-negotiated, perhaps eventually becoming (3) culturally embedded; or (4) idiosyncratic:

![](../ILLOS/taxonomy_status.png)
    
## What ways can things be organized?

Schemas and structures work together. For example, my Zotero library is a hierarchy (structure) of topical categories (schema), automatically sorted by alphabet (schema).
The distinction I draw is that "schemas" are for the conceptual (abstract, disembodied) organization of information into groups, whereas "structures" describe group configuration. 

### Information schemas

Wurman (1990) claims there are only five ways to organize items. Similarly, Wyllys (2000) states that information can be organized according to different schemas that encompass Wurman's five ways (italicized):

- **Exact schemas** (mutually exclusive, AKA classes): _alphabet, location, chronology, continuum_
- **Inexact schemas,** AKA _categories:_ topical, task-oriented, audience-specific, metaphor-driver (e.g., information lifecycle)
- **Hybrid schemas,** combining multiple methods (may be confusing)

### Information structures

But, Wyllys adds, there are also three fundamental information structures, AKA **data models:** hierarchic, graph, and relational. In their review of [database history,](databases.html#history-of-databases) Connolly and Begg (2015) classify Wyllys' three structures as "record-based" and add "object-based" as another top-level category:

#### Record-based

Can’t express constraints on the data, but express structure well.

##### Hierarchical

Per Zeng (n.d.), 

- **Strict hierarchies** (trees) can model three different types of real-world relationships: 
    - __Generic:__ _X is a Y_ or _All X are Y, and some Y are X_
        - Succulent: Cactus (NTG)
        - Cactus: Succulent (BTG)
    - __Instance:__ _X is a Y_ but not a _kind_ of Y or a _part_ of Y
        - Mountain: Alps (NTI), Himalayas (NTI)
        - Alps: Moutain (BTI)
        - Himalayas: Mountain (BTI)
    - __Partitive:__ _Y consists of Xs_
        - Central nervous system: Brain (NTP), Spinal cord (NTP)
        - Brain: Central nervous system (BTP)
        - Spinal cord: Central nervous system (BTP)
- __Polyhierarchy__ (family tree) is more complex than a strict hierarchy

##### Graph

AKA networks, triples, ontologies. Enables explicit modeling of different kinds of relationships (Has, IsCreatedBy, etc.) as well as more relationships (not restricted to one parent). Records are also called nodes and segments; relationships are also called edges.

##### Relational

Tables with columns, implicitly related via attributes; see [notes on relational databases.](databases.html#the-relational-data-model)
    
#### Object-based

Allow specification of constraints, but not overall structure. Objects are instances of classes; classes and objects have attributes (properties, characteristics, adjectives/nouns) and methods (actions, functions, behaviors, verbs).

- Entity-Relationship (ER)
- Semantic (not sure how this is different from a graph data model)
- Functional
- Object-oriented


### Questions that arise in organizing

- What organization schema/s to use
- Whether to classify (1:1) or tag (1:&infin;)
- Whether to base category divisions off anticipated volume of material
- What information structure/s to use
- Whether to prefer shallower or deeper hierarchies





# What is taxonomy?

Per Hedden, taxonomy --- the law or science (nomos) of order (taxis) --- has both general and specific meanings. 

In general, taxonomy is the discipline of creating and managing taxonomies, a term synonymous with knowledge organization structures/systems (KOS) and very nearly synonymous with controlled vocabularies (CVs). As a field, taxonomy has roots in biological taxonomies and library catalogs (which existed even in antiquity as scrolls). Companies began to use taxonomies for corporate knowledge asset management very marginally in the 1980s, then increasingly in the 1990s as web technologies spread. 

Beyond this general sense, taxonomies are also [specific kinds of KOSs.](#what-are-knowledge-organization-structures?)

## What are knowledge organization structures?

<img src="../ILLOS/KSOs.png" width="550px">

### KOSs by structure

In order of increasing complexity (number, kind, and configuration of relationships):

| KOS | Ambiguity control | Synonym control | Hierarchical relationship | Associative relationship |
| --- | --- | --- | --- | --- |
| Term list | X | | | |
| Syn ring | X | X | | |
| Taxonomy | X | X | X | |
| Thesaurus | X | X | X | X |

**Term lists** are essentially flat list, though they may include "See"/"Use" to steer people towards preferred language, or use a synonym ring approach (no preferred term among multiple). 

- **Name authority file:** Identifies the definitive version of a name among all variants
- **Glossary** (vocabulary, clavis, specialized dictionary): An alphabetical list of terms with definitions inside a particular domain of knowledge.
- **Stop list:** Words to ignore in search query processing (because they are extremely common)
- **Dictionary:** Definitions, history, etc. for words.
- **Gazetteer:** A geographical dictionary

**Synonym rings** AKA synsets associate synonyms without indicating preference for one over the others (equivalence relationship). Synsets are usually invisible to users, e.g. underpinning a search engine.

**Taxonomies** AKA hierarchies AKA hierarchical taxonomies AKA tree structures arrange terms into parent/child relationships beneath a single top term (TT). A strict hierarchy requires that each term have a single parent AKA broader term (BT), though they may have multiple siblings and children AKA narrower terms (NT). 

Indexing a content object with a term is called posting; a given term contains X postings. The practice of indexing a content object with a broader term rather than a narrower term is called generic posting. If, instead, the content object is indexed with narrower and broader terms, this is called up-posting AKA autoposting. Generic posting also means subsuming narrower terms under a preferred broader term in a controlled vocabulary.

Taxonomies are a top-down approach. Thus, there may occasionally be node labels AKA blind references AKA facet indicators: terms in a CV that are never applied to a content object, but exist to preserve a coherent logical structure or to expose the logic behind choices made.

**Microcontrolled vocabularies** are subsets of a controlled vocabulary, creating a specialist CV.

**Faceted taxonomies** are a bottom-up approach to providing multiple views of the same content objects, based on shared attributes like topic, location, format, author, etc. They are often presented as navigation aids or search refinments; they may also be used to organize  very large controlled vocabulary. 

**Polyhierarchies** are hierarchies in which children may have multiple parents.

**Thesauri** capture associative relationships AKA related terms (RT) in addition to equivalence (U/UF) and hierarchical (BT/NT) relationships. Relationships are also called cross-references; they should be reciprocal (explicit entry at term Y linking to term X, explicit entry at term X linking to term Y) and may or may not be symmetric. Thesauri are useful for representing a very large controlled vocabulary.

**Semantic networks** fall short, in some way, of full **ontologies,** which are defined by their:   

- Ambition to "[define] a set of representational primitives [atoms] with which to model a domain of knowledge or discourse" (Tom Gruber, qtd. in Hedden, 2015)
- Network/graph model of interconnected triples (subject-predicate-object)
- Semantic relationships (predicates)
- Structured attributes
- Entities are instances of classes

Relevant technologies include RDF, OWL, and topic maps.

### KOSs by role in IR

As part of ingesting a content object into an information system, KOSs can be applied to the content object at several levels, creating more or less granular pictures of what the object is about: 

- Subject headings applied during cataloging may be organized in hierarchical (single parent), polyhierarchical (multi-parent), and faceted taxonomies
- Indexing may use indexing terms from an authority file or other CV

Information consumers depend on KOSs to browse and search content objects: 

- Browsing and navigating are supported by hierarchical taxonomies, faceted taxonomies, and pick lists
- [Search engines](search-engines.html) may use synonym rings AKA synsets to convert between user search terms and indexing terms that represent content objects
    - Small search engines (for website, intranet, CMS, repository) may use thesauri to facilitate search

See also [uses of CVs.](#uses-of-cvs)    
    

## What are reference works?

Reference works are information-dense resources meant to be consulted for specific information, not read comprehensively. Terminology for reference works is reviewed here because it often overlaps with KOS terminology. Definitions are quoted/paraphrased from Wikipedia:

- **Dictionary:** (1) A collection of words in one or more specific languages, often arranged alphabetically, which may include information on definitions, usage, etymologies, phonetics, pronunciations, translation, etc. (2) A book of words in one language with their equivalents in another, sometimes known as a lexicon.
- **Lexicon:** The ‘catalog’ of a language’s words, which together with a ‘grammar’ constitutes the language.
- **Lexical database:** Stores terms along with information about terms.
- **Encyclopedia:** A type of reference work or compendium holding a comprehensive summary of information from either all branches of knowledge or a particular branch of knowledge.
- **Bibliography:** Collected citations for related books, articles, etc., which may be as simple as a reading list or may be extensively annotated. 
- **Thesaurus:** A list of words grouped together according to similarity of meaning (containing synonyms and sometimes antonyms), in contrast to a dictionary, which provides definitions for words, and generally lists them in alphabetical order. 
- **Almanac:** An annual publication that includes information such as weather forecasts, farmers' planting dates, tide tables, and tabular information often arranged according to the calendar. 
- **Gazetteer:** A gazetteer is a geographical dictionary or directory used in conjunction with a map or atlas. They typically contain information concerning the geographical makeup, social statistics and physical features of a country, region, or continent.
- **Directory:** A listing of information for location/navigation; e.g. business direcfory, telephone directory, web directory.
- **Catalog:** A listing of content objects (their description and location). 





## What are controlled vocabularies?

CVs (AKA authority lists) consist of **terms, syntax** (how terms may be combined), and **term records** containing each term's **semantic relationships, scope notes,** and **history notes.**    

Controlled vocabularies stand in contrast with natural languages, which undermine information retrieval because they are fraught with polysemes* (ambiguous words --- both homographs and synonyms/near-synonyms/quasi-synonyms). At minimum, to avoid the IR pitfalls of natural languages CVs must:

- Clearly define the applicability of terms with scope notes (SN)
- Connect synonyms, thereby increasing recall; this is often accomplished with **UF (use for), U (use)**
- Disambiguate homographs, thereby increasing precision; this is often accomplished with a **gloss** AKA modifier AKA difference AKA qualifier term

*_Why so many polysemes in natural language?_

- _Words inherited from different parent languages, e.g. Greek and Latin_
- _Popular words versus jargon_
- _Trade names that become generic, e.g. bandaids_
- _Dialectical variations reflecting geographic boundaries and social groups_
- _Formal words versus slang_
- _Political changes leading to changes in preferred terms_
- _Competing terms for emerging objects and phenomena_



### Uses of CVs

Svenonius (2005), also see [KOSs by role in IR:](#koss-by-role-in-ir) *"In the form of terminological databanks, CVs are used to assist in both manual and automatic translation. In the form of glossaries, they standardize and explicate the meaning or usage of terms in specialized fields of activity. In the form of literary thesauri they assist in composition by facilitating the expression of ideas. In the form of conceptual structures they give backbone to knowledge representation systems."*    

#### CV-metadata connections

Per ANSI/NISO Z39.19 (p. 19), 

- A metadata standard may require that values for certain fields come from CVs
- Metadata may be used to describe a CV, supporting its discovery as a resource
- A CV may be expressed as a metadata schema, e.g. in XML




### Constructing CVs

#### Where do terms come from?

Terms are chosen from a specific domain/information space if their inclusion is warranted:

- **User warrant** emerges from user search terms and feedback;
- **Literary warrant** emerges from review of documents and standards; and
- **Organizational warrant** emerges from an organization's existing standards.
- **Structural warrant** emerges from, for example, a hierarchical KOS, where a parent term is created to relate child terms that arose from other warrants
- **Consistency,** though this "frequently conflict[s]" with the other warrants (Svenonius, 2005)

Since warrants shift over time, there should be a 'parking lot' for candidate terms AKA provisional terms. Terms can be generated by a committee (top-down, bottom-up); by empirical methods (deductive, inductive); by a machine; or from an existing CV (don't duplicate effort!).

#### How are terms expressed?

##### Scope & coordination/syntax

The basic rule per ANSI/NISO Z39.19 is that a term should denote a single **concept** or unit of thought; this is challenged by Svenonius (2005), who argues that concepts are inherently fuzzy and that quantitative linguistics provides a more objective foundation for extracting terms from natural language. At any rate, there are different kinds of concepts:

- Things and their parts
- Materials
- Activities, processes
- Events, occurrences
- Properties, qualities, states
- Disciplines, fields
- Units of measurement

The simplest term form is a **single-word term;** there are several kinds of multiword AKA **compound terms,** the construction of which is governed by the CV's **syntax:** 

A <span style="color: red; font-weight: bold;">(1)</span> **bound term** uses multiple words or a phrase to denote a single concept, e.g. _oral surgery._ The inverted form of bound terms may be included as an entry term, e.g. _surgery, oral: see oral surgery._ These <span style="color: red; font-weight: bold;">(a)</span> naturally occurring compound terms are generally preferable to <span style="color: red; font-weight: bold;">(b)</span> **qualified homographs** --- for example, _religious tolerance_ is generally better than _tolerance (religious)._  If no bound compound term exists, ambiguous terms (AKA head or focus nouns) should receive a modifier (AKA difference). Additionally, the scope of any term, not just homographs, may be clarified with a scope note (SN). If a SN mentions another term in the vocabulary, it should receive a reciprocal SN or cross-reference: _term2: X SN term1._

- Try to standardize qualifiers (e.g., don't have _biology_ and _bioscience_ both as qualifiers)
- The qualifier must not itself be a homograph
- The addition of a qualifier must not make the term a compound concept
- Qualify even the domain-specific dominant use of a term
- Qualify even if only one use of the homograph occurs in the CV
    - ... unless other meanings of the homograph are entirely outside the domain
        - ... but even then, anticipate metasearching and include a qualifier

While compound terms denote a single concept, <span style="color: red; font-weight: bold;">(2)</span> **coordinated terms** AKA synthesized terms associate several concepts with a single information object and generally fall in the domain of indexing languages. Coordinated terms can be <span style="color: red; font-weight: bold;">(a)</span> embedded in the object **(precoordination),** as with Library of Congress subject headings in books: _English Language--Rhetoric, Persuasion (Rhetoric), Report Writing._ Precoordinated terms enable browsing with great specificity, describe complex concepts, and impose alphabetic proximity on related terms that would otherwise be far apart. Given the high cost of taxonomist labor, though, it's often better to let users <span style="color: red; font-weight: bold;">(b)</span> combine terms interactively during search **(postcoordination),** e.g. _English Language AND Rhetoric AND Persuasion AND Report Writing._ 

Use of a compound term should be determined by warrant, total #terms in the CV (more compound terms means more overall terms), and intended format (print sometimes benefits from precoordinated terms); see ANSI/NISO Z9.19 pp. 39-40 for more guidance and examples.

##### Grammatical forms

- **Parts of speech:**
    - <span style="color:red;">Prefer nouns\*</span> or in the case of bound compound terms, noun phrases;
        - Prefer premodified/adjectival noun phrases, e.g. _historical drama, cold fusion_
        - Avoid postmodified/prepositional noun phrases unless idiomatic, e.g. avoid _hospitals for children_ in favor of _children's hospital,_ but prefer _burden of proof_ to _proof's burden_
    - Avoid verbs used as nouns; e.g., prefer _reading_ to _read,_ _cookery_ to _cook_
    - Single adjectives are sometimes included:
        - ... to facilitate coordination and avoid multiple compound terms, e.g. _small, portable, offshore_
        - ... as an entry term, e.g. _cardiac: see also terms beginning with heart_
    - Single adverbs are rarely needed, but depends on the domain
    - Drop initial articles unless they're important for finding an item (may depend on the underlying natural language)
- **Pluralization:**
    - Count nouns (how many?) should be plural unless the singular form is common in the domain
    - Mass nouns (how much?) should be singular unless users regard the noun as a class with >1 member
    - Regardless, abstract concepts and unique entities are singular
    - Singular and plural forms of a noun may denote different concepts, so both may appear (qualified) in the CV

<span style="color:red;">\*Svenonius (2005)</span> explains the intended effect of this standard: CVs where every term is "context independent", i.e. self-contained and thus reusable. However, context independence is "not normally operative in classification schemes where the verbal headings are governed by the principle of hierarchical force."     

##### Typographic forms

- **Capitalization:** don't capitalize unless it's a proper noun
- **Punctuation:** minimize because it can interfere with sorting, searching, etc.
    - Reserve parentheses for qualifiers 
    - Use apostrophes for possessives and proper names
    - Diacritical marks are acceptable e.g. for loan words
 



#### How are terms related?

All relationships are **reciprocal,** which should be captured by the taxonomy software. Most relationships (except RT) are asymmetric. **Orphan terms** lack any relationship.

##### Equivalence & preferred terms

Synonyms, near-synonyms, lexical variants, and (when **generic posting** is practiced) child terms are subsumed into a single **preferred term** with **USE/USED FOR** or **U/UF.** Preferred terms are also called descriptors and headings; non-preferred terms are also called entry or lead-in terms.

- Only equate near-synonyms if warranted
- Include a term's full form and acronym:
    - Prefer full form unless acronym dominant in the domain
    - If acronym is dominant but ambiguous, prefer full form 
- Include a term's popular and scientific forms; prefer whichever is dominant in the domain
- Consult lexical authorities for official spelling
- Prefer whichever is more popular for loan words versus translations 
- Romanization converts non-Roman characters to Roman characters via a table, which may not align with informal but dominant Romanizations; prefer the dominant form
- For compound terms where postcoordination is intended, and component terms have no other use: 
    - _coal mining: USE coal AND mining_
    - _coal: USED FOR coal AND mining_
    - _mining: USED FOR coal AND mining_

##### Hierarchy

See [discussion of hierarchy, above.](#hierarchical-(1g))

##### Association

Associative relationships should be recorded between terms that are needed to explain each other, or readily evoke each other, or are etymologically related, or are derived one from the other. Although they may be disambiguated in a graph database, in a thesaurus the following relationships (and more) are collapsed under RELATED TERM/RT:

- Cause/effect
- Process/agent or counteragent
- Action/property
- Action/target
- Action/product
- Raw material/product
- Concept/property
- Concept/origins
- Concept/measurement unit or measurement mechanism
- Concept/antonym
- Object/property
- Object/origins
- Object/measurement unit or measurement mechanism
- Discipline/practioner



#### CV elements by impact on IR

Per ANSI/NISO Z39.19 (p. 16), 

- **Recall** is improved by indicating relationships:
    - Equivalence relationships/preferred terms
    - Preferred term form
    - Associative relationships/related terms
    - Classified and hierarchical relationships
    - Postcoordination
    - Concept mapping/clustering
- **Precision** (minimizing false hits) is improved by controlling ambiguity:
    - Parenthetical qualifiers
    - Broader and narrower term hierarchical relationships
    - Compound terms
    - Precoordination 


### Testing CVs

The usability and IR performance of CVs should be tested:

- for conformance with standards;
- heuristically (by expert opinion); and/or 
- through user research (e.g. comparing to user-generated card-based affinity models).


### Documenting CVs

Documentation should cover:

- Purpose of CV
- Scope of CV
- Explanation of conventions and rules governing
    - Term choice
    - Term form (punctuation, abbreviation, etc.)
    - Filing and sorting
    - Navigation
- Total #terms and #entry_terms; #entry/#total is a user accessibility metric
- Date last updated, and [update policies](#maintaining-cvs)
- Contact information


### Maintaining CVs

CVs must be updated to stay valid, so there should be plan for reviews at defined intervals. 

Updates may include:

- Auditing validity of links
- Adding new terms
- Deleting over- and underused terms (may be proactive or retroactive)
- Adding **history notes (HN)** that indicate date and nature of modifications; they are important for provenance


### Managing CVs

ANSI/NISO Z39.19 (pp. 99-102) lists desirable features of CV management software:

- Notes: SN, HN
- Relationships: U/UF, BT/NT, RT
    - Audit validity of relationships
    - Create reciprocal relationships automatically
- Allow definition of fields for metadata, other semantic relationships
- Display contents by hierarchy and alphabet
- Open source and hardware independent
- Usable and well-documented
- Generates reports
- Accessible to multiple users


### Displaying CVs

#### Overarching forms

- **Alphabetic:**
    - Alphabetic listing
    - Flat format (only shows immediate BT/s & NT/s)
- **Graphic** 
- **Hierarchical:**
    - Tree AKA systematic display, classified display:
        - Top term
    - Multilevel:
        - Symbols and generic structures (GS)
        - BT1, BT2, ... BTn
        - NT1, NT2, ... NTn
    - Two-way hierarchical structure
    - Broad categories
    -  Faceted display
- **Permuted** AKA rotated display:
    - KWIC
    - KWAC
    - KWOC

#### Organization within or across forms

- Term detail
    - Relationships can be organized:
        - Alphabetically or logically
        - Into categories, under node labels (not used for indexing)
    - Depth of U/UF can be adjusted to compensate for hierarchy display type (?)
- Sorting:
    - Prefer word-by-word ("nothing before something")
    - Sort numbers by magnitude
    - Ignore commas in inverted USE references
    - Treat parentheses around glosses as special characters; don't ignore

#### Format considerations

- **Print:** Minimize double lookups, and use a running header to indicate position
- **Screen:** 
    - UF principles
    - Keyword search
    - Consider accessibility
    - Term detail
    - Display hierarchy
    - Pick list
- **Web,** a subset of screens: 
    - Path hierarchy display: show the file path (URL) of the current page
    - Path hierarchy in context: show all file paths together (e.g., [PKB index)](../../pkb.html) 
    - Facilitate browsing with scroll bars, arrows, hyperlinks, [+] (click to expand)



## Interoperability

Per ANSI/NISO Z39.19, the need for interoperability arises from different sources:

- Users want to metasearch across different IR systems
- Users are multilingual [(see ISO 5964)](https://www.iso.org/standard/12159.html)
- Two databases, indexed differently, must be merged
- A content object must be indexed with a CV developed for a different domain

Interoperability may be approached in different ways:

- Combine CVs
- Create a micro CV
- Create a map/crosswalk between CVs

Interoperability data may be stored in:

- Authority records
- Vocabulary mappings
- Semantic networks
- Lexical databases, e.g. WordNet





# Sources

Abrahamson, E. & Freedman, D. H. (2008). _A perfect mess: The hidden benefits of disorder --- How crammed closets, cluttered offices, and on-the-fly planning make the world a better place._ New York City, NY: Back Bay Books.

AfterHoursProgramming.com. (n.d.) IA tutorial. Retrieved from [http://www.afterhoursprogramming.com/tutorial/Information-Architecture/Overview/](http://www.afterhoursprogramming.com/tutorial/Information-Architecture/Overview/)

ANSI/NISO. (2005). Z39.19-2005: Guidelines for the construction, management, and format of monolingual controlled vocabularies. Retrieved from [http://www.niso.org/apps/group_public/download.php/12591/z39-19-2005r2010.pdf](http://www.niso.org/apps/group_public/download.php/12591/z39-19-2005r2010.pdf)

Connolly, T. & Begg, C. (2015). _Database systems: A practical approach to design, implementation, and management_ (6th ed.). New York City, NY: Pearson Education.

Hedden, H. (2016). _The accidental taxonomist_ (2e). Medford, NJ: Information Today, Inc.

Hjørland, B. (2011). Theoretical clarity is not "Manicheanism": A reply to Marcia Bates. _Journal of Information Science,_ 37(5), 546-552. Retrieved from [http://pure.iva.dk/files/31053333/JIS_1568_v3.pdf](http://pure.iva.dk/files/31053333/JIS_1568_v3.pdf)

Jacob, E. K. (2004). Classification and categorization: a difference that makes a difference. _Library Trends,_ 52(3), 515. Retrieved from [https://pdfs.semanticscholar.org/774e/ab27b22aa92dfaa9aeeeafbe845058e85f58.pdf](https://pdfs.semanticscholar.org/774e/ab27b22aa92dfaa9aeeeafbe845058e85f58.pdf)

NISO. (1997). TR02-1997: Guidelines for indexes and related information retrieval devices. Retrieved from [http://www.niso.org/publications/tr/tr02.pdf](http://www.niso.org/publications/tr/tr02.pdf)

Pomerantz, J. (2015). _Metadata._ The MIT Press Essential Knowledge Series. Boston, MA: MIT Press.

Rosenfeld, L., Morville, P., & Arango, J. (2015). _Information architecture for the web and beyond_ (4e). Sebastopol, California: O'Reilly Media.

Svenonius, E. (2005). Design of controlled vocabularies. _Encyclopedia of Library and Information Science, 45_(10), 82–109. Retrieved from [http://polaris.gseis.ucla.edu/gleazer/260_readings/Svenonius.pdf](http://polaris.gseis.ucla.edu/gleazer/260_readings/Svenonius.pdf)

UX Booth. (2015). A complete beginner's guide to information architecture. Retrieved from [http://www.uxbooth.com/articles/complete-beginners-guide-to-information-architecture/](http://www.uxbooth.com/articles/complete-beginners-guide-to-information-architecture/)

Wurman, R. S. (2014). Give yourself permission to follow your nose [video]. Retrieved from [https://www.youtube.com/watch?v=SDm1zXxpkr8](https://www.youtube.com/watch?v=SDm1zXxpkr8)

Wurman, R. S. (1990). _Information anxiety._ New York City, NY: Bantam Books.

Wurman, R. S. (1997). _Information architects._ New York City, NY: Graphis Inc.

Wyllys, R. E. (2000). Information architecture. Retrieved from [https://www.ischool.utexas.edu/~l38613dw/readings/InfoArchitecture.html](https://www.ischool.utexas.edu/~l38613dw/readings/InfoArchitecture.html)

Zeng, M. (n.d.). 4.3 Hierarchical relationships. In _Construction of controlled vocabularies: A primer._ Retrieved from [http://marciazeng.slis.kent.edu/Z3919/43hierarchy.htm](http://marciazeng.slis.kent.edu/Z3919/43hierarchy.htm)


# What are information systems (IS)?

Per Annabi and McGann (2014a), information systems (IS) consist of three components, represented as the **IS triangle:**

- **Business processes:** a predefined way in which an organization performs its functions
- **People:** various [stakeholders](project-management.html#stakeholder-analysis)
- **IT:** hardware and software

IS professionals work with IT professionals to create IS systems. IT people build the IT, but IS people identify the problems that an IS system may solve or the opportunities that an IS system may create, and coordinate implementation of an IS that meets the organization's needs. 

## Evolution of IS
    
Per Annabi and McGann (2014b):

- **Computational systems** were very narrow and focused on automating tedious tasks, e.g. keeping a ledger
- **Functional systems** were slightly broader and place automated tasks in context of all related tasks, e.g. accounting and financial reporting
- **Enterprise systems** are broadest and try to integrate processes across functional areas, e.g. recognizing the relationship between accounting and HR, as well as bridging internal and external users

## What are MIS?
    
There are three major MIS in an enterprise, collectively referred to as an organization's **IS architecture.**

### Enterprise systems (ES)

ES are used to perform the daily tasks of a business.

#### Enterprise resource planning (ERP) 

Internal. Major providers are SAP, Oracle, Sage Group, Microsoft Dynamics, SSA Global Technologies. ERP systems eliminate data inconsistencies; enable reporting that covers the entirety of a business function or process; and supports better availability of data. ERPs entail costs (purchase, training, data conversion, consulting fees, etc.) and also enforce standardization of business processes, which users may resist.

#### Customer relationship management (CRM)

External interface; incl. sales, marketing, customer service, and analysis of such. CRM systems enable targeted marketing and experiences; help discover new customers; and support better customer service to attract and retain customers. However, their success depends on a customer-centric organizational culture, which can be hard to develop.

#### Supply chain management (SCM)

External interface; covers suppliers, manufacturers, distributors, and customers (presumably in a way that isn't redundant to the CRM??). SCMs help avoid waste in supply chain by identifying inefficiencies and improving forecasting. Their success depends on (1) trust among members of the supply chain, and (2) accurate models of the supply chain, which can be dauntingly complex.

### Business intelligence (BI)

See [notes on BI architectures.](BI-architectures.html) BI systems are used to inform strategic decisions, and include:

- Reporting
- Data mining
- Decision support systems (DSS)
- Expert systems (ES)
- AI systems

### Collaboration systems (CS)

CS are used to connect people, and include:

- Groupware
- Workflow
- Collaboration
- Social media
- Email
- Knowledge management
- Internet, extranet, intranet

## Other IS

- Digital asset management (DAM)
- Content management systems (CMS)
- Websites
- Catalogs
- Libraries
- Archives

        


# Implementing IS 

See [notes on project management](project-management.html) for complete description of phases and techniques employed across the systems development lifecycle (SDLC).

## Common pitfalls & best practices

Per Bloch, Blumberg, and Laartz (n.d.), IT projects often suffer major delays and budget overruns. Best practices to avoid or limit these unwelcome outcomes:

- Focus on strategy and stakeholders, not exclusively budget and scheduling
- Ensure talent is sufficient to execute the project
- Align team incentives with project incentives and have adequate team infrastructure
- Have excellent project management, including short delivery cycles and quality checks


    

    
    
# Sources

Annabi, H. & McGann, S. (2014a). Unit 1 -- What is MIS? In _The real deal on MIS._

Annabi, H. & McGann, S. (2014b). Unit 2 -- Enterprise systems: The benefits and challenges of integrating people, process and information technology. In _The real deal on MIS._

Bloch, M., Blumberg, S., & Laartz, J. (n.d.). Delivering large-scale IT projects on time, on budget, and on value. McKinsey & Company. Retrieved from [http://www.mckinsey.com/business-functions/digital-mckinsey/our-insights/delivering-large-scale-it-projects-on-time-on-budget-and-on-value](http://www.mckinsey.com/business-functions/digital-mckinsey/our-insights/delivering-large-scale-it-projects-on-time-on-budget-and-on-value)

# Prototyping

Prototyping is the practice of making a simple, quick, cheap, etc. versions of a future product, and getting user input on those versions before moving on to production. 

## Types of interface prototypes

Per Ambler, n.d. and my friend Clint:

<table>
<tr><th>Term</th><th>Alt Term</th><th>Definition</th></tr>
<tr><td>Sketch</td><td>Essential UI</td><td>Conceptual model of UI</td></tr>
<tr><td rowspan="2">Mock-up</td><td>Sketch</td><td>Manual rendering of a screen's appearance</td></tr>
<tr><td>Concrete prototype</td><td>Digital rendering of a screen's appearance</td></tr>
<tr><td colspan="2">Wireframe</td><td>Shows how screens interrelate</td></tr>
</table>

### Sketches

- [http://www.agilemodeling.com/artifacts/essentialUI.htm](http://www.agilemodeling.com/artifacts/essentialUI.htm)

### Mock-ups

- [http://agilemodeling.com/artifacts/uiPrototype.htm](http://agilemodeling.com/artifacts/uiPrototype.htm)

### Wireframes

## Interface prototyping tools

- [http://uxmag.com/articles/bringing-relevant-content-into-your-designs](http://uxmag.com/articles/bringing-relevant-content-into-your-designs)
- [https://www.cooper.com/prototyping-tools](https://www.cooper.com/prototyping-tools)








# Types of interfaces

## Reporting dashboards

A summary of best practies from Eckerson and Hammond (2011):

- Focus on:
    - User needs
    - Actions! not views
        - Actions that require data
        - Actions that data could suggest
    - Data quality
- Methods and workflow:
    - Get user feedback on prototypes
    - Plan to interate continuously
        - E.g., add more complexity as users become fluent
    - Create and use templates (for designer-helpful efficiency and user-friendly consistency)
- Design principles:
    - Group related elements, maybe using tabs
    - Balance visual sparsity and information density, both desirable
    - "Ideally, users should be able to view all pertinent data at a glance without clicking. Then, they should be able to view details of any top-level metric in three clicks or fewer."
    - Keep data, not decorative elements, at the forefront

[http://uxmag.com/articles/bringing-relevant-content-into-your-designs](http://uxmag.com/articles/bringing-relevant-content-into-your-designs)
    
    
    

## Interactive interfaces

### Forms

Per Wroblewski (n.d.), forms are used in a variety of scenarios both on- and offline:

- Shop, conduct transactions
- Log in, gain access
- Input data, give feedback

#### Requirements and validation

- **Usability Testing:** Errors, issues, assists, completion rates, time spent 
per task, satisfaction scores
- **Field testing:** Sources used, environment, context
- **Customer Support:** Top problems, number of incidents
- **Web Conventions Survey:** Common solutions, unique approaches
- **Site Tracking:** Completion rates, entry points, exit points, elements utilized, data entered 
- **Eye Tracking:** Number of eye fixations, length of fixations, heat maps, scan paths 

#### General best practices

- Minimize input with smart defaults, inline validation, forgiving inputs
- Provide a progress indicator
- Provide a consistent voice across pages of a form
- Indicate errors, available help, reasons for requesting data, success

#### Best practices by form element

##### Layout 

Label positions:

- Label above field: fastest completion times but may be harder to associate label with appropriate field; use if the form represents a familiar task and if _vertical space_ isn't an issue
- Label to left of field: better for associating labels with appropriate fields, but increases (as much as doubles!) form completion time
    - Right-aligned: Minimizes vertical space and distance the eye must move, but jagged left edge is harder to process
    - Left-aligned: Most time-consuming but labels are easily read
    
Content groupings 

##### Input Affordances 

Per Norman (2013), an object has attributes; a person has capabilities; when a person's capabilities combine with an object's attributes, this is an affordance and we can say that the object affords action X. For example, a rock is hard; many people can move their arms and grip with their hands; therefore, a rock affords hitting something and causing damage. More coloquially, in web design an interface element that evokes and invites interaction from users.

- Based on the total number of fields and their relative incidence, choose to mark fields as required or optional 
- Field lengths should be right-sized for their content (an affordance), but fairly consistent across a form (not wildly varying lengths)


- Formats   

##### Actions 

Primary 

Secondary 

##### Help & Tips 

##### Visual Hierarchy


### Multimedia









# Sources

## Cited

Ambler, S. (n.d.). User interface (UI) prototypes: An Agile introduction. (n.d.). Retrieved from [http://agilemodeling.com/artifacts/uiPrototype.htm](http://agilemodeling.com/artifacts/uiPrototype.htm)

Eckerson, W., & Hammond, M. (2011). _Visual reporting and analysis._ TDWI Best Practices Report. Retrieved from [http://www.smartanalytics.com.au/pdf/Advizor-TDWI_VisualReportingandAnalysisReport.pdf](http://www.smartanalytics.com.au/pdf/Advizor-TDWI_VisualReportingandAnalysisReport.pdf)

Norman, D. (2013). _The design of everyday things._ New York City, NY: Basic Books.

Wroblewski, L. (n.d.). Best practices for form design [presentation slides]. 

## References

- [Questionnaire bank for usability research](http://www.usabilitynet.org/tools/r_questionnaire.htm)
- [User Experience Questionnaire](http://www.ueq-online.org/)
- [Usability.gov](https://www.usability.gov/)
- [Colorgorical palette generator](http://vrl.cs.brown.edu/color)
- [HTML color codes, pickers, etc.](http://htmlcolorcodes.com/)
- [Font Pair](http://fontpair.co/)

## Read

- [Coursera - Design Principles](https://www.coursera.org/learn/design-principles)
- [Coursera - Graphic Design](https://www.coursera.org/learn/fundamentals-of-graphic-design)

## Unread

- [7 Rules for Creating Gorgeous UI](https://medium.com/@erikdkennedy/7-rules-for-creating-gorgeous-ui-part-1-559d4e805cda)
- [Graphic Design tutorial](http://www.afterhoursprogramming.com/tutorial/Graphic-Design/Introduction/)
- [Usability tutorial](http://www.afterhoursprogramming.com/tutorial/Usability/Introduction/)
- [Design for programmers](https://blog.prototypr.io/design-for-programmers-d38c56982cd0#.cx4hjk2o1)
- [In defense of eye candy](http://alistapart.com/article/indefenseofeyecandy)
- [System Usability Scale](https://www.usability.gov/how-to-and-tools/methods/system-usability-scale.html)
- [Buttrick's practical typography](http://practicaltypography.com/index.html#toc)
- [A short introduction to graphic design history](http://www.designhistory.org/index.html)
- [Users’ Computer Skills: Worse Than You Think](https://www.nngroup.com/articles/computer-skill-levels/)
- [Dark Patterns: fighting user deception worldwide](http://darkpatterns.org/)
- Eyal, N. _Hooked: How to Build Habit-Forming Products_.
- Weinschenk, S. _100 Things Every Designer Needs to Know About People_.

# Sources

## References

## Read

- [Advice given to journalists:](http://www.politico.com/magazine/story/2015/06/brian-williams-how-not-to-become-118847) don't go on TV, don't talk about yourself, and don't trust your memory.
- [What Google learned from its quest to build the perfect team](https://www.nytimes.com/2016/02/28/magazine/what-google-learned-from-its-quest-to-build-the-perfect-team.html?_r=0)
- [Why groups fail (Hint: for the same reasons that nations fail)](http://evonomics.com/why-groups-fail-nations-fail-david-sloan-wilson/)

## Unread

- [http://www.labornotes.org/blogs/2011/08/how-walmart-trains-managers](http://www.labornotes.org/blogs/2011/08/how-walmart-trains-managers)

# Sources

## Unread

- [Coursera - Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning)
- [Top 10 Data Mining Algorithms in Plain English](http://rayli.net/blog/data/top-10-data-mining-algorithms-in-plain-english/)
- [Has deep learning made traditional ML irrelevant?](http://www.datasciencecentral.com/profiles/blogs/has-deep-learning-made-traditional-machine-learning-irrelevant)
- [How Google used artificial intelligence to transform Google Translate—and how machine learning is poised to reinvent computing itself](http://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html)
- [https://en.wikipedia.org/wiki/Data_mining](https://en.wikipedia.org/wiki/Data_mining)
- [Essentials of machine learning algorithms](https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/)
- [A tour of ML algorithms](http://www.datasciencecentral.com/profiles/blogs/a-tour-of-machine-learning-algorithms-1)
- [Implementation of 17 classification algorithms in R](http://www.datascience-zing.com/blog/implemetation-of-17-classification-algorithms-in-r-using-car-evaluation-data)
- [Map of Data Mining Techniques](http://saedsayad.com/data_mining_map.htm)
- [R2D3: Visual Intro to Machine Learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)
- [Undergrad ML syllabus](https://piazza.com/umd/fall2015/cmsc422/home)
- [Complete tutorial on tree based modeling](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/)
- [Machine learning in a week](https://medium.com/learning-new-stuff/machine-learning-in-a-week-a0da25d59850#.r517qamav)
- [Machine learning in a year](https://medium.com/learning-new-stuff/machine-learning-in-a-year-cdb0b0ebd29c#.zg6ty8x8x)
- [In-depth introduction to machine learning (15 hours)](https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/)
- [Power to the People: How One Unknown Group of Researchers Holds the Key to Using AI to Solve Real Human Problems](https://medium.com/@atduskgreg/power-to-the-people-how-one-unknown-group-of-researchers-holds-the-key-to-using-ai-to-solve-real-cc9e75b1f334#.2k74ego8p)
- [DataCamp - Kaggle Python tutorial on Machine Learning](https://www.datacamp.com/courses/kaggle-python-tutorial-on-machine-learning)
- [How to use data analysis for machine learning](http://www.r-bloggers.stfi.re/how-to-use-data-analysis-for-machine-learning-example-part-1/?sf=roexglx#aa)
- [GitHub - Dive into Machine Learning](https://github.com/hangtwenty/dive-into-machine-learning)
- [Udacity - Intro. Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120)
- [Big Data U - Machine Learning with Python](https://bigdatauniversity.com/courses/introduction-to-machine-learning-with-python/)
- [DataCamp - Machine Learning with R](https://www.datacamp.com/courses/introduction-to-machine-learning-with-r)
- [Big Data U - Machine Learning: Cluster Analysis](http://bigdatauniversity.com/courses/machine-learning-cluster-analysis/)
- [A concise history of neural networks](https://medium.com/@Jaconda/a-concise-history-of-neural-networks-2070655d3fec#.b0ip33aeq)
- [A mostly complete chart of neural networks](http://www.asimovinstitute.org/neural-network-zoo/)
- [Understanding neural networks with TensorFlow Playground](https://cloud.google.com/blog/big-data/2016/07/understanding-neural-networks-with-tensorflow-playground)
- [10 deep learning terms explained in simple English](http://www.datasciencecentral.com/profiles/blogs/10-deep-learning-terms-explained-in-simple-english)
- [Deep learning demystified](http://www.datasciencecentral.com/profiles/blogs/deep-learning-demystified)
- [Deep learning simplified (video series)](https://www.youtube.com/channel/UC9OeZkIwhzfv-_Cb7fCikLQ)
- [Deep learning textbook](https://www.youtube.com/channel/UC9OeZkIwhzfv-_Cb7fCikLQ)
- [16 free machine learning books](https://hackerlists.com/free-machine-learning-books/)Import notes from Hala's class; be sure to cover:

- Organizational architecture
- Organizational strategy & strategic leadership
- Business processes & the improvement thereof
- Project management

# Sources

## References

- [http://www.nwlink.com/~donclark/contents.html:](http://www.nwlink.com/~donclark/contents.html) "your window to learning, training, leadership, design and all matters related to improving human performance"

# What is metadata?

- Metadata record

## Types of metadata

### Descriptive metadata

- Unique identifier
    - For books
        - Library of Congress Classification (LCC)
        - Dewey Decimal System
        - ISBN
    - For web, URI
        - URL

### Administrative metadata

- technical
- structural
- provenance
    - WC3
- preservation
    - PREMIS
- rights
    - CC Rel
    - RightsDeclarationMD
- METS

### Use metadata

- data exhaust
- logs
- paradata

## Metadata association models

Per Duval et al. (2002), thinking from the perspective of a metadata manager (not the user perspective), metadata may be:

- **Embedded** within the markup of a resource, either automatically (as with cameras) or by a person, trained or untrained in resource description'
- **Associated** with a resource, but stored and managed in a separate format;
- **Third-party metadata**, maintained without any connection to the underlying resource.

Per Pomerantz (2015), metadata stored internally is authoritative but static; metadata stored externally is flexible and required for resource discovery.




# Metadata schemas AKA element sets AKA standards

- Elements
- Values
    - Dates
        - ISO 8601
    - Format
        - MIME types

## Dublin Core

- Core terms
- Qualifiers



# Metadata encoding schemes

## Controlled vocabularies

- Library of Congress Subject Headings
- Authority files
    - VIAF
- Thesaurus
    - J. Paul Getty thesaurus of geographic names
    - network topologies
        - ring
        - star
        - tree
    - "use for", "part of", "instance of"
- Ontologies
- [Tagging]

## Syntaxes

AKA structured data??

There should be a distinction here between data serialization formats (e.g. JSON, XML, YAML) and data models (RDF); not sure how this relates to the concepts of file format, markup, and encoding.

### HTML 

## XML

- DTD
- DOM

### RDF

"... an additional layer on top of XML that is intended to simplify the reuse of vocabulary terms across names" (Duval et al., 2002)

### OWL??

### Relational databases

### DCMI abstract model



# Interoperability

Per Duval et al. (2002), "Semantics is about meaning; syntax is about form. Agreements about both are necessary for two communities to share metadata." The semantic and machine ioteroperability of metadata is facilitated by adherance to the following principles:

- **Modularity:** thematic modules populating clearly delineated namespaces in objects across the web (in other words, the difficulty is not in combining different metadata element sets; the difficulty is indicating when and where different MES are being used)
- **Extensibility:** it should be possible to customize a MES by adding elements beyond those in the base schema, which could then be selectively ignored by any interoperability mechanisms
- **Refinability:** 
    - elements should have predefined qualifiers so that users can choose their level of detail
    - elements should include recommended controlled vocabularies or encoding schemes, that users may follow or not
- **Multilingualism:** strive to make information globally accessible through
    - internationalization: the development of 'neutral' standards
    - localization: the adaptation of international standards to local needs

There are practical considerations that arise in pursuit of these principles:

- "The purpose of an application profile is to adapt or combine existing schemas into a package that is tailored to the functional requirements of a particular application, while retaining interoperability with the original base schemas". Application profiles must handle:
    - Cardinality enforcement (optional/conditional/mandatory)
    - Value space restrictions
    - Relationship and dependency restrictions
    - Namespace declarations
- It is important to keep syntax standards and semantic standards independent
- Tokens are machine-readable, so should be invariant; labels are meant for humans, and should vary locally
- People are working on development of metadata registries to help metadata users access metadata standards, syntax schemas, and controlled vocabularies
- Software should compensate for the tendency of users to fill out irrelevant metadata elements or leave relevant ones empty
- Subjectivity of metadata 
- Balancing cataloguing (which produces metadata) and searching (based on machine-generated indexes); "Between these two extremes lies a broad range of metadata creation that can be automated to some degree, and which can be expected to grow in importance as advances in such areas as natural language processing, data mining, profile and pattern recognition algorithms become more effective.



# Semantic web

- SEO & keyword stuffing
- linked data
    - defererencability
    - sameAs
    - Facebook's OGP

## Schema.org

- microdata


    
# Sources

Duval, E., Hodgins, W., Sutton, S., & Weibel, S. L. (2002). Metadata principles and practicalities. _D-Lib Magazine, 8_(4), 16-33.

Pomerantz, J. (2015). _Metadata._ The MIT Press Essential Knowledge Series. Boston, MA: MIT Press. 



_The following notes are largely based on Steirn (1999) and [Scott Page's](https://vserver1.cscs.lsa.umich.edu/~spage/) Coursera class on [Model Thinking.](https://www.coursera.org/course/modelthinking)_

_Note that there's a lot of overlap between depicting a system and [implementing one;](project-management.html) between depicting a process and [improving it;](process-improvement.html) between modeling a problem and [analyzing it;](models.html) between modeling a system and [modeling a database;](databases.html#database-design) between model forms and [knowledge organization structures.](information-architecture.html#kos-by-structure)_



# Overview of modeling

Because a model is a representation of a system, the model needs to include contextual metadata clarifying the date and/or version of the system it describes.


## Why model?

Per Scott Page, modeling helps us:

- **Be an intelligent citizen of the world.** Models are everywhere, so full participation requires knowing them. They are "the new lingua franca" in nonprofits, businesses, politics, academia. 
- **Be a clearer thinker.** Models are "wrong, but useful"; people make better decisions using a formal model versus either a single model, or multiple casual models. “You are a modeler … But typically, it is an implicit model in which the assumptions are hidden, their internal consistency is untested, their logical consequences are unknown, and their relation to data is unknown.”
    - **Work through all the logical possibilities** for parts; relationships between parts; logic
    - **Identify the general class of outcome:** equilibrium, cycle, random, complex
    - **Identify logical bounds:** "Models give us the conditions under which we can adjudicate" between ideas, e.g. contradictory folk proverbs. _When_ are "two heads better than one"? *When* do "two cooks spoil the broth"?
    - **Reduce complexity.**
- **Understand and use data,** turning it into knowledge.
    - **See patterns** and understand where they come from
    - **Make predictions** (point or range/bounds)
    - **Make retrodictions** (predict the past, when data is absent or when testing predictive models)
    - **See if something is missing,** e.g. orbits makes us think that there’s a planet beyond our sight
    - **Strategize data collection**
    - **Estimate unobservable parameters** & calibrate based on data
- **Better decide, strategize, design, and act.**
    - **Make decisions** (e.g., the Monty Hall door selection problem)
    - **Consider counterfactuals**
    - **Identify and rank levers:** where do we intervene to have an impact?
    - **Design experiments and institutions**
- **Communicate what we know very simply.** “[M]odels can be the focal points of teams involving experts from many disciplines”
- **Discover new things:** "Models are fertile", i.e., they have multiple and unexpected uses. “Models can surprise us, make us curious, and lead to new questions. This is what I hate about exams. They only show that you can answer somebody else's question, when the most important thing is: Can you ask a new question?”

## Model typologies

Models can have a general form (e.g. entity relationship diagram) somewhat corresponding to [information structures,](information-architecture.html#information-structures) but be executable in different notations (e.g. Chen, crow's foot). This page organizes models by **form and notation,** following Steirn (1999), who seems very similar to Dennis et al. (2012):

- **Functional** (==flow chart?) models capture processes:
    - Use-case diagrams
    - Activity diagrams
- **Structural** (==ERD?) models capture objects, their attributes, and their interactions:
    - CRC cards
    - Class diagrams
- **Behavioral** (==object-oriented?) models capture even more detail about interactions:
    - Interaction diagrams
    - Behavioral state machines
    - Crude analysis

Smartdraw.com (n.d.): 

- **Graphs** represent entities and relationships
    - Venn
    - Flowchart
    - Network diagram
    - Genograms
- **Charts** represent data
    - Histogram
    - Line graph
- **Schematics** represent the elements and architecture of a system
    - Circuit diagram
    - Floorplan
    
Models could also be grouped by the **business problems they solve** (e.g. poor quality, lack of strategic direction, etc.); as they appear in **stages of a process/lifecycle** (e.g. identifying a problem, analyzing a problem, brainstorming solutions, etc.); by **domain of origin** (models have been developed sequentially or in parallel by Taylorists, postwar Japanese manufacturers, industrial engineers, social scientists, and software developers); by **practice area** (different models may tend to be used in UX, database development, consulting, requirements management, etc.); by **methodology** (e.g. Agile, SDLC); or by **notation** (e.g. UML, crow's foot).








# Models by form and notation

## Flow charts

AKA decision flow charts, logic flow charts, and logical decision flow charts. Flow charts model **decisions,** a type of process. Languages that model processes more generally can also represent decisions.

The basic elements of flow charts are available in MS Visio's language level diagrams stencil:

- **Parallellograms** for inputs
- **Diamonds** for decisions
- **Rectangles** for functions
- **Hardcopy symbol** (rectangle with wavy bottom edge) for outputs

### Nassi-Schneiderman (N-S) diagram

AKA Chapin charts, structograms, structured flowcharts. Per Nassi and Shneiderman (1973), "We propose a flowchart language whose control structure is closer to that of language amenable to **structured programming:"**

<img src="../ILLOS/N-S-diagram.png" width="250px">



## Entity-relationship diagrams

All ERDs capture the entities in a system, along with their attributes and interrelationships; enhanced ERDs include superclasses and subclasses. 

### ERDs for databases

Per Dybka (2014), there are many notation styles:

- **Crow's foot**
- **Martin**
- **UML**
- [Chen](../ILLOS/chens.png)
- [Baker](../ILLOS/bakers.png)
- [Arrow](../ILLOS/arrow.png)
- [IDEF1X](../ILLOS/idef1x.png)

#### Bachmann/crow's foot notation

This is a notation that describes the optionality/modality/participation and cardinality/multiplicity of a relationship, so it can be used within other modeling systems. Crows-foot notation annotates relationships with the symbols:

- **Open circle** for optional participation (zero to many)
- **Bar** for mandatory participation (one to many)
- **Crow-foot** for many
- **Bar** for one

Entity A is on the left, entity B is on the right. They are connected with an annotated line. Annotations on the right side of the line describe how A relates to B: for a single row in A, how many rows minimum and how many rows maximum could appear in B? Annotations on the left side of the line describe how B relates to A: for a single row in B, how many rows could appear in A?

<img src="../ILLOS/crowsfoot.gif" width="350px" align="left">

<img src="../ILLOS/EERD.png" width="280px" style="padding-bottom:60px;">

#### UML notation

Predates ER notation, but increasingly popular as a database modeling language; see [notes on UML.](UML.html)

<img src="../ILLOS/UML-EERD.png" width="450px">


### Data flow diagrams

The Gane-Sarson and Yourdon-Coad notations are slightly different, but both depict the flow of data through a system using the following elements:

- **Circles** to represent system processes that transform the data
- **Lines** to represent data flows, with descriptive labels and arrows for directionality
- **Open rectangles** to represent internal data stores
- **Rectangles** to represent external interactors

![](../ILLOS/DFD.jpg)



## Object-oriented modeling

OO models show **inheritance** as well as decisions, relationships, and processes. Per Steirn (1999), several earlier methods (Shlaer/Mellor, Rumbaugh's Object Modeling Technique (OMT), Booch) were subsumed by UML in 1997.

### Activity diagrams

Similar to flowcharts and data flow diagrams, since they focus on depicting a _process_ corresponding to a specific [use case](project-management.html#use-cases-&-requirements-definition-reports) or [usage scenario.](project-management.html#use-scenarios) Activity diagrams use the following UML elements:

- **Rounded rectangle** to indicate activities
- **Rectangle** to capture explanatory notes
    - **Dotted line** to connect notes to relevant entities
- **Oval** to indicate that a use case is being covered
    - **Rake** annotating an activity, to indicate it's described by its own activity diagram
- **Lines** for flows
    - **Black bar** for forks and joins (to capture parallel processes: AND)
    - **Diamonds** for conditional branching and merging (OR)
        - **Bracketed text annotation** of a flow to denote a logical condition that must be satisfied 
    - **Solid circle** to indicate the starting node
    - **Solid circle with halo** to indicate terminal node/s   
    - **Halo around X** to indicate unsuccessful end of process
- **Swimlanes** to capture which actor performs the activity

<img src="../ILLOS/activity-diagram.jpg" width="580px"/>









# Sources

Ambler, S. (n.d.). Agile models distilled: Potential artifacts for agile modeling. Retrieved from [http://www.agilemodeling.com/artifacts/](http://www.agilemodeling.com/artifacts/)

Ambler, S. (n.d.). UML 2 activity diagrams: An Agile introduction. Retrieved from [http://www.agilemodeling.com/artifacts/activityDiagram.htm](http://www.agilemodeling.com/artifacts/activityDiagram.htm)

Dybka, P. (2014). ERD notations in data modeling. Vertabelo Academy. Retrieved from [http://www.vertabelo.com/blog/technical-articles/comparison-of-erd-notations](http://www.vertabelo.com/blog/technical-articles/comparison-of-erd-notations)

Dennis, A., Haley Wixom, B., & Tegarden, D. (2012). Requirements determination. In _Systems analysis and design: An object oriented approach with UML_ (4th ed., pp. 109–152). Hoboken, NJ: Wiley. 

Meadows, D. H., & Wright, D. (2008). _Thinking in systems: A primer._ White River Junction, Vt.: Chelsea Green Pub.

Nassi, I., & Shneiderman, B. (1973). Flowchart techniques for structured programming. _SIGPLAN Not., 8_(8), 12–26. Retrieved from [https://www.cs.umd.edu/hcil/members/bshneiderman/nsd/1973.pdf](https://www.cs.umd.edu/hcil/members/bshneiderman/nsd/1973.pdf)

Smartdraw.com. (n.d.). Diagrams. Retrieved from [https://www.smartdraw.com/diagrams/?exp=ste](https://www.smartdraw.com/diagrams/?exp=ste)

Stiern, K. (1999). Comparison of diagramming methods. Retrieved from [http://www.umsl.edu/~sauterv/analysis/dfd/DiagrammingMethods.html](http://www.umsl.edu/~sauterv/analysis/dfd/DiagrammingMethods.html)


_The following notes are largely based on [Scott Page's](https://vserver1.cscs.lsa.umich.edu/~spage/) Coursera class on [Model Thinking.](https://www.coursera.org/course/modelthinking) Another excellent resource is the Farnam Street Blog roundup of [mental models.](https://www.farnamstreetblog.com/mental-models/)_

_For information systems-related applications of modeling, see [notes on system and process modeling;](modeling.html) for business applications of models as analytic frameworks, see [notes on project management](project-management.html) and [process improvement.](process-improvement.html)_
 
 
 
 
# Models for ... 

## Decision-making 

We talk about decision-making models for normative reasons (to help us make better decisions) and positive ones (to predict or analyze others’ decisions)

Multi-criterion models (many features bundled together): 
Can plot options in multidimensional space and calculate least distance to ideal point; this is a spatial model. A preference is spatial if people prefer things “just right” rather than infinitely large or small.
Use a matrix and compare column by column; compute score or weighted score, e.g. MCRI decision

Probabilistic models (uncertainty in the world):
Probability basics:
Classical probability: easy to calculate, e.g. for a dice
Frequency: can estimate probability by counting lots of data; makes assumption of stationarity
Subjective probabilities: dangerous, subject to bias, make easily violate axioms of probability
Decision trees: write down payoffs and probabilities; also can infer what other people think p is based on their decisions, or calculate payoffs knowing probabilities
Value of information: calculate value without and with information; then take the difference
   
## Mechanism design

Formalizing institutions; designing the rules of the game by defining permitted actions and payoffs.

Hidden actions: Effort={0,1}; Outcome={Good,Bad}; p(Good|Effort=1)=1; p(Good|Effort=0)=p; c, cost of effort=1. Contract: pay M if Good, 0 if Bad→ Payoff(effort=1)=M-c and Payoff(effort=0)=pM; set M-c>pM → M>c/(1-p).

Hidden information: Ability={a,b}; cost(a)=c, cost(b)=C. Contract: pay M after k hours where M>kc, M<kC → k=M/C. Costly signaling; how does this apply to internships?

Auction design: From the perspective of the seller, the objective is to get as much money as possible for a fixed amount of goods. Three basic designs: ascending bid vs. sealed bid vs. second price bid. In second price bidding, people makes a sealed bid; the highest bid wins but they actually pay the amount of the second highest bidder. 
For an ascending bid, the winner may not reach their upper limit; the item sells at just over the upper limit of the person with the second highest WTP. 
For second price bidding, a rational bidder will bid their true WTP because this maximizes ‘net gain’: WTP - actual price paid. Prevents arms race dynamic.
In sealed bids, people tend to underbid; in fact, the optimal bid is WTP/2 .This turns out to be the expected value of the second highest bidder.
→ revenue equivalence thm

Mechanisms in public goods projects: 
Clarke-Groves-Vickery Pivot mechanism: each person states what the project is worth to them. If the sum of these claims is higher than the cost of hte project, then the project goes ahead with each paying only the minimum amount needed to make the project viable: e.g. person_1 pays max{cost - V2 - V3 … - Vn, 0}. → everyone has an incentive to reveal their true value, rather than over- or under-bidding.
All mechanisms for these sorts of problems have trade-offs.

## Prediction

Prediction at the individual level → “[W]e tend to pay undue attention to single model thinkers. We shouldn’t. We should heed the advice of those who think more subtly. Better still, we should learn many models ourselves.”
We’ve looked at three types of predictive models: categorical, linear, and Markov

Prediction by groups → Wisdom of crowds = f(individual’s accuracy, diversity of ideas)
Crowd’s error = Avg error - Variation

## Categorization

Categorical: data can be sorted into different ‘boxes’
Calculation of total variation: subtract mean from each data point, square and sum the results
Proper categories greatly reduce total variation
R2 = %variation explained = (total_variation_pre-categories - total_variation_with_categories)/tv_pc





# Models theorizing ... 
 
## Segregation
 
There's an identification problem with segregation: do we deliberately seek the company of those who are similar to us (sorting), or do we become like those with whom we associate (peer effects)?
 
### Measuring segregation

- **Index of dissimilarity**
    - o ≤ DI ≤ 1
    - b = #units-in-block; B = #total-units-across-blocks b/B = %units-in-block; and y, Y, y/Y → |b/B - y/Y|/2
    D- I for a region of blocks = sum of (#blocks*block’s-DI)
- **[Isolation index](https://en.wikipedia.org/wiki/Index_of_dissimilarity)**

### Thomas Schelling’s spatial segregation model

Agent-based model where people choose whether to move or stay depending on the composition of their immediate neighborhood. The deep insight is that a similarity threshold of 30% has an equilibrium (everyone happy) with segregation of 70%; and 50% tolerance leads to nearly 100% segregation, including borders made up of empty cells. Perhaps counterintuitively, when intolerance is set very high, the system won’t find an equilibrium since everyone is perpetually moving. 
Micromotives ≠ macrobehavior
I would want to augment this model, giving cell-classes their own mobility parameter.
Exodus tip: agent leaves because someone else exits the neighborhood
Genesis tip: agent leaves because someone else enters the neighborhood

### Granovetter’s model of peer effects 

In re: people’s decision to join in a social movement

N individuals; the jth individual has threshold Tj, and will join if T others do
‘tail wagging the dog’: a population needs someone with Tj=0 to get things going
collective action more likely to happen if: low thresholds, variation in thresholds
calculate avg. T for a population; avg. discontent doesn’t matter

### Standing ovation model

- builds off Granovetter model; adds information to peer effects
- T, threshold to stand, and T=f(Q) is a function of quality
- S, signal from peers, and S=Q+E is a function of quality and error
- If S > T, then stand; if more people, X%, stand, then stand
- Since Q+E>T is the decision rule, either Q↑ or E↑ increases the number of people standing
- Lower threshold ⇒ higher number of people standing
- Lower X% ⇒ higher numbers of people standing
- If Q<T, more stand
- Big groups more likely to stand together (local X%)
- Celebrities (people in front row, everyone sees them, they don’t see anyone) → wield large influence

## Aggregation

Aggregation is tricky; “more is different” (can’t just look at individual parts). So, we model to get the logic correct. e.g. water is made of molecules, where wetness comes from the hydrogen bond; cognition comes from connected neurons.

Additional readings:

- http://stattrek.com/sampling/sampling-distribution.aspx
- http://stattrek.com/probability-distributions/binomial.aspx
- https://en.wikipedia.org/wiki/Six_Sigma 

### Central Limit Theorem

[Aggregating actions or numbers] : If you add up many independent events with finite variance, the distribution should be bell-shaped with mean=N/2 for the binomial distribution (or mean=p*N, more generally) and sigma=sqrt(N)/2 for the binomial distribution (or sigma=sqrt(p(1-p)N), more generally).
p(x > 6*sigma) = 3.4 in a million. In production processes, the six sigma approach means that you shoot for a sigma such that your likelihood of exceeding some specific tolerance is 3.4 in a million.

### Game of Life

[Aggregating rules]  by John Conway, a mathematician in group theory:  
Dead → Come alive if three of your neighbors are alive
Alive → Die if you have fewer than two neighbors or more than three neighbors
Results: Self-organization into complex patterns; emergence of distinct functional entities, e.g. f-pentomino, that can be used for computation
    
### Cellular automata
    
[Aggregating families of rules] , developed by John von Neumann and covered extensively in Wolfram, S. A New Kind of Science.
For 1D cellular automata models, we can visualize them on a grid treating each row as an instance of time
From the perspective of each cell, there are 8 possible states for its immediate neighborhood
Wolfram shows that 1D models, depending on their governing rules, exhibit all forms of behavior: equilibrium, oscillation, pure randomness, and complexity → “an information theoretic universe”?
There are a total of 256 rules: 8 neighborhood states, 2 possible responses → 28 = 256
Langton’s lamba: for a given set of rules, what % tell cells to switch on? e.g. ⅛ or ⅞

### Aggregating preferences

[Aggregating preferences] Politics and economics:  
How do we represent preferences? Through inequalities: preference rankings/orderings
How do we discern preferences? Revealed or stated
Usually assume rational, transitive preferences, but even so collective preferences may be nontransitive (Condorcet paradox)
   
## Individual human behavior
    
Murray Gell-Mann: “Imagine how difficult physics would be if electrons could think”; people=purposeful, diverse.

“Three attributes of physical systems (1) simple parts (2) interacting in large numbers (3) that follow fixed rules render physical models amenable to mathematics … Systems that include people as actors– and these include ecological systems – lack the three attributes that produce regularity. The parts of these systems aren’t simple: People are sophisticated, multi-dimensional, and capable of a range of behaviors. We march to our own drummers. Further, though billions of people exist, we interact in small to moderate sized groups. Finally, we don’t follow the same fixed rules. Unlike carbon atoms, we learn. We adapt. We do crazy things.”

### Rational 

== optimizing agents who have goals; can be represented by objective function.
Functions of form X(C-X) are optimized when C=X-C → C=X/2 
Rationality in no way assumes selfishness
Decisions (individual): payoff depends only on your choices
Games (multiplayer): payoff depends on what other people do. In this case, assuming rationality on the part of the other person is very helpful because it helps you make you own decision.
When are we likely to see rationality? When the stakes are large; when decisions are repeated; when groups of people make decisions (although other phenomena like groupthink can occur); or when a problem is particularly simple.
Why is rationality an important assumption? It provides a unique and comparatively easy-to-calculate benchmark; as people learn, they tend towards rationality; and many variations cancel out (i.e., high variance but no bias).

### Behavioral 

== based on empirics; irrational in systematic ways; based on observations and neuroscience
http://us.macmillan.com/thinkingfastandslow/danielkahneman
http://yalepress.yale.edu/book.asp?isbn=9780300122237 
http://www.econlib.org/library/Enc/BehavioralEconomics.html 
Prospect theory: we look at mathematically equivalent gains and losses differently
Hyperbolic discounting: we discount one day in the near future differently than one day in the far future
Status quo bias: 10% check a box to not donate their organs; 25% check a box to donate their organs
Base rate bias: If you get people thinking about one number and then ask them for a second, it will tend to be close to the first number
Hundreds of documented biases; but, themselves subject to criticisms as WEIRD (Western, educated, industrialized, rich, developed countries) rather than universal. 
Can model as rational plus some relevant bias/es.

### Rule-based

Assume that people follow rules/strategies
Can apply fixed or adaptive rules in decision or game contexts
Fixed rule: random choice, most direct route, tit for tat
Adaptive: gradient, mimicry, best possible choice
Rules can be easy to compute, good at capturing main effects; but create ‘people’ who are easily exploited (in a strategic context) and can be very ad hoc

For the sake of aggregate effects, when does individual behavior matter? Which model of people should we choose? “One of the reasons we models is to determine how much it matters how accurate our model is.” 
“In markets, the institution itself has such an influence that we don’t worry about modeling behavior”; this was studied with zero intelligence agents (1,2), and they yield the same equilibrium price as rational agents.

## Coordination & culture

Culture: http://www.worldvaluessurvey.org/WVSContents.jsp, http://geert-hofstede.com/national-culture.html  
differences between groups and
similarities within groups that are 
interesting in some way, i.e., appearing suboptimal or confusing to outsiders.

Culture is an n-person coordination game. In a coordination game, players receive payoffs when they choose the same action. An n-person coordination game can be modeled with a Lyapunov function, F(x)= # of coordinations, k=2. BUT, the process doesn’t have to stop with everyone being the same; you can get blocks of different behaviors. 
Note that suboptimal equilibria are possible with coordination games

### Axelrod’s culture model

http://ccl.northwestern.edu/netlogo/models/community/Dissemination%20of%20Culture 
Features: {1, 2, ..., N}
Traits are actions you take for a feature: {1, 2, …, aN}
Person is a vector of traits on features: [a1, a2, …, ai, aN]
People are placed into a social space, a matrix; they look at neighbors and 
choose whether to interact based on some similarity threshold
if they’re similar enough to interact, then they randomly select a feature and change their behavior to match their neighbor’s trait (if they don’t match already)
→ emergence of distinct cultures with thick boundaries

### Coordination & consistency model

people don’t want to keep infinitely adjusting to people around them; they also have some consistency of worldview. The rule here is, adjust your own traits so they are more numerically similar with each other. → process takes a long time to converge, and small errors at the individual level lead to large population-level divergences. In the transition map below, if we add an error term that allows us to exit the red-red-red-red state of total consistency and coordination, then we suddenly have a Markov process and can represent the process with a transition matrix:

All possible states:


Transition map between states:


Markovian matrix:



Suboptimal outcomes can arise from:
Desire for coordination
Desire for consistency
External change in what’s optimal, but system remains stuck at equilibrium

### Prisoner's dilemma

In Prisoner’s Dilemma, the incentive to defect must be less, on average, than the payoffs available for cooperation. Here are the constraints: T>R, 2T>F, F>T.

What makes PD so interesting is that, in terms of matrix addressing, cell (1,1) is the highest overall payoff; cells (1,2) and (2,1) are Pareto efficient; but cell (2,2), the worst by every criterion, is where the game ends up. It’s the Nash equilibrium. This contrasts with the self-interest game where individual incentives lead players to achieve the highest overall payoff which is also Pareto efficient.
In the case of price competition or tech adoption, we don’t care that the players suffer becomes consumers benefit. (Unless we also consider that consumers are workers.)

7 ways that cooperation arises in 2-person games: Additional constraint b>c where c=cost of cooperation, b=benefit to others. See http://books.simonandschuster.com/SuperCooperators/Martin-Nowak/9781451626636. 
Repeated games with opportunity for direct reciprocity. If p=p(meet again), then p>c/b is the condition.
Indirect reciprocity, i.e. reputation. Let q=p(reputation is known) for the condition q>c/b.
→ denser ties better
Network reciprocity with k neighbors, condition is k > b/c.
→ denser ties worse
Group selection is the mechanism where, through competing against other groups, groups with a higher percentage of cooperators are more successful. 
Kin selection where players are related, r, and care about it; rb>c is the constraint.
Laws or other prohibitions
Creation of incentives

### Collective action problems

n-person Prisoner’s Dilemmas with payoff function for person j making action xj (note that xj’s action is counted as a cost and as a benefit scaled by β): -xj+i=1Nxi, 0 < xj, β < 1.
Common pool resource problems: Rt+1=f(X,Rt) where X is the total consumed, sum of all xi.
What helps with collective action problems? The particulars matter; there’s no panacea. See http://www.onthecommons.org/magazine/elinor-ostroms-8-principles-managing-commmons. Possible options:
Clear ownership
Rotation schemes

## Diversity & innovation 

linked to prior discussion of  economic growth & role of innovation)

F(a), the payoff function for action a. The question is: how do we get to c; and further, how do we avoid getting stuck on local optima to find the global optimum?

Perspectives: how you encode the problem, how you represent it to yourself, creating a landscape (above)
A perspective is a representation of the set of all possible solutions to a problem
A landscape assigns a value to all possible solutions to a problem
A better perspective has fewer peaks: fewer places to get stuck
“Mount Fuji landscape”: single peaked
Savant existence thm: for any problem there’s a perspective that creates a Mt Fuji landscape
With N alternatives, there are N! ways to create a perspective … many bad

Heuristics: rules that guide how you move across/search your landscape
Hillclimb: check left, check right, and move to the higher point
Do the opposite ... of the existing solution
Big rocks first: when filling a bucket with rocks and sand
Random search
Neumann neighborhoods: look N,S,E,W

### No free lunch thm 

(Wolpert & McCready): all algorithms that search the same number of points with the goal of locating the maximum value of a function defined on a finite set, e.g., a landscape, perform exactly the same when averaged over all possible functions → You need to have some instinct for what heuristic suits which landscape; if you don’t know anything about the problem, then no heuristic is better

### Collaborative problem-solving

He uses ‘teams’ in a very loose sense; doesn’t need to be face-to-face or synchronous
Ability: the avg value of all the peaks in your perspective
For teams, look at intersection of perspectives and take the avg
⇒ The team can only get stuck on a solution that’s a local optimum for every member of the team. It might not be the global optimum, but it will be better than individual solutions.
⇒ We want teams with a diversity of perspectives and heuristics
Missing from this model:
Imperfect communication
Error in interpreting the value of a solution

### Recombination

Martin Weitzman’s theory of recombinant economic growth
Exaptation: when something developed for one purpose proves highly useful for another, e.g. feathers
Joel Mokyr: gifts of Athena == technological developments that facilitate the exchange of ideas

## Tipping points & diffusion

“The straw that broke the camel’s back”; not an exponential growth pattern, but a moment when a small change comparable to other prior changes has a wildly disproportionate impact.

### Percolation model (contextual)

Each square in a grid has p probability of being permeable; water can move from the top to the bottom of the grid through adjacent permeable cells, but otherwise stops
At some p ≈ .593, percolation happens (from top to bottom of the grid); this is the tipping point
Can be applied forest fires, where the cells represent trees and we track the spread of fire and also the likely yield of a managed forest
Can be applied to banks/countries in a simple way, to see how failure spreads
Can be applied to the spread of information, where p represents the ‘juciness’ of a rumor; this implies that we shouldn’t expect information to be spread in a linear way
Can be used to explain the synchronicity of scientific discoveries, where p represents the accumulation of related knowledge and technologies: “Once we get above the threshold, there are many paths”

### Contagion models 

#### Diffusion

not a tipping point!

N = total population
Dt = # of people infected with disease D at time t
N - Dt = # of people not infected
T = transmission rate, likelihood of infection upon contact
p(an individual has D) = Dt/N
p(an individual is healthy)  = (N - Dt)/N
c = contact rate; how often people meet, in general
Nc = # of meetings in a population of size N
p(individuals meeting each other will be a healthy-infected pair): Dt/N * (N - Dt)/N * Nc
p(meeting of healthy-infected pair will lead to a new infection): Dt/N * (N - Dt)/N * Nc * T
Dt + 1 = Dt + Dt/N * (N - Dt)/N * Nc * T
Curve D = f(t) is sigmoidal, because initially, there are few people to spread the disease; and eventually, there are few healthy people to spread it to

#### SIS 

tipping point!
Susceptible-Infected-Susceptible
a = likelihood of recovering from the disease
Dt + 1 = Dt + Dt/N * (N - Dt)/N * Nc * T - aDt = Dt + Dt[cT * (N-Dt)/N - a]
If Dt is small, then term N-Dt/N is close to 1, and cT - a > 0, then the disease will spread, i.e.:
R0 = cT/a is the basic reproduction number; if R0 > 1, the disease spreads; this is the tipping point
R0(measles) is about 15, although for this disease, we need to use the SIR model
R0(flu) is about 3
Implications for policy: for V is %N(vaccinated), we need V >= 1 - 1/R0

### Classifying tipping points

Active/direct: due to change in variable, i.e., an unstable equilibria in the phase plane
Contextual: due to change in parameter/s, i.e. a shift of the phase portrait s.t. fixed points change
Within-class tip, e.g. stable equilibrium to new stable equilibrium
Between-class tip, e.g. oscillation to complexity

### Measuring tipping points

how big is the tip? how rare? “measure tippiness by changes in uncertainty”
For p(A) + p(B) + … + p(N) = 1, the likelihood that any member of set A will another member of set A is
 p(A)p(A) = p(A)2, and the diversity index is 1i=A,B,...Np(i)2. This means, roughly, how many ‘types’ of outcomes are there? To quantify tippiness, we calculate the diversity index pre- and post-tip. 

Entropy is -Σ p(i)*log2(p(i)). This tells us the number of bits of information we need to know to identify the outcome. I.e. for outcomes A,B,C,D, the questions are: is it in (A,B) or (C,D)? is it A?


## Economic growth 

Introduction to growth
Growth discussed in terms of GDP per capita
Correlation between GDP and life satisfaction: yes, more money makes you happy if you start poor

### Exponential growth

Compounding: Vt=V0(1+r)t
Continuous compounding: Vt=V0ert, since lim n→ ∞ (1+r/n)nt = ert 
Rule of 72: 72/r is approximately how long it will take GDP or a bank balance to double, where 0.08 interest rate means r=8 ⇒ even small changes in annual growth rate matter

### Basic growth model 

(capital investment, capital depreciation)

Elements:
L1=labor at time t, plus
Kt=capital at time t combine to produce
Yr=GDP at time t, which can be
Ct=consumed at time t, or 
It=invested at time t --- a function of 
s=savings rate.
Finally, d=depreciation rate determines how fast capital deteriorates; and dK is total depreciation.
Assumptions:
Output is an increasing but concave function of labor and capital: Yt = Lt1/2 Kt1/2 
Output is either consumed or invested: Yt = Ct + It and Ir = sYt
Capital can accumulate, but depreciates: Kt+1 = Kt + It - dKt
Result: long-run equilibrium occurs where investment = depreciation
Growth eventually stops because output is concave while depreciation is linear

### Solow growth model 

(capital investment, capital depreciation, innovation)
Same variables as in basic growth model, adding only: 
At, quality of technology at time T, and 
β, a measure of the capital-intensivity of a specific output
Then Yt=f(At,Lt,Kt)=AtLtβKt1-β
Innovation multiplier: since labor and capital are more productive, it makes sense to invest in them more
Notice that multipliers are, obviously, multiplicative rather than additive relationships

“Why are some countries rich and other countries poor?”
Will China keep growing? No, because huge gains early on are possible because there is so little capital relative to labor. Once you accumulate enough capital, you stop growing so rapidly and need to innovate.
Two growth strategies: invest in capital; invest in productivity (innovation). If this is so obvious, why are some countries stagnant? Acemoglu & Robinson, Why Nations Fail: 100s of years of data, a “rich study of history”; make the following main points:
“Growth requires a strong central government to protect capital and investment …” so that people have incentives to invest and innovate,
“… but the government cannot be controlled by a select few,” because it become extractive or protects industries that should die, viz.:
an increase in At, in Solow’s model, means that less labor is needed; i.e., “growth requires creative destruction”, which he asserts is a “short-term” phenomenon.
How do historians feel about Acemoglu?
http://www.nybooks.com/articles/2012/08/16/why-nations-fail/
http://www.economist.com/blogs/buttonwood/2012/04/duelling-academics
http://levine.sscnet.ucla.edu/general/aandrreview.pdf
http://www.historynewsnetwork.org/blog/20780
http://pastspeaks.com/2012/04/17/theory-and-historians/ 

Other applications of this model: for growth of ‘personal GDP’, pursue innovation and continuous learning






   
# Models using ...

## Linear regression

Linear models, not the same thing as a line; y=f(x), a causal relationships; line of best fit; reading regression output
http://www.niaoren.info/pdf/Beauty/9.pdf & other studies, experts never did better at prediction than simple linear models

Strategies for modeling nonlinear data
Approximate nonlinear model with a linear function
Break nonlinear data into parts, e.g. quartiles, each with their own line
Introduce nonlinear terms as linear terms: y = mz+b where z=sqrt(x)

The Big Coefficient approach: making policy decisions based off which coefficient in a multivariate regression is largest, as in evidence based medicine/philanthropy/education/management. The process: build a model, gather data, identify important variables, design intervention.
vs Big Data: gather data, find patterns, identify important variables, design intervention
vs accounting for feedbacks (people’s behavioral adaptations in response to interventions)
vs The New Reality: trying to create a better world, which necessarily means departing from data
e.g. Big Coefficient thinking = tax cigarettes, New Reality thinking = universal health care


## Markov processes

A particularly fertile model that captures dynamic processes between a finite and fixed set of states. It must be possible to get from any one state to any other and the probabilities of moving between states (transition probabilities) must remain fixed over time. If those assumptions hold, then the process will have a unique equilibrium (i.e., initial state doesn’t matter; history doesn’t matter; interventions/perturbations don’t matter).

If A is the transition matrix and x is the state vector, then the equilibrium is: Ax=x. 
Equilibrium=nothing changes vs. statistical equilibrium: individual elements move, but no net change.
http://nicolewhite.github.io/2014/06/10/steady-state-transition-matrix.html    
http://www2.math.uu.se/~takis/L/McRw/mcrw.pdf 

Exaption of Markov models: Useful when there’s a sequence of events with known historical probabilities
Voter turnout, school attendance, student attentiveness, regime transitions
Identification of authors: make a transition matrix of key phrases; how often is the word “for” followed by “the record”, “example”, “the sake of”?


## Lyapunov functions

If we can represent a system/model with a Lyapunov function, then we know it goes to equilibrium and we also know how quickly it does that. A system with equilibrium behavior can lack Lyapunov representation.

Conditions:
There is an absolute minimum/maximum
Movement is only possible in one direction: down/up. In math: if xt+1 ≠ xt, there is some k >0 such that F(xt+1) > F(xt) +/- k, that is, with each timestep the system moves by at least k amount. [Zeno’s paradox; need to define a fixed step distance otherwise we will never arrive at the maximum]
Therefore, eventually, xt+1=xt.
\#periods_to_equilibrium ≤ (max_value - initial_value)/step_size
Does the process necessarily reach a max/min? No, it can get stuck. For example, in the case of preferences, it could be that a pairwise trading scheme leads to a suboptimal allocation while a more sophisticated trading scheme would attain maximum happiness.

The hard part is constructing a Lyapunov function to represent the system.
Self-organized avoidance of gridlock in cities: people have a list of places they need to go in the week, and they determine their route randomly. If they encounter a crowd at their desired location on a particular day, then they modify their route. Say the F(x) = # of people you encounter in your weekly route. This has a minimum value (0), and if people move according to their rule, then #encounters falls by at least k=2.
Exchange economies & externalities: each person brings items to a market, and trades only if that trade increases their happiness by k amount. F(x) = total happiness of all participants in the market. Because of externalities, though, we can’t say that this system will attain equilibrium. Participants not materially involved in a transaction may yet experience its impacts on their happiness.

Even if a system has an equilibrium, we can’t necessarily determine that it does … because of externalities. E.g. Collatz problem: pick a number. If even, divide by 2. If odd, multiply by 3 and add 1. Stop if you ever reach 1. Does this process ever stop?

Difference between Lyapunov processes (reaches an equilibrium that need not be unique or efficient) and Markov processes (stochastic equilibrium that doesn’t depend on initial conditions).

## Path dependence

What is path dependence?
Path dependence=the sequence of previous outcomes matter
Early path dependence, or full path dependence
Recent path dependence
Path dependence of outcomes
Path dependence of equilibria
Phat dependence=the set of events in the path matter, but not their order
State/set dependence=the paths can be partitioned into a finite number of states which contain all relevant information; the outcome in any period depends only upon the state of the process at that time; a kind of Markov process

What isn’t path dependence?
Strict Markov processes, where history doesn’t matter; path-dependent systems violate the Markov assumption of fixed transition probabilities
Externalities: can play a role in creating path dependence, especially negative externalities
Increasing returns/virtuous cycle/self reinforcement/positive feedbacks: often co-occurs with path dependence, but doesn’t need to; increasing returns are a kind of externality
Chaos, where systems exhibit extreme sensitivity to initial conditions; deterministic
Tipping points: similar to the case of chaos, where one moment or point holds undue influence over the fate of the system; it’s a difference of degree, an issue of abrupt versus gradual change

Urn models illustrate types of in/dependence: different colored balls in an urn, picking some out
Bernoulli process (independent): Select ball & return after picking.
Polya process (phat dependent; outcome and equilibrium dependent): Select, return along with another ball of the same color. Results: any probability of picking a red ball is an equally likely equilibrium, and a given set of outcomes is equally likely, no matter what order they occur in.
Balancing process (phat dependent; outcome dependent, equilibrium independent): Opposite of Polya process, so add the opposite color ball. Result: converges to equal percentages.
Sway process (path dependent): In period t, add a ball of the same color as the selected ball and add (2t-s - 2t-s-1) balls of the color chosen in each period s<t. Result: the past takes on more weight over time.


## Networks

Internet has made networks more visible and measurable; popular and scientific interest has increased. Networks have been used to examine segregation; information flow within organizations (e.g. email traffic patterns); link patterns among blogs to demonstrate political polarization; intellectual traditions, citation patterns, etc.
https://flowingdata.com/category/visualization/network-visualization/
https://gephi.org/
http://www.kdnuggets.com/2015/06/top-30-social-network-analysis-visualization-tools.html
http://kateto.net/network-visualization 
http://www.ladamic.com/netlearn/ 
http://www.cs.cornell.edu/home/kleinber/networks-book/ 

Logic of network formation:
Random: N nodes with P probability that they’re attached; interesting to investigate, but not realistic
Contextual tipping point: for large N, P > 1/(N-1) the network almost always becomes connected
Small worlds: Some percentage of local friends and some percentage of random friends 
Random-clique: A variant where local friends are extremely interconnected
Preferential attachment: Node arrives; P(connects to an existing node) is proportional to the existing node’s degree (i.e., more likely to attached to currently well-connected nodes)

Structure & measurement of networks:
Nodes/points & edges/lines
A node might be a state and an edge might be a shared border
Directed edges vs undirected edges
K-neighbor: a node that is k edges distant from another node by the most efficient path

Degree: #edges connected to a node; avg. degree across all nodes, calculated as 2*#edges/#nodes
thm: your friends have more friends than you do
degree has implications for density of connections; social capital; speed of diffusion
Path length: min(#edges) to get from A→B; avg path length between all pairs of nodes
path length has implications for choosing efficient routes, measuring social distance, calculating likelihood that information will spread
Connectedness vs. disconnectedness
has implications for whether Markov convergence thm applies; Internet/power failure
Clustering coefficient: percentage of triples of nodes that have edges between all three nodes
has implications for redundancy/robustness of network; social capital; innovation adoption

Function of networks: is emergent
Explaining 6 degrees of connection through random-clique model
Granovetter’s “Strength of weak ties” paper
Internet is extremely robust against random failure, but not targeted failure (of highly-connected nodes) because it’s a preferentially-attached network
Helps identify who to vaccinate (more targeted than SIS model)


## Randomness

Randomness is a quality of a variable. We want to know two things: What is the source of the randomness, i.e. the error term? What is the distribution of the randomness?

Skill vs luck model: http://success-equation.com/
Outcome = a*luck + (1-a)*skill, 0<a<1
If luck figures heavily, we expect to see regression to the mean
The “paradox of skill”: when the very best compete against each other, randomness matters a lot

Random walk is a process where each time step is random. A random walk has these properties:
For n an even number, n flips has an expected value of 0
For any k,  a random walk will cross |k| an infinite number of times; i.e., no trend will emerge
For any k, a random walk will exhibit a streak/cluster of k outcomes in a row an infinite number of times
Binary random walk: keep a running total when flipping a coin where H=1, T=-1
Normal random walk → Efficient Markets Hypothesis: stock prices are random walks because markets are efficient; people anticipate trends and act in ways that cancel them out. OR, p(tomorrow)=f(p(today), news_tommorow) and news is unpredictable.
http://books.wwnorton.com/books/978-0-393-24611-7/ 
But, there is too much fluctuation in stock prices, and there are consistent winners, e.g. Warren Buffett. 
Finite memory random walk: VT=XT + XT-1 + … XT-n 

## Game theory

### Colonel Blotto

CB is about strategic mismatch and allocation of resources, e.g. as of opposing troops forming battle fronts. One of two models, the other being Prisoner’s Dilemma, that brought game theory into policy settings.

Basic CB model: 2 players with T troops trying to form N fronts, where T >> N. Action = allocation of troops; payoffs =  # fronts won, determined by simple majority (although different fronts can have different payoffs associated with them). Reflected by a matrix of players*fronts. CB is a zero-sum game → Very competitive; “you have to work very very hard to win”. The behavior of this system is equilibrium with a random element. That is, there is an area within the space of possible choices where each player has an expected value of 0, and the winner is determined by luck. → Best strategy: randomize your troop allocation; be confusing.
Any strategy can be defeated
Don’t need all your troops to win
If Blotto is played sequentially, the second player has enormous advantage

Model where one side has an advantage in the form of more resources: As the number of fronts increases, so does the amount of additional resources needed in order to maintain an advantage. Or, the advantage of having k more troops decreases as N increases. If you’re the weaker player, what you want to do is increase the number of fronts/dimensions of competition.

Model where many players compete: Can have nontransitive victories → Lots of cycles, not equilibrium

Applications of this model: US Electoral College, terrorism, trials, hiring


## Replicators

Used in psychology, to model learning; in economics, to model populations of people learning; and in ecology, to model evolution. In a nutshell, there is a distribution containing types, and the distribution is shifting in response to the payoffs that each type experiences. The dynamics of the model offer commentary on the strategies of (1) copying the most common strategy and (2) copying the most successful strategy. Elements of model:
Set of types, {1, 2, 3, … N}
Payoff for/performance of each type, π(i)
Proportion of each type in population, Pr(i)
The proportion of any type in the next period its its population ‘weight’ divided by the sum of all types’ population weights:
Prt+1(i) =Prt(i) (i)j=1NPrt(j) (j)
Weights can be represented graphically by a fitness wheel where (1) the number of slices a type gets reflect its proportion, and (2) the width of the slices reflect its fitness.

Can use replicator models to investigate games if proportions are specified (since payoffs are specified as part of games). This can lead to a different equilibrium that arises from rational actors.
The tricky part here is calculating weights, since payoffs can vary with actions. For a 2x2 game with Pr(½,½) and where player A has payoffs a or b depending on player B’s actions, weightA=½ (½ a + ½ b). 

Fisher’s fundamental theorem: Higher variance increases rate of evolutionary adaptation. Can examine this by taking the average of each generation of low-variation and high-variation populations. Seems to run counter to Six Sigma! But SS is for a fixed problemscape, while Fisher’s is for complex systems.


# Sources

## References

## Read

- [Qualitative analysis](http://www.sosmath.com/diffeq/system/qualitative/qualitative.html)
- [<i>Dynamic models in biology</i>&nbsp;](https://people.cam.cornell.edu/~dmb/DMBsupplements.html)

## Unread

- [11 important model evaluation techniques](http://www.datasciencecentral.com/profiles/blogs/7-important-model-evaluation-error-metrics-everyone-should-know)
- [Prediction vs extrapolation vs interpolation](http://www.datasciencecentral.com/forum/topics/what-are-the-differences-between-prediction-extrapolation-and)
- [The model complexity myth: (or, yes you can fit models with more parameters than data points)](http://jakevdp.github.io/blog/2015/07/06/model-complexity-myth/)
- [Overfitting](http://nlpers.blogspot.co.uk/2015/09/overfitting.html)
- [A Data Scientist’s Guide to Disarmament](https://www.r-bloggers.com/weapons-of-math-destruction-a-data-scientists-guide-to-disarmament/)
- [The Chaos Hypertextbook](http://hypertextbook.com/chaos/)
- [The simple and useful side of dynamical analysis I](http://www.ideaeconomics.org/blog/2015/2/20/inside-vol-2-no-4)
- [Dimensions](http://www.dimensions-math.org/Dim_E.htm)
- [Chaos](http://www.chaos-math.org/en)
- _Weapons of Math Destruction_



# MySQL

[phpMyAdmin](https://www.phpmyadmin.net/) is a MySQL GUI, or you can work with MySQL from Bash: 

```Bash
/usr/bin/mysql -u [uname] -p [pwd]
quit;

/var/log/mysql/error.log
# errors log location 

less my.cnf
# view configs
```

## Manage databases

```SQL
CREATE DATABASE dname;

SHOW DATABASES;
```

## Manage users

```SQL
SELECT User, Host, Password FROM mysql.user;
-- view users

INSERT INTO mysql.user (User,Host,Password) VALUES('username','localhost',PASSWORD('pwd')); 
FLUSH PRIVILEGES;
-- add user

UPDATE mysql.user SET Password = PASSWORD('pwd') WHERE User = 'root'; 
FLUSH PRIVILEGES;
-- change password
 
GRANT ALL PRIVILEGES ON demodb.* to demouser@localhost; 
FLUSH PRIVILEGES; 
SHOW GRANTS FOR 'demouser'@'localhost';
-- grant privileges
```
# Neo4j

The **editor** accepts input:

- [Cypher queries](Cypher.html) to work with graph data
- Client-side commands available, e.g. ```:help```
- Run query: `ctrl+enter`
- Multiline command: `shift+enter`

The **stream** displays output:

- Clear stream: `:clear`
- Monitor: `:play sysinfo`

# Cypher

## Create

### Nodes

This makes a node **named** "ee" with **label** "Person" and **attributes** "name", "from", and "klout":

```SQL
CREATE (ee:Person { name: "Emil", from: "Sweden", klout: 99 })
```

This makes two nodes with different names, the same label ("Person"), and differing attributes:

```SQL
CREATE (js:Person { name: "Johan", from: "Sweden", learn: "surfing" }),
(ir:Person { name: "Ian", from: "England", title: "author" })
```

### Relationships (edges)

```SQL
CREATE (node_name)-[:RELATIONSHIP_NAME {relationship_attribute: value}] -> (node_name)

CREATE (ee)-[:KNOWS {since: 2001}]->(js), (ee)-[:KNOWS {rating: 5}]->(ir)
```


## Retrieve

Find a nodel labeled "Person" and named "Emil":

```SQL
MATCH (ee:Person) WHERE ee.name = "Emil" RETURN ee;
```

Find all Emil's friends by: 

- MATCHING the node:Lobel (ee:Person) WHERE ee.name is Emil;
- Traversing all -:KNOWS- relationships of which either direction involves Emil; 
- Calls result nodes "friends" and returns both Emil and result nodes 

```SQL
MATCH (ee:Person)-[:KNOWS]-(friends) WHERE ee.name = "Emil" RETURN ee, friends
```

### Recommend 

Recommend a similar friend of friend by:

- MATCHING the node:Label (js:Person) WHERE js.name is Johan;
- Traversing all -:KNOWS- relationships to identify unknown friends of friends whose hobby is surfing; 
- Calls result nodes "surfer", drops redundant results, and returns both Johan and result node(s). 

```SQL
MATCH (js:Person)-[:KNOWS]-()-[:KNOWS]-(surfer)
WHERE js.name = "Johan" AND surfer.hobby = "surfing"
RETURN DISTINCT surfer
```

### Analyze queries

```SQL
PROFILE MATCH ...
EXPLAIN MATCH ...
```

# Sources


## References

- [Gephi, the open graph viz platform](https://gephi.org/)
- [Victor Preciado's links about network analysis](https://sites.google.com/site/victormpreciado/links)

## Read

## Unread

- [Coursera - Social Network Analysis](https://www.coursera.org/course/sna)
- [Coursera - Social &amp; Economic Networks](https://www.coursera.org/course/networksonline)
- [EdX - Networks, Crowds &amp; Markets](https://www.edx.org/course/networks-crowds-markets-cornellx-info2040x-2)
- [Coursera - Graph Analytics for Big Data](https://www.coursera.org/learn/big-data-graph-analytics)
- [Understanding the economics of networks](https://www.youtube.com/watch?v=VulfA7eiCw4)
- [Introduction to social network methods](http://faculty.ucr.edu/~hanneman/nettext/)
- _Nexus: Small Worlds_
- _Economic Networks_# Sources

## References


## Read

- [Larry Page: The untold story](http://www.businessinsider.com/larry-page-the-untold-story-2014-4): _One day in July 2001, Larry Page decided to fire Google’s project managers ..._
- [The Making Of Tesla: Invention, Betrayal, And The Birth Of The Roadster](http://www.businessinsider.com/tesla-the-origin-story-2014-10)
- [Janja Lalich on the basics of cults](http://cultresearch.org/category/cults-the-basics/)
- [The tyranny of structurelessness](http://www.jofreeman.com/joreen/tyranny.htm)


## Unread

- [Stafford Beer’s organizational cybernetics can help us understand Occupy’s non-hierarchical democratic processes and the role of autonomy within them](https://roarmag.org/essays/cybernetics-occupy-anarchism-stafford-beer/)
- [Cults at scale in Silicon Valley](http://dismagazine.com/discussion/72970/kate-losse-cults-at-scale/)
- [The unplanned organization](http://www.margaretwheatley.com/articles/unplannedorganization.html)
- ['Superorganisations' - Learning from nature's networks](https://thenatureofbusiness.org/2012/08/15/superorganisations-learning-from-natures-networks/)
- [The organic learning and performance ecosystem](http://davidkelly.me/2015/03/lessons-from-nature-the-organic-learning-performance-ecosystem-resources-shared-at-ecocon/)
- [Seven lessons from nature on how to make change](http://grist.org/article/2011-04-05-seven-lessons-from-nature-how-to-make-change-center-ecoliteracy/)
- [Wiki - Shearing layers](https://en.m.wikipedia.org/wiki/Shearing_layers)
- [Wiki - Organizational behavior](https://en.wikipedia.org/wiki/Organizational_behavior)
- [Building ethical environments for people](https://hackernoon.com/building-ethical-environments-for-people-585cd37bc1bc)

# Tools by degree of change sought

Per Dennis et al. (2012), 

![](../ILLOS/BPA-BPI-BPR.png)

## Business process automation (BPA)

BPA is focused on making current processes more efficient through automation; it requires thorough analysis of the current system.

### Problem analysis

This entails asking current system users about problems they face or features they desire. Problems identified with this technique are often related to UI and their resolution produces little if any business value.

### Root cause analysis

This entails asking current system users to explore the root causes of problems they've identified (since these problems may only be symptoms). Root causes may be uncovered by asking "Why?" five times; causal relationships may be represented as a hierarchy, web, or fishbone/Ishikawa diagram.

## Business process improvement (BPI)

BPI goes beyond increased efficiency to the realm of increased effectiveness, for instance taking advantage of new opportunities and technologies.
    
### Duration analysis

First, major processes are timed; second, all individual steps in those processes are timed. If the sum of step durations is smaller than the overall duration, this indicates a need for improvement --- which may be achieved through **integration** (fewer people involved in doing the work across its lifecycle) or **parallelization** (steps being done concurrently).

### Activity-based costing

Steps in a process are costed, and improvement efforts are focused on the costliest step.

### Informal bench-marking 

Examine competitors to identify performance targets and process improvements.
    
## Business process reengineering (BPR)

BPR is a radical upending of current business processes and systems, so spends little time analyzing them. Per Rigby (2015; also gives recommendations for further reading), a BPR effort focuses on:

- Reorienting organizational culture towards user needs
- Reorganizing traditional functional departments into cross-functional teams
- Redesigning and improving core processes, perhaps using IT to:
    - Reduce costs, waste, and cycle times
    - Increase quality

### Outcome analysis

Trying to think about the business from the perspective of users' ultimate goals and needs (recognizing that your product or service may be only a means to an end).

### Technology analysis

Reviewing existing and emerging technologies and asking how they might be applied in your company (either as a process improvement or a product).

### Activity elimination

Systematically (but as a thought exercise) eliminating each step in a process, and asking what the implications are.




# Continuous Process Improvement

_These notes are based on Jeevon Powell's class, [Process Improvement Tools.](https://ucs.admin.uw.edu/pod/Course/Details/Q1170)_

## Depict system

See [notes on systems modeling.](modeling.html)

### Value stream mapping

### Spaghetti diagram

## Identify problems

### Eleven wastes checklist

### Pareto analysis

## Analyze problems 

### Cause-and-effect diagram

### CEDAC

## Address problem

### Opportunities rating matrix 

### 5S organization methodology

### Brainstorming


# Design Thinking

- [Draw toast](http://www.drawtoast.com/)
- [Microsoft's radical bet on a new type of design thinking](https://www.fastcodesign.com/3054927/the-big-idea/microsofts-inspiring-bet-on-a-radical-new-type-of-design-thinking)
- [Service design toolkit](http://www.servicedesigntoolkit.org/downloads.html)
- [Principles and glossary of presencing](https://www.presencing.com/principles)
- [Design to improve life (cases and methods)](http://designtoimprovelife.dk/tools/)
- [Principles for complex systems](http://capita.wustl.edu/ME567_Informatics/contents/complex.html)




# Operations Management

_These notes are based on Christian Terwiesch's Coursera class, [Intro. to Operations Management.](https://www.coursera.org/learn/wharton-operations)_

Strategy and assessment often reference four **operational dimensions:** 

- Cost
- Time
- Variety/responsiveness to consumer tastes
- Quality (performance, conformance)

Between any two dimensions, there may be a **tradeoff;** obtain this curve by locating the performance of existing companies for any two dimensions. Then the question is whether your company is operating on the frontier or within it.

## Business process analysis

### Flow diagram

Business processes underlie performance. One way to measure them: graph flow units vs. time, showing cumulative inflow and cumulative outflow. Then:

- (Instantaneous) inventory is the vertical distance between c.inflow and c.outflow
- Flow time is the horizontal distance between c.inflow and c.outflow—the time a flow unit spends in the system

Create a process flow diagram aka value stream map showing how flow units are transformed from inputs to outputs by process activities. Represent wait times, lines, or inventories with triangles, and activities with boxes (labeled with activity time in units, and m, the number of workers or resources). Then:

- Processing time is the sum of all the individual activity times
- Capacity is driven by processing time: m/processing time
- The bottleneck is the activity with lowest capacity, and this lowest capacity is the overall process capacity
- Flow rate aka throughput is min{demand_rate, process_capacity}, so flow rate is driven by these factors
- Utilization is flow_rate/capacity (dimensionless)

Realistically, processing times will vary from flow unit to flow unit. Flow units may even follow different pathways through all possible process activities. So, how to find the bottleneck and determine the flow rate? Depict multiple flow units on same process diagram, symbolized as types of flow units, then:

- Approach #1, “adding up demand streams”: (1) Compute capacity of each process activity: m/activity_time (2) Compute demand experienced by each activity (3) Compute implied utilization as ratio of demand to capacity (can sum to more than 100%); now, the activity with highest implied utilization is the bottleneck
- Approach #2, “minutes of work”: (1) Based on m, how many minutes of work per hour can each activity supply? (2)  Calculate how many minutes of work is required by each demand stream (3) Compute ratio of minutes required to minutes available; highest is the bottleneck

### Measuring labor productivity

Why focus on this when labor seems to contribute less to modern firms’ costs? —because many firms keep labor costs off their balance sheet by ‘outsourcing’ them to their suppliers. If you account for labor in the cost of goods supplied, the role of labor becomes evident.

- Labor content: sum of all the individual activity times
- Cycle time: 1/flow_rate
- All activities except the bottleneck have idle time=cycle_time-process_time, and the sum of these is direct idle time
- Average labor utilization is labor_content / (labor_content+direct_idle_time)
- Cost of direct labor: total wages/time / flow_rate/time

### Inventory metrics

#### Little's law

Inventory=Flow_rate*Flow_time. Given any two variables, can solve for a third (especially flow time); also, can decide how to manipulate outcomes by adjusting a variable. Note that flow time here is essentially average flow time, a line fitted to the actual data recorded from observations of inflow and outflow. “Not an empirical law; to prove it, we need to turn to stochastic optimization.”

#### Inventory turns

Cost_Of_Goods_Sold/Inventory. Gives the amount of time that a flow unit spends inside the process; comes from Little’s law (where COGS=Flow_rate). High inventory turns can dramatically reduce inventory costs (capital, storage, obsolescence). Per unit inventory costs=Annual_inventory_cost/Inventory_turns.

#### Make-to-stock vs make-to-order

Motivating question here is why should there be inventory? What are the drivers of inventory? (Note that, per Little’s law, these same factors are implicit drivers of flow time.) Contrast McDonald’s and Subway—not all companies use inventory to increase flow rate; there are make-to-stock vs. make-to-order strategies.

- The reason is variability of activity times; workers do not “work like robots”, so “buffer or suffer”. 
- Reasons for inventory: buffer seasonal demand, internal demand (between processes or activities), customer demand, inherent time lag in production (e.g., aging cheese).

## Analyzing & improving productivity

Frederick Winslow Taylor (1911), [Principles of Scientific Management.](https://en.wikipedia.org/wiki/The_Principles_of_Scientific_Management). In general, (multifactor) productivity  is a ratio of output_produced / input(s)_used. 

- Basic productivity ratio is output/input; at firm or industry level, “output” is revenue, and “input” is cost categories
- Disaggregate ratio to pinpoint source of productivity difference: 
    - Productivity = Operational_yield * Transformation_efficiency * Per-unit_capacity_cost
    - Error
    - Revenue/Cost = Revenue/Output * Output/Capacity * Capacity/Cost

### Sources of wastes

- Waste (or inefficiency) may be represented as the distance between a firm and an industry-wide PPF
- Overproduction: Produce sooner or in greater quantities than required by customers or inventories
- Transportation: Unnecessary, unergonomic movement (Taiichi Ohno: “Moving is not necessarily working”)
- Rework: Having to doing it right the second time; called “bounce backs” in hospitals
- Over-processing: Doing more work than is required to meet the customer’s demands; driven by operator’s high standards, or ignorance about customers’ actual requirements
- Inventory: Poorly managed, can be one of the biggest forms of waste. Inventory may take the form of raw materials, works in progress (WIP), or finished products.
- Waiting: Underutilizing resources because of poor process design
- Intellect: Esp. the intellect of workers

### Financial value of productivity

- “Productivity [i.e., capacity improvements] are not the goal in and of themselves; the goal is to save money”
- Profit: Process_capacity*Per_capita_revenue - (Fixed_costs + Variable_costs)
- “How does the profit change as we change the operational variables?”
- “Every second counts—however, not every second counts equally.” The largest impact comes from improvements to the bottleneck when demand is not a binding constraint; also, in organizations with large fixed costs and lower marginal costs (can see this by checking the relative slopes of the cost and revenue lines past the break even point).

### Key Performance Indicators (KPIs) and KPI trees

KPI trees are about “visualizing the relationship between operational variables and the financial bottom line, and are also the starting point for sensitivity analysis wherein we identify those operational variables that have the largest impact.” Map out dependencies between variables, then take the partial derivative of the terminal KPI w/r/t an operational variable or evaluate using a spreadsheet.

<img src="../ILLOS/KPI-tree.png" width="450px">

### Overall equipment effectiveness (OEE) and overall people effectiveness (OPE)

- Available_time=Total_planned_uptime - Downtime
- Downtime = Break_downs + Product_change_overs
- Net_operating_time = Available_time - Speed_losses
- Speed_losses = Idling + Minor_stoppages + Reduced_operating_speed
- OEE = Net_operating_time - Quality_losses
- Quality_losses = Defects + Startup_time
- Availability_rate * Performance_rate * Quality_rate = Value_add_time / Available_time = OEE

### Reducing idle time

- Takt time is the pace required to keep up with demand: “every person has to dance to the meet of demand”
    - Demand leveling: Average out demand over a period of time
- Line balancing: Equal processing time at each station (ideal). Calculate takt time; assign tasks such that all processing times fall below the takt time; make sure that all tasks are assigned; and minimize the number of workers needed.

### Smoothing performance across workers

- Quartile analysis: Observing workers, noting differences in processing time, grouping into quartiles based on processing time, and identifying best practices that appear to shorten processing time
    - e.g. 260% different observed in ER between 10% and 90% quartiles
    - Biggest differences tend to be observed in knowledge-intensive tasks

## Quality control

- Ford production system: Influenced by Taylor; aspired to optimization of work. Used a moving line, big machinery, economies of scale, standardization of product.
- Toyota production system: c. 1950s, focused on elimination of waste and matching demand after failure to replicate Ford system in a postwar context where inputs and domestic demand was scarce.
    - Worry about: waste, inflexibility, and variability

### Basic defect calculations

- Activity yield is %units produced according to specification = 1 - p(defect)
- Process yield = f(activity_yield); if dependent (Swiss cheese) activities, then product of activity yields, 1-p(defect)n; if independent, then sum, represented as (1-p(defect))<sup>n</sup>. 
- Swiss Cheese model: think of a hole as a defect; as you stack slices of cheese, there is always the chance that all the holes will line up. This redundancy reduces the probability of process failure. Then the process yield is 1-p(defect)<sup>n</sup>.

### Quality & flow

- Adding representations of quality to a flow diagram:
    - Dropped flow units: Calculate end demand; calculate yield of each step (include defects); calculate how much each step must produce to meet demand (e.g., a 50% defect rate means that an activity must produce 2xD); calculate implied utilization (D/capacity); highest implied utilization is the bottleneck.    
    - Re-worked flow units: Calculate the expected processing time. E.g. if there is a 30% defect rate and defects are re-worked, then the real processing time is 0.7*processing_time + 0.3*rework_time. After this modification, the activity with lowest capacity is the bottleneck.
- Costs of defects: Say that we pay $2 per flow unit as an input, but receive $20 per flow unit post-processing, as an output. Then the cost of defects depends not on where they occur, but where they are detected—before or after the bottleneck. Before is cheaper. Pre-bottleneck, defect costs are driven by input costs; post-bottleneck, by revenue (opportunity cost).
- Variability & buffering dilemma: 
    - For a 2-step process where each step has p(defect)=0.5, there are four possibilities: both defect-free; first step defective, leaving step 2 “starved”; second step defective, leaving step one “blocked”; and both defective. This variability dramatically lowers the expected flow rate; by adding a buffer, the flow rate may be increased. 
    - However, buffers remove the incentive for process improvement; buffering hides problems.
    - Toyota developed the Kanban “demand-pull” card system to manage this dilemma. Kanban cards authorize work, and are themselves authorized by customer demand. This puts a cap on inventory.

### Six Sigma

Improving a process by reducing internal variability.

- LSL, USL: lower and upper specification limits
- Capability score (AKA CP score), where higher is better: (USL-LSL)/(6*process_stdev). Clearly, the CP score can be raised by widening the range between upper and lower specifications, or by decreasing the standard deviation in the process.
- For interpretation, relates to defect probabilities.
- Quality targets are often expressed as ppm, parts per million.
- A “six sigma” quality target corresponds to a defect probability of 0.002 and a capability score of 2.

### Control charts

Help distinguish between normal and abnormal variation; part of statistical process control

- Common cause variation may be high or low, but has one basic root
- Assignable cause variation is when the variation stems from multiple sources: “something in the underlying process that changed”, resulting in more variability
- How to identify assignable causes? 
- Establish LCL, UCL---upper and lower control limits, different from LSL, USL. 
- LCL= mean - 3*stdev; UCL = mean + 3*stdev 
- Plot means of samples against LCL, UCL; if a sample’s mean crosses control limits, we suspect assignable cause.

### Jidoka

Detect → Alert → Stop, as quickly as possible, to prevent defects from reaching the bottleneck. In manufacturing assembly lines, jidoka is often implemented with andon cords (for workers to pull, freezing the whole line) and a central andon board (to indicate which station initiated the stop).

- ITAT is “information turnaround time”, something that is (detrimentally) increased by inventory. Low ITAT means quick feedback and the potential for quick learning.

### Problem solving

Looking for root cause(s):

- Kaizen: Process improvement is best carried out by frontline employees
- Ishikawa diagram, also called fishbone diagram, for brainstorming; complemented by 5 whys
- Pareto chart “maps out the assignable causes of a problem in the categories from the Ishikawa diagram, ordering root causes in decreasing order of frequency of occurrance”; 
    - Pareto principle: 80% of the defects are explained by 20% of the root causes

![](../ILLOS/reality-models.png)

<br/>

![](../ILLOS/problem-solving.png)
    



# Sources

Dennis, A., Haley Wixom, B., & Tegarden, D. (2012). Requirements determination. In _Systems analysis and design: An object oriented approach with UML_ (4th ed., pp. 109–152). Hoboken, NJ: Wiley.

Rigby, D. K. (2015). _Management tools 2015: An executive’s guide._ Boston, MA: Bain & Company.

## References

## Read

## Unread

- [Wiki - Six Sigma](https://en.wikipedia.org/wiki/Six_Sigma)
# Sources

## References

- [http://www.makeuseof.com/tag/5-interesting-third-party-evernote-clients-linux-mac/](http://www.makeuseof.com/tag/5-interesting-third-party-evernote-clients-linux-mac/)

## Read

- [https://lifehacker.com/how-to-jump-ship-from-evernote-and-take-your-data-with-1782841075](https://lifehacker.com/how-to-jump-ship-from-evernote-and-take-your-data-with-1782841075)

## Unread

- [https://lifehacker.com/i-still-use-plain-text-for-everything-and-i-love-it-1758380840](https://lifehacker.com/i-still-use-plain-text-for-everything-and-i-love-it-1758380840)
- [https://lifehacker.com/5943320/what-is-markdown-and-why-is-it-better-for-my-to-do-lists-and-notes](https://lifehacker.com/5943320/what-is-markdown-and-why-is-it-better-for-my-to-do-lists-and-notes)

# What is an algorithm?

An algorithm is a step-by-step way of solving a problem. (1) The "steps" may be written as instructions for humans or for machines to follow. (2) There are many ways to solve a problem, so we need ways to evaluate and choose algorithms. 

# What are major programming paradigms?

- Structured
- Functional
- Object-oriented
- Literate

<img src="../ILLOS/types-of-programming.gif" width=450px>

# What are good programming practices?

# Which language?

# Programming class notes

What is the function of secondary memory in a computer?

- [ ] Execute all of the computation and logic of the program.
- [ ] Retrieve web pages over the Internet.
- [X] Store information for the long term - even beyond a power cycle.
- [ ] Take input from the user.

*The CPU computes. Web pages are retrieved through networks. User input comes via peripherals. The secondary memory is persistant storage while the primary memory is emptied if ever power cuts out.*

What is a program?

*A program, also called code, is instructions, also called algorithms, for a computer to follow. Programs may contain a single command or millions. Programs my be contained in a single file or many.*

What is the difference between a compiler and an interpreter?

*A compiler performs a one-time conversion of code written in a high-level language to code written in machine-readable language. Once compiled, the program is executable, i.e., it can be run by the computer directly without an imtermediary. The interpreter is this intermediary: it translates code in a high-level language to machine-readable commands dynamically.*

Which of the following contains "machine code"?

- [ ] The Python interpreter. 
- [ ] The keyboard. 
- [X] Python source file. 
- [ ] A word processing document.

What is wrong with the following code. What error message do you get? Does it help?:

```python
>>> print('Hello world!')
File "<stdin>", line 1
print('Hello world!')
```

Where in the computer is a variable such as "X" stored after the following Python line finishes?

```python
x = 123
```

- [ ] Central processing unit
- [X] Main Memory
- [ ] Secondary Memory
- [ ] Input Devices
- [ ] Output Devices

What will the following program print out:

```python
x = 43
x = x + 1
print(x)
```

- [ ] 43
- [X] 44
- [ ] x + 1
- [ ] Error because x = x + 1 is not possible mathematically

Explain each of the following using an example of a human capability

- [ ] Central processing unit
- [ ] Main Memory
- [ ] Secondary Memory
- [ ] Input Device
- [ ] Output Device.



# Sources

## References

- [Paul Ford: What <i>is</i>&nbsp;code?](https://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/)
- [ReadTheDocs.org](https://docs.readthedocs.io/en/latest/getting_started.html)
- [Programmer competencies matrix](http://sijinjoseph.com/programmer-competency-matrix/)
- [<b>My Bash repository</b>](https://github.com/jacobtkovacs/languages/tree/master/Bash)
- [Command line cheatsheets for Mac<span style="background-color:transparent">,&nbsp;</span>Linux<span style="background-color:transparent">,&nbsp;</span>Windows](http://ss64.com/)
- [Software Carpentry’s Unix shell cheatsheet](http://swcarpentry.github.io/shell-novice/reference/)
- [<b>My Python repository</b>](https://github.com/jacobtkovacs/languages/tree/master/Python)
- [Official Python 3.5.2. documentation](https://docs.python.org/3/index.html)
- [Official Jupyter notebook documentation](http://jupyter-notebook.readthedocs.io/en/latest/notebook.html)
- [Python cookbook](http://code.activestate.com/recipes/langs/python/)
- [Python package index (PyPI)](https://pypi.python.org/pypi)
- [<b>My R repository</b>](https://github.com/jacobtkovacs/languages/tree/master/R)
- [Official R documentation](https://www.r-project.org/)
- [R cookbook](http://www.cookbook-r.com/)
- [R documentation search engine](https://www.rdocumentation.org/)


## Read

- [Low quality of scientific code](http://techblog.bozho.net/the-astonishingly-low-quality-of-scientific-code/)
- [Why code written by scientists gets ugly](https://nsaunders.wordpress.com/2014/05/14/this-is-why-code-written-by-scientists-gets-ugly/)
- [Bad scientific code beats following best practices](http://yosefk.com/blog/why-bad-scientific-code-beats-code-following-best-practices.html)
- [Functional Programming](https://en.wikipedia.org/wiki/Functional_programming)
- [The rise of “worse is better”](https://www.jwz.org/doc/worse-is-better.html)
- [Linear vs. Binary Search](https://schani.wordpress.com/2010/04/30/linear-vs-binary-search/)
- [Why is binary search better than linear search?](http://programmers.stackexchange.com/questions/204260/why-is-binary-search-which-needs-sorted-data-considered-better-than-linear-sear)
- [Introduction to inodes](http://www.grymoire.com/Unix/Inodes.html)
- [10 Things Every Linux Beginner Should Know](https://www.codementor.io/linux/tutorial/10-things-every-linux-beginner-should-know)
- [8 deadly commands you should never run on Linux](http://www.howtogeek.com/125157/8-deadly-commands-you-should-never-run-on-linux/)
- [Thinking in SQL vs. Thinking in Python](https://blog.modeanalytics.com/learning-python-sql/)
- [R beats Python beats Julia, anyone else wanna challenge R?](https://matloff.wordpress.com/2014/05/21/r-beats-python-r-beats-julia-anyone-else-wanna-challenge-r/)
- [Surviving MATLAB &amp; R](http://programmers.stackexchange.com/questions/40738/surviving-matlab-and-r-as-a-hardcore-programmer)
- [R: the master troll](http://www.talyarkoni.org/blog/2012/06/08/r-the-master-troll-of-statistical-languages/)
- [The homogenization of scientific computing](http://www.talyarkoni.org/blog/2013/11/18/the-homogenization-of-scientific-computing-or-why-python-is-steadily-eating-other-languages-lunch/)


## Unread

- [Khan Academy - Algorithms&nbsp;](https://www.khanacademy.org/computing/computer-science/algorithms)
- [Algorithms every software engineer should know by heart](https://www.quora.com/What-are-the-top-10-algorithms-every-software-engineer-should-know-by-heart/answer/Adeel-Ahmed-41?srid=uSgUs&amp;share=0b867289)
- [Reasoning about performance](https://www.youtube.com/watch?v=80LKF2qph6I)
- [Lynda - Code Efficiency](https://www.lynda.com/Developer-Programming-Foundations-tutorials/Foundations-Programming-Code-Efficiency/122461-2.html?srchtrk=index:1%0Alinktypeid:2%0Aq:UML%0Apage:1%0As:relevance%0Asa:true%0Aproducttypeid:2&amp;bm=1)
- [How to Design Programs](http://www.ccs.neu.edu/home/matthias/HtDP2e/)
- [On the value of fundamentals in software development](http://www.skorks.com/2010/04/on-the-value-of-fundamentals-in-software-development/)
- [“Big Ball of Mud”](http://www.laputan.org/mud/)
- [Model, View, Controller](https://www.codecademy.com/articles/mvc)
- [Becl testing framework](https://web.archive.org/web/20150315073817/http://www.xprogramming.com/testfram.htm)
- [Lynda - Design Patterns](https://www.lynda.com/Developer-Programming-Foundations-tutorials/Foundations-Programming-Design-Patterns/135365-2.html)
- [Udacity - Design of Computer Programs](https://www.udacity.com/course/design-of-computer-programs--cs212)
- [MIT - Software construction](http://web.mit.edu/6.005/www/fa15/)
- Literate programming: [1,](https://en.wikipedia.org/wiki/Literate_programming) [2,](http://www.literateprogramming.com/) [3](http://www.witheve.com/)
- [Code Complete](http://cc2e.com/Page.aspx?nid=71)
- [Seven virtues of a good object](http://www.yegor256.com/2014/11/20/seven-virtues-of-good-object.html)
- [Foundations of Computer Science](http://i.stanford.edu/~ullman/focs.html#pdfs)
- [Using Access Control Lists on Linux](http://bencane.com/2012/05/27/acl-using-access-control-lists-on-linux/)
- [Configuring your Linux for development](https://www.codementor.io/linux/tutorial/configure-linux-toolset-zsh-tmux-vim)
- [Vim Adventures](http://vim-adventures.com/)
- [Vim Genius](http://www.vimgenius.com/)
- [R vs Python? No!](http://www.datasciencecentral.com/profiles/blogs/r-vs-python-r-and-python-and-something-else)
- [Paul Graham - The 100 year language](http://www.paulgraham.com/hundred.html)
# What is project management?

## Overview of PM methodologies

Project Life Cycle (PLC) per Watt (2014):

1. Initiation
2. Planning
3. Implementation/Execution
4. Closing

Systems Development Lifecycle (SDLC) per Annabi and McGann (2014), with my additions bracketed:

1. **Planning:** defining scope, objectives/charter, budget, schedule, etc.
2. **Analysis:** characterizing and understanding the system as-is; producing requirements specifications
3. **Design:** producing IT-focused mock-ups, blueprints etc. of system to-be
4. **Development:** building software/hardware
5. **Implementation:** when users encounter the built system
6. [Monitoring and maintaining]
7. [Improving]

Others:

- [Agile](https://hbr.org/video/4846148015001/a-quick-introduction-to-agile-management)
- [MITRE systems engineering lifecycle](https://www.mitre.org/publications/systems-engineering-guide/se-lifecycle-building-blocks)



# Initiating a project

## Strategic assessment

PESTEL, Porter's five, and SWOT/SLOT analyses are more common in the context of organizational strategic planning, but they can also be scoped for projects.

### PESTEL analysis

Systematic review of the broadest trends and forces that constitute the business environment, to identify the implications for organizational strategy (since projects should be related to an organization's strategic goals):

- **Political:** trade policy, international relations, fiscal policy, electoral cycle, tax policy
- **Economic:** unemployment, disposable income, exchange rates, interest rates, trade tariffs, inflation, taxation 
- **Social:** demographics, ethics, consumer attitudes, media
- **Technological:** maturity of various technologies, trends
- **Environmental:** ecological impacts of operations, consumer attitudes
- **Legal:** patents & licensing, employment law, consumer protections, industry-specific regulations, environmental protections, competitive regulations

### Porter's five forces analysis

Framework for evaluating the intensity of competition in a specific market or industry, which may have implications for whether a project is worth undertaking or may point to profitable niches:

- Bargaining power of **consumers**
- Bargaining power of **suppliers**
- Threat of **established competitors**
- Threat of **new entrants**
- Threat of **product substitutes**

### Risk analysis



#### POET analysis

#### SWOT/SLOT analysis

Framework for making connections between a company's external landscape and internal characteristics (which can be restricted to the scope of a single project). Data is collected and sorted into a matrix, with one matrix for each alternative under consideration:

- Internal **strengths**
- Internal **weaknesses** or liabilities
- External **opportunities**
- External **threats**

#### Risk matrix



### Gap analysis

Also called need-gap analysis, need analysis, or need assessment. Gap analysis is a way of ensuring that planned actions align with objectives and present a reasonable pathway from the current reality to the desired state. (The 5 whys or fishbone/Ishikawa/cause-and-effect diagrams may be useful in analyzing the current reality to identify possible actions; see [notes on process improvement.)](#process-improvement.html)

| Objective | Reality | Action |
| --- | --- | --- |
| 12 widgets daily | 2 widgets | Hire more workers |

### Stakeholder analysis

The general aim of stakeholder analysis is to identify stakeholders; analyze their interests and expectations; categorize interests and expectations based on importance and level of stakeholder influence; and develop an action plan that delimits roles for different stakeholders. Stakeholder analysis is important for managing the scope, influence and interorganizational politics of a project, as well as ensuring that projects address all relevant needs (including social equity goals).

Different authors present different stakeholder typologies. Per Leffingwell (2010):

- **Users** are stakeholders
- **System stakeholders** are those who:
    - Will use the system
    - Will work with those who use the system
    - Will be impacted by the deployment and operation of a system
- **Project stakeholders** are those who:
    - Have a vested interest in the project's budget and schedule
    - Will be involved in marketing, selling, installing, or maintaining the system

Two typologies per Rabinowitz (n.d.):

- **Primary** stakeholders use a system directly 
- **Secondary** stakeholders have only indirect contact
- **Key** stakeholders have decision-making authority

<table>
<tr><th></th><th>Low interest</th><th>High interest</th></tr>
<tr><th>High influence</th><td>Latents</td><td>Promoters</td></tr>
<tr><th>Low influence</th><td>Apathetics</td><td>Defenders</td></tr>
</table>

#### RACI plan
    
Once identified, decisions must be made about levels of stakeholder involvement. A RACI plan can capture stakeholder roles as well as roles for those involved in executing the project. Per Kantor (2012):

1. Identify all the tasks involved in delivering the project and list them on the left-hand side of the chart in completion order.
2. Identify all the project stakeholders and list them along the top of the chart.
3. Complete the cells of the model identifying
    a. who has **responsibility (R)** and  **accountability (A),** and 
    b. who will be **consulted (C)** and **informed (I)** for each task [(see: Communications plan)](#communications-plan).
4. Validate and, if needed, amend cells:
    a. Ensure every task has at least one R, but no more than needed to accomplish the work.
    b. Ensure every task has one A, and no more.
    c. Ensure that all participation levels are the minimum necessary, e.g. downgrade from C to I where possible.
    d. Ensure that stakeholders accept their roles.
  
### Force field analysis

Facilitates organizational change by enumerating the forces that help or hinder an organization's ability to make change (note that individual people may constitute a force).

![Example force field analysis](https://www.odi.org/sites/odi.org.uk/files/odi-assets/embedded-images/Forcefield.gif)

## Writing a business case or proposal

Overall, a business case must clearly outline a problem and a solution. If it's a proposal (meaning you must convince someone to hire you), you also need to demonstrate your credibility and capacity to deliver the solution.

Per Tom Sant as summarized by Obuchowski (2015):

<table>
<tr><th>Problem with business proposal</th><th>Remedy</th></tr>
<tr><td style="text-align:left; width:420px;">Failure to focus on the client’s business problems and payoffs; the content sounds generic.</td>
<td style="width:160px;" rowspan=3>**Research the client**</td></tr>
<tr><td style="text-align:left;">No clear differentiation of this customer compared with other customers.</td></tr>
<tr><td style="text-align:left;">Failure to offer a compelling value proposition and clear solution.</td></tr>
<tr><td style="text-align:left;">No persuasive structure --- the proposal is an "information dump".</td>
<td rowspan=3>**Use structuring devices and simple language**</td></tr>
<tr><td style="text-align:left;">Key points are difficult to read because they’re full of jargon, too long, or too technical.</td></tr>
<tr><td style="text-align:left;">Key points are buried --- no punch, no highlighting.</td></tr>
<tr><td style="text-align:left;">Credibility killers --- misspellings, grammar and punctuation errors, use of the wrong client’s name, inconsistent formatting, and similar mistakes.</td>
<td>**Proofread**</td></tr>
</table>

### Writing an executive summary

Per Clayton (2003):

- An executive summary is not a summary; it's an articulation of the business case.
- Can't just echo the RFP, because readers of the full RFP and the executive summary differ.
- Beyond other respondents, you're competing against inertia (since the client may ultimately do nothing).
- Describe your solution in terms of business outcomes; don't get into the technical details.

## Writing a project charter
    
- Project Charter
- Project Initiation Document (PID)
- Project Mandate

### Vision & success criteria

Per Hill & Cantera (2015): 

- Establish a simple definition for "business process" that everyone understands. Adopt Gartner's definition as a starting point.
    - Management process: highest level of abstraction; processes centered on mission statement and long-term vision
    - Administrative process: maintaining a functioning organization
    - Operational process: day-to-day operations
- Assess proposed changes to business processes in terms of the probability of the change directly impacting the desired business outcome. A "good" change is one that improves enterprise business outcomes (not just functional unit outcomes), or that must be made for compliance reasons. Our 2014 study of business transformation initiatives shows that one-third of organizations have an insufficient understanding of their business strategy, and 35% fail to translate the business strategy into measurable business goals.
- Work backwards from the business outcome using 5 whys, Fishbone diagram, or customer journey mapping.
- A Level 0 process diagram is a simple way of communicating how the organization works. It is the highest representation of the enterprise's business operations from a process perspective ... In our view, every organization would benefit from having such a visual to facilitate communications about improving the outcomes from business operations. Such a visual should fit on a single piece of paper.
- Identify key processes and pair them with metrics. (Metrics can be of the form 'from X to Y' but should be outside-in; e.g.,"from purchase request to receipt".)
    - How do we make our product or service?
    - How do our customers receive/experience the value of our product or service?
    - How do we collect money for our product or service?
    - How do we support (provide customer service for) our product or service?
    - How do we source the raw materials or talent we need to create our product or service?
- Look at the process as-is and try to simplify it, rather than leaping to IT automation as the solution. "Automation is very good for scaling work (with speed and capacity) without a commensurate increase in head count, for lowering the cost of performing the work (since, theoretically, machines can work 24/7) and for increasing the consistency of the output. If these are the kinds of improvements desired, then automation is an appropriate answer."

### Defining & managing scope

Per Sheen (2015), scope creep is pervasive. Scope should be clearly addressed during the project initiation phase by (1) listing what's in and out of scope, provided stakeholders agree about scope; or (2) establishing scope ranges AKA scope tolerance parameters, which can be pinpointed as information emerges. A task is out of scope if it (1) doesn't make a direct contribution to the project goal, or (2) if time and money are binding constraints. **Project the impact of requested additional tasks;** never simply agree to perform them.



# Project planning

## ... in Agile methodologies

In the traditional "serial" or lifecycle project management approach, requirements are translated into deliverables, deliverables are translated into a work breakdown structure (WBS), and the WBS is translated into a schedule and budget. Per Ambler (n.d.), about two-thirds of requirements elicited in this way lead to features that are never or rarely used, i.e. "spectacular levels of waste". In response, Agile tries to match development processes to the realities of constantly shifting requirements using:

- ... in place of a schedule, a **stack** of prioritized requirements with detailed modeling done 'just in time';
- ... in place of a budget, funding is continuously adjusted in response to the **value** delivered by products.

## ... in lifecycle methodologies

### Communications plan

Per Collella (2009), effective organizational communication requires a **communications strategy,** which includes (1) a core message that is not burdened with IT jargon, (2) the capacity to refine messages in response to stakeholder cues, and (3) assessment; a **communications plan** for institutionalizing and executing the strategy; and **communication delivery skills.**

Per PMI (2013):
 
- Be able to sell people on the strategic value of communication
- Know who you need to talk to, and how much [(see: RACI plan)](#raci-plan)
- Plan should specify modes of communication; adjust to people's habits where possible

### Work breakdown structure

Per Wikipedia (2017), a work breakdown structure (WBS) is a **hierarchical decomposition of a project's total work.** WBS elements are coded as 1.0, 1.1, 1.10.11, etc. Child elements must sum to 100% of their parent element, and so on until 100% of the project's total work is accounted for. Elements must be mutually exclusive, which is easier to accomplish if elements are _outcomes,_ not tasks. There are different heuristics for establishing the terminal granularity of a WBS:

- 80 hour rule: smallest element must not exceed 80 hours of work
- Smallest element must be small enough to occur within the standard reporting period

Once the hierarchy is established, terminal elements are budgeted and scheduled.

### Budgeting

### Scheduling

- Critical Path
- Crash
- Gate
- Milestone
- Gantt Chart
- PERT Chart

### Monitoring & reporting

- CARDI Log (Constraints/Assumptions/Risks/Dependencies/Issues)
- RAG Status
- Project Dashboard




# Analysis, design & specification

## Requirements determination

Designs must emerge from in-depth analysis of [stakeholder](#stakeholder-analysis) (not just user) needs; requirements determination is the process of eliciting, analyzing, and synthesizing stakeholder needs so they can influence system design. Dennis et al. (2012) note that the analysis and design phases of a system implementation effort are very closely linked. That is, the product/s of requirements determination are "initial designs". They also note that requirements are expressed first as **business requirements** (from the perspective of stakeholders, including users), second as **system requirements** (from the perspective of developers). 

Requirements are also categorized as **functional** (what business tasks a system must perform) and **nonfunctional** (operational, performance, security, cultural and political requirements that affect how tasks are performed, and may arise from regulations such as Sarbanes-Oxley or the desire to comply with standards such as [COBIT,](https://www.isaca.org/COBIT/Pages/default.aspx) [ISO 9000,](https://www.iso.org/iso-9001-quality-management.html) and the [Capability Maturity Model).](https://resources.sei.cmu.edu/library/asset-view.cfm?assetid=6759) Per [StackExchange answers,](https://softwareengineering.stackexchange.com/questions/82763/why-bother-differentiating-between-functional-and-nonfunctional-requirements) is important to differentiate functional from nonfunctional requirements because:

- They apply to different levels of the system (nonfunctional = system as a whole)
- They are of interest to different people (or some people may only be interested in a subset of requirements)
- They are of interest at different points in the system development lifecycle (nonfunctional = architecture)
- They may be modeled/represented in different ways (nonfunctional = quality attribute scenario)
- Differentiating conceptually helps you be more thorough when eliciting and analyzing requirements
- Modularity (differentiation and cross-referencing) supports iteration

Per Whitney (n.d.), good requirements are:

- Complete, correct, unambiguous, verifiable
- Necessary, feasible, prioritized

Per Avison and Fitzger (2006) qtd. in Dennis et al. (2012), common problems with requirements determination are:

- Inadequate access to users or access to the wrong users for requirements elicitation
- Requirements are inadequately specified
- The inevitably iterative nature of requirements is poorly managed
- Requirements are not verified or validated

### Current state analysis

Per Dennis et al. (2012), a requirements determination process may begin with current state analysis, yielding models of the **as-is system** (see [notes on system & process modeling techniques](modeling.html) that are used to characterize the current states of the system). This, however, depends on:

(1) **The methodology used by the systems development team:** "Users of traditional design methods such as waterfall and parallel development (see Chapter 1) typically spend significant time understanding the as-is system and identifying improvements before moving to capture requirements for the to-be system. However, newer RAD, agile, and object-oriented methodologies, such as phased development, prototyping, throwaway prototyping, extreme programming, and Scrum (see Chapter 1) focus almost exclusively on improvements and the to-be system requirements."

(2) **The context of the system being developed:**
    (a) No current system exists
    (b) Little effect on current systems

(3) **The amount of system change desired,** i.e. BPA vs. BPI vs. BPR (see [notes on process improvement](process-improvement.html) for definitions and associated methods). The amount of change desired and amount of effort spent analyzing the as-is system are inversely related.
    
### Gathering requirements

Per Dennis et al. (2012), requirements may be obtained from users, domain experts, existing processes, [process improvement efforts,](process-improvement.html) existing documents, and competing software using the following techniques (and see [notes on qualitative methods):](qualitative-methods.html) 

#### Interviews

1. Select interviewees
2. Write an **interview plan:**
    a. Design interview questions (closed, open-ended, probing; note that early interviews should be less structured than later interviews)
    b. Sequence interview questions (top-down or bottom-up, depending on how much information is needed and the role of the interview subject)
4. Conduct the interview, focusing on:
    a. Building rapport and interviewee comfort
    b. Conveying the importance of the interview
    c. Uncovering the facts that underlie interviewee opinions
    d. Taking careful notes (if recording is disallowed)
5. Prepare an **interview summary,** and ask the interviewee for any corrections

#### Observations

#### Surveys or questionnaires 

#### JAD sessions

#### Document analysis



### Analyzing requirements

Per Whitney (n.d.), Dennis et al. (2012), and Ambler (1999), once gathered, requirements may be preserved, analyzed, and expressed in different ways:

#### Actors & personas

**Actors** are the generic users of a system, e.g. customers, that might appear on a system diagram. Note that actors include other software as well as people. **Personas** are different personalities that are meant to humanize a generic actor and show the range of users' needs. Personas are fictional but research-based biographies that reflect your understanding of your users; they are an exercise in fostering empathy and user-centered design.

A **primary persona** is one whose needs are distinct enough to require a dedicated interface.

#### User stories

User stories describe, at a high level, the various actions users need to complete. User stories should be written by business or subject matter experts, using the form **"As an X, I need to Y so I can Z."** User stories are refined (split, grouped, reprioritized, etc.) throughout the development process.

#### Concept maps

A informally-constructed network of concepts, reflecting entities and their interrelationships. 

#### Requirements trace matrices

Table format linking requirements with other information, e.g. requirement category, priority level, affected class, etc.

#### Card formats

- **CRC cards,** each reflecting a class; responsibilities of that class; and how it collaborates with other classes. 
- **XP Story Cards,** in which a customer writes a story on a card.
- **Volere Snow Cards:**

<img src="../ILLOS/volere.jpeg" width="350px">

#### Use cases & requirements definition reports

Requirements are often analyzed by grouping them into **use cases,** capturing actors' archetypical interactions with the system. Per Whitney (n.d.), "Use cases integrate the requirements into a comprehensive package that describes the interaction of the user with the system ... [They] should describe the interaction between the actor and the system - what the actor does and how the system reacts. Use cases are expressed textually (AKA **requirements definition report),** usually including these elements:

- Overview
- Notes
- Actors
- Preconditions
- [Scenario(s),](#use-scenarios) AKA alternate course(s)
- Post conditions
- Exceptions
- GUI requirements
- Dependencies and relations (to other use cases; often modeled with UML)

##### Use scenarios

There may be different paths through a use cases, perhaps corresponding to different personas; these paths are called **use scenarios,** and they may be depicted with a [UML activity diagram.](modeling.html#activity-diagrams)

##### Use case diagrams

Use cases may be depicted collectively with a **use case diagram:** 

<img src="../ILLOS/use-case-diagram.gif" width="350px"/>



## Future state design

See [notes on system & process modeling](modeling.html) for methods used to characterize the **to-be system.** 



## Choosing software

See [notes on BI systems](BI.html) for information specific to selecting BI software. 

General criteria to consider:

- [Capability Maturity Model](https://resources.sei.cmu.edu/library/asset-view.cfm?assetid=6759)




# Monitoring & governance





# Sources

## Cited

Ambler, S. (n.d.). Comparing approaches to budgeting and estimating software development projects. Retrieved from [http://www.ambysoft.com/essays/comparingEstimatingApproaches.html](http://www.ambysoft.com/essays/comparingEstimatingApproaches.html)

Annabi, H. & McGann, S. (2014). Unit 1---What is MIS? In _The real deal on MIS._

Clayton, J. (2003). Writing an executive summary that means business. _Harvard Management Communication Letter._

Collella, H. (2009). Effective communications: A strategy. Gartner.

Dennis, A., Haley Wixom, B., & Tegarden, D. (2012). Requirements determination. In _Systems analysis and design: An object oriented approach with UML_ (4th ed., pp. 109–152). Hoboken, NJ: Wiley.

Hill, J. B., & Cantera, M. (2015). Use business outcomes to determine the scope of the “business process” to be improved (No. G00277312). Gartner.

Kantor, B. (2012, May 22). How to design a successful RACI project plan. _CIO._ Retrieved from [http://www.cio.com/article/2395825/project-management/how-to-design-a-successful-raci-project-plan.html](http://www.cio.com/article/2395825/project-management/how-to-design-a-successful-raci-project-plan.html)

Leffingwell, D. (2010). Stakeholders, user personas, and user experiences. In _Agile software requirements: Lean requirements practices for teams, programs, and the enterprise._ Boston, MA: Addison-Wesley Professional.

MITRE. (n.d.). Risk impact assessment and prioritization. In _MITRE systems engineering guide._ Retrieved from [https://www.mitre.org/publications/systems-engineering-guide/acquisition-systems-engineering/risk-management/risk-management-tools](https://www.mitre.org/publications/systems-engineering-guide/acquisition-systems-engineering/risk-management/risk-management-tools)

Obuchowski, J. (2005). A winning proposition: Crafting effective proposals. _Harvard Management Communication Letter._

Project Management Institute (PMI). (2013). Communication: The message is clear.

Rabinowitz, P. (n.d.). Identifying and analyzing stakeholders and their interests. In _Community Tool Box._ Work Group for Community Health and Development at Kansas State University. Retrieved from [http://ctb.ku.edu/en/table-of-contents/participation/encouraging-involvement/identify-stakeholders/main](http://ctb.ku.edu/en/table-of-contents/participation/encouraging-involvement/identify-stakeholders/main)

Rigby, D. K. (2015). _Management tools 2015: An executive’s guide._ Boston, MA: Bain & Company.

Sheen, R. (2015). How to manage scope creep [video]. _Harvard Business Review._ Retrieved from [https://hbr.org/video/2942763785001/how-to-manage-scope-creep](https://hbr.org/video/2942763785001/how-to-manage-scope-creep)

Watt, A. (2014). The project life cycle (phases). In _Project Management._ BCcampus Open Textbook Project. Retrieved from [https://opentextbc.ca/projectmanagement/chapter/chapter-3-the-project-life-cycle-phases-project-management/](https://opentextbc.ca/projectmanagement/chapter/chapter-3-the-project-life-cycle-phases-project-management/)   

Whitney, E. (n.d.). Introduction to gathering requirements and creating use cases. Retrieved from [http://www.codemag.com/Article/0102061](http://www.codemag.com/Article/0102061)

Wikipedia. (2017, March 28). Work breakdown structure. Retrieved from [https://en.wikipedia.org/w/index.php?title=Work_breakdown_structure&oldid=772556888](https://en.wikipedia.org/w/index.php?title=Work_breakdown_structure&oldid=772556888)

## References

## Read

## Unread

- [_Project Management from Simple to Complex_](https://open.umn.edu/opentextbooks/BookDetail.aspx?bookId=36)# Environment

## Writing & running Python

```Bash
python --version 
python  # launches some version of python 2
python3  # launches some version of python 3
quit()

python fname.py  # run a script

# this code will run only if the script is executed from the command line
# it won't run if the script is imported by another script
if __name__ == '__main__': 
    # do something

pip3 install jupyter  # install Jupyer Notebook
# http://jupyter.readthedocs.io/en/latest/install.html

jupyter notebook  # launches JN in wew browser
# quit JN by typing ctrl+c twice in the command line
# share JN by uploading it to GitHub, 
# then pasting its URL into http://nbviewer.jupyter.org/
```

## Managing modules

[Libraries/packages are directories of Python scripts/modules](https://docs.python.org/3/tutorial/modules.html#packages); each script contains special functions, methods, and/or types.

```Python
python3 get-pip.py
pip3 install module_name

import module_name  # use functions from module as module_name.function_name()
import module_name as nickname  # use functions as nickname.function_name()

from module_name import function_name  # partial import; use function as function_name()
from module_name import *  # full import; bad practice because: 
# (1) it floods __name__, the local namespace; 
# (2) names you’ve defined locally or have previously imported may be overwritten; 
# (3) the module's contents are no longer contained in the module's namespace

dir(module_name)  # view contents of module
```

## Which modules?

See also: Doug Hellmann's [Python Module of the Week](https://pymotw.com/2/contents.html), SciPy's [directory of science-related Python resources and modules](https://www.scipy.org/topical-software.html), Fredrik Lundh's [tour of the Python standard library modules](http://effbot.org/media/downloads/librarybook-core-modules.pdf) [pdf], and the [Python Module Index](https://docs.python.org/3/py-modindex.html).

- For data wrangling - `collections (defaultdict), pandas (dataframes), numpy (arrays), GraphLab Create`
  - For datetimes - `datetime, pytz`
  - For web scraping & parsing - `urllib2, requests, scrapy, beautifulsoup, robobrowser`
  - For I/O - `csv, json, lxml`
- For data analysis - `math, statistics, random, numpy`
- For data visualization - `matplotlib, seaborn, prettytable, tablib, bokeh (interactives)`
- For scientific computing - `scipy (integrals, diffeqs, matrixes)`
- For machine learning - `scikit-learn, GraphLab Create`
- For text analysis - `nltk, re, string`
- For functional Python - `operator, functools`
- For testing - `nose, logging, coverage, unittest, exceptions, pdb`




# Language


## Operators

```Python
# COMPARISON operators
a == b  # checking equality/equivalency; returns True
a != b  # checking nonequality; returns False
a is b  # checking identicality; returns False
a is not b  # checking nonidenticality; returns True
a <= b  # inequality
a < b  # strict inequality

# LOGIC operators are used for selection and filtering;
# also with conditional operators to control program flow, although
# multiline if/else expressions are often more readable than complex Booleans
if x > y or y != 1:
    print(x)
   
between1_and5 = [i for i in my_list where i > 1 and i < 5] # a list comprehension

# avoid negation of positive expressions, e.g. if not a is b
# prefer inline negation:
if x is not y:
    print(x) 

all(my_iterable)  # returns True if my_iterable is empty, or all its elements are True
any(my_iterable)  # returns False if my_iterable is empty, or any element is False
```



## Control flow statements

```Python
# CONDITIONAL operators: 
if x > 2:
    continue  # jumps to next iteration 
elif x < 0:
    if x == -1:  # conditionals can be nested
        break  # completely exits loop 
else:
    # if included, else clause must be at the end  elif, else 

# there is a conditional execution structure for errors; 
# this is called catching an exception: 
try: # to run code based on input 
except: # ask for better input 
else: # execute if try was successful; visually distinguishes the success case
finally: # run if all prior code has failed, e.g. close file handles

# DEFINITE loop: 
for i in [set]: …
# or range(x) in python3 // xrange(x) in python2
# or range(len(my_list) in python3 // range(x) in python2
# or i, em in enumerate(my_iterable, [starting_index])

# INDEFINITE loop: 
while [condition]: …  
```



## Comprehensions

Often, `for` loops can be conveniently replaced with a comprehension. Comprehensions can be fairly complex, but at a certain point it's better to switch back to a loop.

```Python
squares = [i**2 for i in range(10)]  # list comprehension
squares3 = [i**2 for i in range(30) if i%3==0]   # conditional list comprehension
two_filter = [x for x in a if x>4 if x%2==0]  # multiconditional list comprehension
squared = [[x**2 for x in row] for row in matrix]  # nested list comprehensions

grid_list = [(x,y) for x in rows, y in cols]   # list comprehension returns list of tuples
set = {num * 2 for num in [5, 2, 18, 2, 42, 2]}  # set comprehension

# dictionary comprehensions
squares_dict = { i : i**2 for i in range(20)}  
transposed_dict = {dict[key]:key for key in dict} 
dict = {letter: num for letter, num in zip('abcdef', range(1, 7))} 
```



## Generator expressions

A [generator expression](http://anandology.com/python-practice-book/iterators.html), also called a naked comprehension, is useful for processing large datasets because intermediate results are not stored, so RAM isn't overwhelmed.

- Generators are "stateful"
- You won't get any errors when you iterate over an already exhausted iterator; see pp. 40-41 of _Effective Python_.
- Generators are great for functional programming; they execute very quickly when chained together

```Python
it = (len(x) for x in open('myfile.txt'))
print(next(it))
print(next(it))

roots = ((x, x**0.5) for x in it)
print(next(roots))

sum(i**2 for i in range(10))

list(my_generator(data))  # to convert generator to list, but why??
```



## Datatypes

Overview of [standard types](https://docs.python.org/3.5/library/stdtypes.html):

- Numerics are integers, floats, complex(re,im), decimals
- Sequences are strings, lists, queues, tuples, ranges
- Strings, bytes, unicode are character types
- Collections AKA containers are lists, queues, tuples, sets, dicts; 
  - Collections support operators `in, not in`
  - Not all containers are iterable, see [iterables vs iterators vs generators](http://nvie.com/posts/iterators-vs-generators/)
  - The only container also a mapping is dict

```Python
print(type(my_var))  # check type
print(repr(my_var))  # printable representation; differentiates '5' and 5 when printing

str()  # convert to string
text.decode('utf-8')  # convert bytes to unicode
text.encode('utf-8')  # convert unicode to bytes

int()  # convert numeric to integer
my_int.to_bytes(length, byteorder=big, *, signed=False)  # OverflowError if too small
my_int.from_bytes(bytes, byteorder=little, *, signed=True)
my_integer.bit_length()  # how many bits to represent an integer?

# also bool(), float()
```

### Booleans

- In addition to Boolean operands `True` and `False`, all Python objects have truth values 
- `None`, `0` for any numeric type, and empty collections evaluate as `False`

### Numerics

```Python
my_float = 5.519
abs(my_float)
sum(my_iterable)  # sum up numerics stored in an iterable
round(my_float[, n])  # round float to n digits; n defaults to 0

# standard numeric operators;
# Python converts types as necessary to perform operations:
2 + 3  # addition
2 - 3  # subtraction 
2 * 3  # multiplication 
2 ** 3  # exponentiation
6 / 3  # division 
7 % 2  # modulo; returns remainder of division, e.g. 7/2 = 2*3 + 1, the remainder is 1 
5 // 2  # floor division, AKA integer division; divides int by int, drops remainder; e.g. 5//2 = 4 
divmod(5, 2)  # returns (x//y, x%y)

import math
math.factorial(my_integer)
math.sqrt(my_float)
math.pi  # a constant
math.e  # a constant
math.gcd(my_float1, my_float2)  # greatest common divisor
math.trunc(my_float)  # truncates float to integer part, without rounding
math.floor(my_float)  # greatest float(integer) less than or equal to x
math.ceil(my_float)  # smallest float(integer) greater than or equal to x
math.log(my_float[, base])

# the decimal library is useful for currency:
import decimal
my_decimal_price = Decimal('5.003')
my_decimal_price.quantize(Decimal('0.01'), rounding=ROUND_UP) # returns 5.01
```

### Sequences

```Python
# Operations supported for all sequences:
x not in s  # membership check; returns True or False
x in s

s * n  # adds s to self n times, n is an integer
s + t  # concatenation
len(s)
min(s)
max(s)

# Addressing for all sequences:
s.index(x[, i[, j]])  # find index of element x between optional i (inclusive) to j (exclusive)
s[i:j:k]  # slice s, taking every kth item from index i (inclusive) to j (exclusive)
# More slicing syntax: s[:], s[i:], s[:j], s[-3:-1] 
# Stride k can be negative, but keep it positive to avoid confusion
# For readability, consider two statements: one to stride, the next to slice
```

#### Strings, bytes, & unicode

Like lists, strings are composed of elements that can be accessed via their index. Unlike lists, strings are immutable: individual elements cannot be deleted or modified. 

- Your program should use unicode at its core, with helper functions to convert input and output:
  - Convert bytes (or string of bytes): `text.decode('utf-8')` 
  - Convert unicode string: `text.encode('utf-8')`
- In Python 3, `str()` is unicode, `bytes()` is raw 8-bit. In Python 2, `str()` is raw 8-bit, `unicode()` is unicode
- String output is formatted with with [.format() and its mini-language](https://docs.python.org/3.5/library/string.html#formatstrings), since [% formatting is depreciated](https://docs.python.org/3.5/library/stdtypes.html#printf-style-string-formatting)

```Python
my_string1 = 'allows embedded "double" quotes'
my_string2 = "allows embedded 'single' quotes"
my_string3 = 'quotes can be \'escaped\' using the backslash character'

print("Hello " + user_name + ", how are you doing?") # string concatenation

# split string
my_str.partition(sep)  # returns 3-tuple: (str_before_separator, separator, str_after_separator)
my_str.split(sep=None, maxsplit=-1)  # split string every time delimiter occurs or #maxsplit 
my_str.splitlines([keepends])  # keepends is a Boolean

' '.join(my_iterable)  # join elements in an iterator using ' ' as the separator between elements
my_string.replace(old, new[, count])  # optional 'count' specifies #instances to replace
my_string.isalpha()  # False if nonalphabetic character in string
my_string.zfill(width)  # left-pads a string with zeros 
my_string.ljust(width[, fillchar])
# Many of these methods have counterparts that start from the end of the string 
# s.rindex(), s.rfind(), s.rpartition(), etc.
```

#### Lists

Lists store multiple elements of any type, including mixed type and including other lists. Lists are mutable; unlike string methods, most list methods alter the list in-place and return None. Lists are both sequences and containers. 

```Python
my_list = list()
my_list = []
my_list = list('abc')
my_list = ['a', 'b', 'c']
my_list = [i for i in range(len(n))]

b = a  # an ALIAS, not a copy! a is b; changes to b affect a
b = list(a) # copies list; b is equivalent, but not identical to a 
b = a[:]  # copies list; b is equivalent, but not identical to a 

em in my_list  # check membership

my_list[i[:j]] = em  # update list; em will replace slice i:j, even if len(em) < len(list[i:j])
my_list.insert(index, em)  # adds element at index
my_list.append(my_list2)  # adds element/s at end of list 
my_list1 + my_list2  # adds element/s at end of list
my_list.extend(em)  # adds element/s at end of list; faster for large lists

my_list.remove(em)
del my_list[i:j]
my_list.pop([i])  # deletes and returns last element, or ith element

' '.join(my_iterable)  # join elements in an iterator using ' ' as the separator between elements
my_list.reverse()  # reverses list elements in-place
sum(my_list)  # if list elements are numerics
my_list.sort()  # sorts list elements in place
my_sorted_list = sorted(my_list, reverse=False)  # returns sorted copy of unaltered list
# https://wiki.python.org/moin/HowTo/Sorting
```

#### Queues

Use a [double-ended queue](https://docs.python.org/3/library/collections.html#deque-objects), a list-like datatype, when you need to quickly insert or remove items from the end and beginning (deques are a stack-queue hybrid):

```Python
import collections
my_deque = deque()

my_deque.appendleft(em)  # add element to left
my_deque.insert(i, em)  # add element at specified index
my_deque.append(em)  # add element to right
my_deque.reverse()  # reverses elements in place, returning None

my_deque.popleft()  # remove and return element from left
my_deque.pop()  # remove and return element from right
```

Use a [heap queue](https://docs.python.org/3/library/heapq.html) when you want a list that's automatically sorted:

```Python
import heapq
my_heap = list()
heappush(my_heap, 3)  # add element
heappush(my_heap, 5)
heappush(my_heap, 1)

my_list = [3, 5, 1]
my_heap = heapify(my_list)

my_heap[0]  # always returns lowest number; here, 1
print(heappop(my_heap), heappop(my_heap))  # removes and prints lowest, next lowest, etc.; here 1, 3
```

#### Tuples

Tuples addressing works like list addressing; unlike lists, though, tuples are immutable. When comparing tuples, Python proceeds on an index-by-index basis. Tuples are used for composite dictionary keys and multivariable assignment:

```Python
my_tuple = 'a',
my_tuple = 'a','b','c','d','e'
my_tuple = tuple(my_iterable)
  
a = 1,2,
b,c = a  # multivariable assignment, aka unpacking a tuple; b=1, c=2
d=b,c  # packing a tuple; d=1,2 and a==d

directory_dict[last,first] = 'phone_number'  # tuple as composite key
for last, first in directory_dict:
        print first, last, directory_dict[last,first]
```

### Sets

The value of sets is access to set operations; by design, seys lack slicing and indexing:

```Python
my_set1 = {'a', 'b'}
my_set2 = set(['a','b','c'])

x in my_set1  # True if x an element of set

words_unique = list(set(words))  # find unique values

my_set1.add(elem)
my_set1.remove(elem)  # raises KeyError if elem not in set
my_set1.discard(elem)
my_set1.pop()  # remove and return arbitrary element
my_set1.clear()  # deletes all elements

my_set1.isdisjoint(my_set2)  # True if nonoverlapping sets
my_set1.issubset(my_set2)
my_set1.union(my_set2)  # creates a new set from union of sets
my_set1.intersect(my_set2)  # creates a new set from intersection of sets
my_set1.difference(my_set2)  # creates a new set: set1 - set2
my_set1.symmetric_difference(my_set2)  # creates a new set: (set1-set2)U(set2-set1)
# many of these operations have more mathematical-looking alternative notation: 
# https://docs.python.org/3.5/library/stdtypes.html#set-types-set-frozenset
```

### Dictionaries

A dictionary maps keys to values; values are retrieved via their key, doing away with indices. A dictionary is much faster to search than a list, and is often used to count letter or word occurrences in a block of text.

```Python
a = dict(one=1, two=2, three=3)
b = {'one': 1, 'two': 2, 'three': 3}
c = dict(zip(['one', 'two', 'three'], [1, 2, 3]))
d = dict([('two', 2), ('one', 1), ('three', 3)])
assert a == b == c == d

my_dict1.update(my_dict2)  # concatenate dictionaries
my_dict['key'] = 'value'  # add element
del my_dict['key']  # delete element
my_dict.clear()  # remove all elements
if key in my_dict: ...  # test membership

# ways to view or unpack a dictionary
for pairs in my_dict.items(): ... 
for k, v in my_dict.items(): ...
for k in my_dict.keys(): ...
for v in my_dict.values(): ... 
print("{}: {}".format(**my_dict))
assert iter(my_dict) == iter(my_dict.keys())  # returns an iterator of dictionary keys 

my_dict.pop('k'[, default_value])  # returns and deletes a random element, or returns default_value
my_dict.popitem()  # deletes and returns arbitrary (k, v) pair
my_dict.setdefault('k'[, default_value])  # return v if k exists, otherwise set k=default_value, returns v
my_dict.get('k'[, default_value])  # returns default_value if k not found; otherwise returns v
my_dict['key'] = my_dict.get('key',0) + 1  # counter

# special dictionaries
import collections
my_ordered_dict = OrderedDict()  # recalls order in which its populated
my_default_dict = defaultdict(int)  # sets default_value == 0, ready to increment
my_default_dict['key'] += 1  # increment values initialized at 0
```

### Datetimes

Code should convert local datetimes to UTC, perform computations, then convert back to local datetimes for display purposes.

```Python
import datetime
import pytz  # a database of timezones
# http://www.saltycrane.com/blog/2009/05/converting-time-zones-datetime-objects-python/

# get current date/time
my_current_datetime = datetime.datetime.now(tzinfo=my_timezone)
my_current_date =  now.date()
my_current_time = now.time()

# create naive datetime object: doesn't know its timezone
naive_datetime_from_timestamp = datetime.datetime.fromtimestamp(my_posix_timestamp)
naive_datetime = datetime.datetime(my_year, my_month, my_day[, my_hour[, my_min[, my_sec[, my_microsec]]]])

# create timedelta objects for timezone assignment/conversion
pacific_tz_offset = datetime.timezone(datetime.timedelta(hours=-8))
eastern_tz_offset = datetime.timezone(datetime.timedelta(hours=-5))

# create aware datetime object: knows its timezone
my_pacific_datetime = datetime.datetime(my_year, my_month, my_day, my_hour, tzinfo=pacific_tz_offset)
my_pacific_datetime = my_naive_datetime.astimezone(pacific_tz_offset)

# create timedelta object to manipulate datetime objects
my_inc_5hrs = datetime.timedelta(hours=5)

# access or update a datetime object
my_incremented_datetime = my_datetime + my_inc_5hrs
my_datetime.replace(hour=my_new_hour)
my_year = my_datetime.year

# convert datetime from string
my_datetime.strftime(my_format_string) 
# Format string mini-language: %Y-%m-%d %H:%M:%S %Z%z
# https://docs.python.org/3/library/datetime.html?highlight=datetime#strftime-and-strptime-behavior
```



## Functions

Functions are pieces of reusable code that solve particular tasks. Brett Slatkin, _Effective Python_, p. 10:

> As soon as your expression get complicated, it's time to consider splitting them into smaller pieces and moving logic into helper functions. What you gain in readability always outweighs what brevity may have afforded you. Don't let Python's pithy syntax for complex expressions get you into a mess ...

- Notation: `fname(req_arg[, opt_arg])`
- Function calls can be nested: `print(type(var_name))`
  - With nesting, inner functions have access to the scope of the outer functions 
- Functions can be recursive (can return a call to themself)
- __Write functions to raise exceptions__; expect the calling code to _handle_ exceptions

```Python
help(fname.mname) 
%timeit function(argument)  # in Jupyter Notebook

# more about function arguments:
# http://stackoverflow.com/a/1419160
# http://markmiyashita.com/blog/python-args-and-kwargs/
# http://geekodour.blogspot.com/2015/04/args-and-kwargs-in-python-explained.html
# https://docs.python.org/3.5/tutorial/controlflow.html#more-on-defining-functions
# *args makes a tuple; **kwargs makes a dictionary
def my_func(positional_arg, optional_keyword_arg = default_value, *args, **kwargs):
    ...
    return my_var

my_func(2, optional_keyword_arg = my_value)
# in a function call, keyword arguments must follow positional arguments

# example of exending a function's parameters while remaining 
# backwards compatible with existing callers:
def log(message, when=None):
    """ Log a message with a timestamp.
    Args:
        message: Message to print.
        when: datetime when message occured. Defaults to the present time.
    """
    when = datetime.now() if when is None else when  # LOOK HERE
    print('%s: %s' % (when, message))
```

## Closures

The scope of closures is tricky; see _Effective Python_, pp. 31-36. The general notion:

```Python
def add_to_five(num): 
    def inner():  # write a nested function
        print(num+f) 
        
    return inner  # return the nested function
    
fifteen = add_to_five(10)  # store function call (with argument) as a variable
fifteen()  # call variable as function
```

## Decorators

- Brett Slatkin: _Decorators are Python syntax for allowing one function to modify another function at runtime._
- [Introduction to decorators](https://www.codementor.io/python/tutorial/introduction-to-decorators)

```Python
import functools

def logme(func): 
    
    import logging 
    logging.basicConfig(level=logging.DEBUG) 
    
    @wraps(func) # applies decorator from functools so inner.__name__ = func.__name__, etc. 
    
    def inner(*args, **kwargs): 
        logging.debug("Called {} with {} and {}".format(func.__name__, args, kwargs) 
        return func(*args, **kwargs) 
        
    return inner 

@logme 
def say_hello(): 
    print("Hello there!") 
    
say_hello() # syntactic sugar!!
```

## Style

- Python prioritizes readability and simplicity; `import this`
- Python is extremely picky about indentation, e.g. to define functions
- Python is case sensitive and has several [reserved words](http://pentangle.net/python/handbook/node52.html)
- Python uses [zero-based indexing](http://python-history.blogspot.com/2013/10/why-python-uses-0-based-indexing.html)
- See PEPs [20](https://www.python.org/dev/peps/pep-0020/), [290](https://www.python.org/dev/peps/pep-0290/), [291](https://www.python.org/dev/peps/pep-0291/), [345](https://www.python.org/dev/peps/pep-0345/), [8](https://www.python.org/dev/peps/pep-0008/) 
- Use [pycodestyle](https://pypi.python.org/pypi/pycodestyle/1.8.0.dev0) or [yapf](https://github.com/google/yapf) to automate style

### Spacing

- Use 2 empty lines between functions
- Use 2 empty lines between methods
- Indent with 4 spaces (don't tab)
- Use spaces after commas: `def my_fname(arg1, arg2)`
- Use spaces around assignments and other operators: `my_str = 'value'`
- Don't use spaces around operators in function calls: `my_fname(kwarg=my_var)`
- Put two spaces between code and inline comments
- Avoid single line if, for, while, excepts

### Naming

- Variable names should be informative nouns; also 
- Avoid single letter names since they might conflict with pdb (debugging library)
- For globals: `MODULE_LEVEL_CONSTANT`
- Function names should be informative verb phrases
- For functions and variables, use `my_fname` (snake case) instead of `MyFname` (camel case)
- For classes and exceptions use camel case
- For methods: `\_protected_instance_attribute` or `\_\_private_instance_attribute`

### Structure

- Group common operations into functions; group common functions into classes
- Put import statements at top of file with one library per line, in order: 
  - standard library, 
  - 3rd party modules, 
  - own modules
- 79 characters or less per line
- When an expression exceeds 79 characters, indent it 4 characters past its normal indentation level on the next line

### Namespace & docstrings

- Profile before optimizing; use `tracemalloc` to profile memory use and leaks
- [PEP 257](https://www.python.org/dev/peps/pep-0257/): write docstrings for every function, class, and module.
  - function docstrings should describe the purpose of the function, its arguments (incl. default values, *args, *kwargs), its return value/s, and any exceptions that callers should handle.
  - class docstrings should describe the purpose of the class, public attributes and methods, how to interact with protected attributes.
  - module docstrings should specify the module's purpose and the classes/functions available in the module.

```Python
dir()  # all names in current local scope
dir(my_object)  # list of my_object's attributes

help()
help(my_object)

print(repr(f_name.__doc__)  # access docstrings
# repr returns the printable representation of an object
# helpful for debugging, to differentiate between print(5) and print('5')

print(my_object.__dict__)  # to view internals, p. 204

# add docstring to functions, classes, methods:
def my_fcn():
  """this is a docstring"""
  
  """for a multiline docstring,
  put closing quotes on their own line
  """

import docstrings
help(docstrings.function_name) 
```




# Paradigms

## Object-oriented Python

Classes are collections of methods and attributes. An object is an instantiation of a class; everything in Python in an object.

- [Explanation from Python 3.5 docs](https://docs.python.org/3.5/tutorial/classes.html)
- [Anand Chitipothu's explanation](http://anandology.com/python-practice-book/object_oriented_programming.html)

```Python

class ClassName(ParentName1, ParentName2, arg1, ...):  # define a class, its inheritance & arguments
    
    def mname(self):  # create method in a class
        vname = my_value  # create attributes in a class 
        return self.vname

    def method_override:  # when method name duplicates a parent's method's name
        ...
    
    def __init__ (self, arg1 = default_val):  # control what happens on instantiation 
        ...
        
    def __str__(self):  # control results of print(my_object)
        return({},{}.format(self.__class__.__name__, self.vname))

from scriptname import ClassName  # use a class
my_var = ClassName.vname  # access attributes in the class
inst_name = ClassName(arg)  # create an instance of a class
inst_name = filename.ClassName()
inst_name.vname = my_value  # define attributes of an instance    
```

### Methods

- Methods are functions inherited from an object's class
- Find methods for datatype: `help(type_name)`
- Methods can be chained: `my_str.lower().strip()`
- CAUTION: some methods change the object they’re called on 

## Functional Python

Core concepts of the functional approach to programming, see also [[1](https://docs.python.org/3/howto/functional.html)], [[2](http://anandology.com/python-practice-book/functional-programming.html)], [[3](https://docs.python.org/3/library/functional.html)]:

- __Computation is the evaluation of functions__; code should be mostly functions
- __Programming is done with expressions__: pass the output of a function to another
  - either by chaining (possible if each function returns an iterable)
  - or by using intermediary variables
- __No side-effects from computation__: a function shouldn’t change values that are outside its proper scope (e.g. defining a variable in a function that reaches outside the function: def my_function(): global var_name, nonlocal var_name)
  - be especially careful about mutable objects like lists & dicts
- __Functions are first-class citizens__, since they can be both inputs to and outputs from other functions
    - `def my_func(other_func): ... `
- __Functions should be limited in scope and functionality__
  - e.g., not too many arguments
  - e.g., prefer short functions that do one thing
  
### Lambdas

Anonymous functions that we won’t need to use again; one line long; can’t contain assignments; automatically return the last value calculated.

```Python
filter(lambda book: book.pages >= 600, BOOKS)
reduce(lambda x, y: x if len(x) > len(y) else y, [s for s in strings])
```

### Currying

currying is the technique of translating the evaluation of a function that takes multiple arguments (or a tuple of arguments) into evaluating a sequence of functions, each with a single argument. 

### Map-reduce, filter, etc.

```Python
# map: transform every element of an iterable
# similar to list comprehension: [my_func(i) for i in my_iterable]
# prefer map to list comprehension when need to nest functions
list(map(my_func, my_iterable))

# reduce: good algorithm for summing numbers, multiplying numbers
from functools import reduce
def product(x,y): return x*y → print(reduce(product([1,2,3,4,5])))

# sorting: operator module is helpful 
# access attributes of an object: attrgetter
# access items in a dict: itemgetter
sorted(my_data, key=itemgetter(‘dict_key_name’), reverse=True)
# key here is a **kwarg
# see also: reversed()

# filtering: 

# filter function tests every item in iterable, and keeps the truthy ones
# equivalent to: [item for item in iterable if func(item)]
def is_long_book(book): 
    return book.pages >= 600
filter(is_long_book, books_data) 

#  builds iterator from True elements of my_iterable or my_function(my_iterable)
filter([my_function,] my_iterable) 

# builds iterator from my_iterable's False elements
import itertools
itertools.filterfalse(my_iterable)  

# partial: 
from functools import partial
def markdown(book, discount): 
    ... 
std_discount = partial(markdown, discount=.2) 
print(std_discount(my_data))
```


## Test-Driven Development

- TDD = write tests for code before writing code itself
- Doc tests are based on string comparison, so may have issues w/ floats; very code-specfic, not portable
- [coverage](http://coverage.readthedocs.io/en/latest/)
- [unittest](https://docs.python.org/3/library/unittest.html)

```Python
# write doc test
def my_function():
  """Explanation of function
  do this code
  expected_result
  """
  
# run doc tests
python -m doctest filename.py

# testing the extent of testing
pip install coverage
coverage run tests.py
coverage report -m  # in terminal
coverage html  # in browser

# unit tests
python -m unittest tests.py
if __name__ == ‘__main__’: 
  unittest.main()

import unittest
class my_unittest(unittest.TestCase):
  def test_addition(self):
    assert 4 + 5 == 9

# quantitative assertions:
self.assertEqual(x, y)
self.assertNotEqual(x, y)
self.assertGreater(x, y)  # x > y
self.assertLess(x, y)
self.assertGreaterEqual(x, y)
self.assertLessEqual(x,y)

# logical assertions:
self.assertTrue()
self.assertFalse(my_function(test_value))

# membership assertions:
self.assertIn(x, y)  # x in y? 
self.assertNotIn(x,y)
self.assertIsInstance(x, y)

# exception assertions: 
with assertRaise(x): // code to test
```

### Logging, errors, & debugging

[Types of errors](https://docs.python.org/3/tutorial/errors.html): can be syntatic (problems with 'grammar' of Python), logical (problems with structure of program), or semantic (works computationally but does the wrong thing or has unintended behavior).

```Python
import logging
logging.basicConfig(filename=’fname.log’, level=logging.DEBUG)
log levels: critical, error, warning, info, debug, notset
logging.info(‘string to log’)
# https://docs.python.org/3/howto/logging.html

import pdb
pdb.set_trace() # launches a psuedo-shell
# type ‘n’ to run the next line of code
# type ‘c’ to run as normal
```





# Sources

## References

- [Official Python 3.5.2 documentation](https://docs.python.org/3/index.html)
- [Python cookbook](http://code.activestate.com/recipes/langs/python/)
- [Python package index (PyPI)](https://pypi.python.org/pypi)
- [Handy Python libraries for data cleaning](https://blog.modeanalytics.com/python-data-cleaning-libraries/)
- [PEP 8 - Style guide](https://www.python.org/dev/peps/pep-0008/)
- [Guido Rossum's Python history blog](http://python-history.blogspot.com/)


## Read

- [Coursera - Introduction to Python](https://www.coursera.org/learn/python/home/welcome) 
- [Coursera - Python data structures](https://www.coursera.org/learn/python-data)
- [Treehouse - Python basics](https://teamtreehouse.com/library/python-basics)
- [Treehouse - Python collections](https://teamtreehouse.com/library/python-collections)
- [Treehouse - Object-oriented Python](https://teamtreehouse.com/library/objectoriented-python)
- [Treehouse - Write better Python](https://teamtreehouse.com/library/write-better-python)
- [Treehouse - Datetimes in Python](https://teamtreehouse.com/library/dates-and-times-in-python)
- [Treehouse - Python testing](https://teamtreehouse.com/library/python-testing)
- [Treehouse - Functional Python](https://teamtreehouse.com/library/functional-python)
- [Treehouse - Python decorators](https://teamtreehouse.com/library/python-decorators)
- [Treehouse - Python comprehensions](https://teamtreehouse.com/library/python-comprehensions-2)
- [Treehouse - Data science basics](https://teamtreehouse.com/library/data-science-basics)
- [Treehouse - Python file I/O](https://teamtreehouse.com/library/python-file-io)
- [Treehouse - CSV & JSON in Python](https://teamtreehouse.com/library/csv-and-json-in-python)
- [DataCamp - Introduction to Python for data science](https://www.datacamp.com/courses/intro-to-python-for-data-science)
- [Iterables vs. iterators vs. generators](http://nvie.com/posts/iterators-vs-generators/)


## Unread

- [Generator comprehensions](http://nedbatchelder.com/blog/201605/generator_comprehensions.html)
- [DataCamp - Intermediate Python for data science](https://www.datacamp.com/courses/intermediate-python-for-data-science)
- [DataCamp - 18 most common questions about Python lists](https://www.datacamp.com/community/tutorials/18-most-common-python-list-questions-learn-python#gs.rD3n5og)
- [The Hitchhiker's Guide to Python](https://www.datacamp.com/courses/intermediate-python-for-data-science)
- [Google's Python class, with exercises](https://developers.google.com/edu/python/)
- [Full stack Python tutorial](https://www.fullstackpython.com/introduction.html) 
- [Lynda - Introduction to data analysis using Numpy](https://www.lynda.com/Numpy-tutorials/Introduction-Data-Analysis-Python/419162-2.html)
- [Udacity - Object-oriented programming with Python](https://www.udacity.com/course/programming-foundations-with-python--ud036)
- [Computational statistics in Python](http://people.duke.edu/~ccc14/sta-663/index.html)
- [Python's magic methods](http://farmdev.com/src/secrets/magicmethod/)
- [Applying operations over pandas dataframes](http://chrisalbon.com/python/pandas_apply_operations_to_dataframes.html)
- _High Performance Python_
- _Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython_  
- _Doing Math With Python_
- [_IPython Interactive Computing and Visualization Cookbook_](https://www.packtpub.com/big-data-and-business-intelligence/ipython-interactive-computing-and-visualization-cookbook)
- _Data Wrangling with Python_
- [Crazy! visualizes how Python executes your code](http://www.pythontutor.com/visualize.html#mode=edit)
- [donnemartin's IPython notebooks repository](https://github.com/donnemartin/data-science-ipython-notebooks)
- [nborwankar's IPython notebooks repository](https://github.com/nborwankar/LearnDataScience/tree/master/notebooks)
- [PythonChallenge.com](http://www.pythonchallenge.com/)
- [Python quiz](http://www.afterhoursprogramming.com/tutorial/Python/Python-Quiz/)
- [Intermediate Python challenges](http://wiki.openhatch.org/Intermediate_Python_Workshop/Projects)

# What's the scientific method?

... the quest to produce useful, high-quality knowledge.

## Institutions of scientific knowledge production

What are major institutions of knowledge production, and how well are they working?

### Academic apprenticeship

#### How does disciplinary training impact knowledge?

### Peer review & replicability

### Libraries & the accumulation of knowledge

## Academic vs. practitioner knowledge

## Expert vs. crowdsourced knowledge



# The research process

## Generating questions

## Operationalizing concepts

## Reviewing literature

### Strategies for finding articles

### Strategies for reading well

Adler, M. J. (). _How to read a book._

### What is a literature review?

The important thing to understand is that a literature review proper is a research method; it is not peripheral to research.

### What types of literature reviews exist?

## Sampling

## Choosing a research method

What are major study designs and purposes? E.g. experiment, difference-in-differences, etc. What exists; how things are connected; what might happen.

Qualitative methods are premised on the ontological position that the world is multifaceted; the epistemological position that one explores a multifaceted world through people's subjectivities; and the political position that including a wider range of subjectivities in knowledge production leads to more just, effective decision-making. See GEOG 425 - Final paper. 

- Surveys and questionnaires
- Interviews
    - Critical incident technique
- Participant-observation
- Focus group
    - Futures workshop

## Analyzing data

See [notes on qualitative methods,](qualitative-methods.html), [data analysis](data-analysis.html) and [models](models.html).

## Communicating results

See Miller (2005).

    



# Sources

## Cited

Miller, J. E. (2005). _The Chicago guide to writing about multivariate analysis._ Chicago, IL: University of Chicago Press.

## References

- [Eigenfactor.org](http://eigenfactor.org/)
- [Dev Nambi's links](https://github.com/DevNambi/blog-drafts/blob/master/490%20-%20Science%20and%20Research.md)
- Earning by major: [1,](https://cew.georgetown.edu/cew-reports/whats-it-worth-the-economic-value-of-college-majors/) [2,](http://www.hamiltonproject.org/charts/career_earnings_by_college_major/) [3](http://www.payscale.com/college-salary-report/majors-that-pay-you-back)
- [IEP entry for care ethics](http://www.iep.utm.edu/care-eth/)
- [ParticipatoryMethods.org](http://www.participatorymethods.org/)
- [Arnold Foundation's Evidence-Based Policy Initiative](http://www.arnoldfoundation.org/initiative/evidence-based-policy-innovation/)
- [Advanced Google search syntax](http://www.cpcstrategy.com/blog/2014/10/advanced-google-search-tips-infographic/)
- [Research Methods Knowledge Base](http://www.socialresearchmethods.net/kb/contents.php)
- [Center for Qualitative &amp; Multi-Method Inquiry](https://www.maxwell.syr.edu/cqmi.aspx)
- [Typeform online questionnaire software](https://www.typeform.com/)
- [Q-BANK question evaluation library](https://wwwn.cdc.gov/qbank/QUest/Quest.aspx)
- [Survey Quality Predictor 2.1](http://sqp.upf.edu/)

## Read

- Schmidt, J. Disciplined Minds.
- Abbott, A. Chaos of Disciplines.
- Kuhn, T.
- McCloskey, D. N. 
- Wenger, E.
- Schon, D.
- ["Buy less, think more"](https://storify.com/afg85/applying-critical-thinking-to-your-critical-readin)
- [Coursera - Questionnaire Design](https://www.coursera.org/learn/questionnaire-design)
- [Coursera - Measuring Causal Effects](https://www.coursera.org/learn/measuring-causal-effects)
- [More hypotheses, less trivia](http://allendowney.blogspot.com/2011/06/more-hypotheses-less-trivia.html)

## Unread

- _Causation: A very short introduction._
- [Academic research online](http://www.onlinecolleges.net/for-students/online-academic-research/)
- [Makessense stop!](http://crookedtimber.org/2014/05/13/makessense-stop/)
- [How do you read academic papers?](https://news.ycombinator.com/item?id=9245467)
- [A typology of reviews: an analysis of 14 review types and associated methodologies](http://onlinelibrary.wiley.com/doi/10.1111/j.1471-1842.2009.00848.x/pdf)
- [A comparative study of methodological approaches to reviewing literature](https://www.heacademy.ac.uk/system/files/resources/comparativestudy_0.pdf)
- [Everything is fucked: the syllabus](https://hardsci.wordpress.com/2016/08/11/everything-is-fucked-the-syllabus/)
- [Science made stupid](http://www.besse.at/sms/smsintro.html)
- [Wiki - Mertonian norms](https://en.wikipedia.org/wiki/Mertonian_norms)
- [You broke peer review; yes, I mean you](https://codeandculture.wordpress.com/2013/11/18/youbrokepeerreview/)
- [Scientific misbehavior in economics: Unacceptable research practice linked to perceived pressure to publish](http://blogs.lse.ac.uk/impactofsocialsciences/2014/07/23/scientific-misbehavior-in-economics/)
- [The rise and fall of debate in economics](http://www.joefrancis.info/economics-debate/)
- [Mathiness in the theory of economic growth](https://www.aeaweb.org/articles?id=10.1257/aer.p20151066)
- [Economics has a math problem](https://www.bloomberg.com/view/articles/2015-09-01/economics-has-a-math-problem)
- [Critical realism in economics](https://www.youtube.com/watch?v=ZHAxdJ-qs7s&amp;feature=youtu.be)
- [Sociologists need to be better at replication](https://orgtheory.wordpress.com/2015/08/11/sociologists-need-to-be-better-at-replication-a-guest-post-by-cristobal-young/)
- [Cut-throat academia leads to 'natural selection of bad science', claims study](https://www.theguardian.com/science/2016/sep/21/cut-throat-academia-leads-to-natural-selection-of-bad-science-claims-study?CMP=twt_gu)
- [7 biggest problems facing science, according to 270 scientists](http://www.vox.com/2016/7/14/12016710/science-challeges-research-funding-peer-review-process?linkId=27003407)
- [Why I worry experimental social science is headed in the wrong direction](http://chrisblattman.com/2015/12/07/if-you-run-field-experiments-this-might-be-paper-that-will-make-it-harder-to-publish-your-work-in-a-few-years/)
- [The fractal nature of scientific revolutions](http://andrewgelman.com/2005/05/20/selfsimilarity/)
- [Disciplines and disorientation](http://www.crassh.cam.ac.uk/gallery/video/simon-schaffer-disciplines-and-disorientation)
- [Are most medical studies wrong?](http://theness.com/neurologicablog/index.php/are-most-medical-studies-wrong/)
- [A large study replicates psychology experiments, but not most of their results](http://www.economist.com/news/science-and-technology/21662619-large-study-replicates-psychology-experiments-not-most-their-results-try?fsrc=scn/tw/te/bl/ed/tryagain)
- [A lot of research makes scientific evidence seem more “significant” than it is](https://www.painscience.com/articles/statistical-significance.php)
- [Understanding the geographical imagination](http://jgieseking.org/understanding-the-geographical-imagination/)
- [Geographical imaginations](http://territorialmasquerades.net/geographical-imaginations/)
- [Wiki - Mode 2](https://en.wikipedia.org/wiki/Mode_2)
- [Wiki - Discipline (academic)<br>](https://en.wikipedia.org/wiki/Discipline_(academia))
- [A conversation with Marion Nestle, PhD](http://defendingscience.org/conversation-marion-nestle-phd)
- [Aaron Swartz's essays on Wikipedia](http://www.aaronsw.com/weblog/wikiroads)
- [Wiki - What Wikipedia is not](https://en.wikipedia.org/wiki/Wikipedia:What_Wikipedia_is_not)
- [Understanding bias: a prerequisite for trustyworthy results](https://medium.com/@akelleh/understanding-bias-a-pre-requisite-for-trustworthy-results-ee590b75b1be#.r2q3cshpm)
- [What is research design?](https://drive.google.com/open?id=0B6XYyy1UbJ3XM25RNXFnU2l2Z28)
- [The nine circles of scientific hell](http://pps.sagepub.com/content/7/6/643.full.pdf+html)
- [A technical primer on causality](https://medium.com/@akelleh/a-technical-primer-on-causality-181db2575e41#.409ytztzt)
- [How to tell if correlation implies causation](https://statswithcats.wordpress.com/2015/01/01/how-to-tell-if-correlation-implies-causation/)
- [Are social science RTCs headed in the wrong direction?](http://chrisblattman.com/2015/12/14/are-social-science-rcts-headed-in-the-wrong-direction-a-roundup-of-the-discussion/)
- [Theory versus data: you shouldn't have to choose](https://www.bloomberg.com/view/articles/2016-02-17/theory-versus-data-you-shouldn-t-have-to-choose)
- [What Americans think of abortion](http://www.vox.com/a/abortion-decision-statistics-opinions)
- [Wiki - Activity theory](https://en.wikipedia.org/wiki/Activity_theory)
- [Q methodology](https://qmethod.org/)

# Environment

R packages are bundles of code, data, documentation and tests. R adds these packages to the search list by default: `grDevices, methods, stats, utils, graphics, datasets, base`; see [standary library package documentation](https://stat.ethz.ch/R-manual/R-devel/doc/html/packages.html).

```R
list.files()  # list files w/ wd
source("script_name.R")  # run a script

install.packages("package name")  # quotes not needed
library("package_name")  # loads package into current workspace (adds package to search list)
require("package_name")  # loads packages, but with warning messages and protections
# working w/ packages: http://faculty.washington.edu/kenrice/rintro/sess08.pdf
```

# Conventions

- R is case sensitive with 1-based indexing
- <http://stackoverflow.com/questions/11563154/what-are-replacement-functions-in-r)>
- <https://jeffknupp.com/blog/2012/11/13/is-python-callbyvalue-or-callbyreference-neither/>

```R
# operators: +, -, *, /, ^, %%, %*%
# logic: !, &, | (inclusive OR)
# when evaluating vectors, && and || evaluate only the first element
# relational: >, >=, !=,==
```

# Get more information

```R
getwd()  # identify working directory
ls()  # list all variables in the workspace
search()  # lists packages that R is using in this session

help(function_name)  # look at documentation
?function_name  # look at documentation
example(function_name)
args(function_name)

class(my_variable)  # check type
typeof(my_variable)
is.numeric(my_variable)
summary(my_data)  # 5 number summary from stats
ls(my_data)
```

# Datatypes

## Vectors

Vectors can hold a 1D array of numeric, character, raw, or logical data. The elements in a vector all have the same data type (AKA it's an atomic vector). Standard operations work row-wise.

```R
my_vec <- c(x1,x2,..., xn)  # create vector with combine function
my_vec <- vector(mode = "type", length = 5)  # create empty vector
names(my_vec) <- c("name1", "name2", ...)  # assign names to vector values

my_vector[i]
my_vector[c(h,i,j,k)]
my_vector[h:k]
my_vector[c(“name3”,"name5")]
my_vector[my_vector>2]

# NULL: the absence of a vector
# NA: the absence of values inside a vector
is.na(my_vec) --> returns logical vector
```
## Lists

A list is the 1D version of a data frame; it can contain different data types (AKA it's a recursive vector: a vector that can contain other vectors.

```R
my_list <- list(component1, ...)
my_list <- list("name1" = component1, ...)  # or list(name1 = ...); quotes not necessary
names(my_list) <-  c("name1", ...)

my_list[[1]]  # subsetting by index
my_list[[3]][5]
my_list[[“nm1”]]  # subsetting by name
my_list$var_name
my_list$varname[3]

# chained selection, returns 2:
x <- list(a = 1, b = list(r = 2, s = 3))
x[[c("b", "r")]]

new_list <- c(my_list, name=component)  # add new element to list
extended_list <- c(my_list, list(item))  # add new list to list-of-lists
list addressing: [ extracts a sublist, [[ or $ extract a value
```

## Factors

Factor: _a statistical data type used to store categorical variables_; may be nominal or ordinal. 

```R
my_fac <- factor(my_vec)  # create nominal factor
my_fac <- factor(my_vec, ordered=TRUE, levels=c("low", ..., "hi")  # create ordinal factor
# addressing same as for vectors

levels(my_vec)  # view factor levels
levels(my_vec) <- c("name", ..., "name")  # (re)name factor levels
```

## Matrices

Matrices store mXn data of a _single datatype_: numeric, character, raw, logical. Standard operators work element-wise; special matrix operators work row-wise.

```R
my_mat = matrix(1:9, byrow=TRUE, nrow=3)
my_mat = matrix(c(my_vec1, my_vec2), byrow=FALSE, nrow=2)

# ADD NAMES 
my_mat = matrix(..., dimnames=list(rname, cname))
rownames(my_mat) <- rname_vec
colnames(my_mat) <- cname_vec

# ADD A COLUMN OR ROW
my_mat = cbind(my_vec1,my_vec2)
my_mat = rbind(my_mat1,my_vec2)

# matrix addressing:
my_matrix[i,j]
my_matrix[1:2,5:7]
my_matrix[ ,4]
my_matrix[1:4, ]
```

## Dataframes

Dataframes are for mxn data where datatypes vary across (not within) columns.

```R
my_df <- data.frame(vec1, vec2, ..., vecn)

str(my_df)  # view structure of dataset
head(my_df)
tail(my_df, num_rows)

# data frame addressing: numeric, same as for matrix
my_df[1:3, “var_name”]
my_df$var_name equivalent to my_df[,"varname"]
my_df[my_boolean_vec, ]

subset(my_df, subset = some_condition)
```

## Datetimes

- lubridate, zoo, xts are relevant libraries

```R
today <- Sys.Date()
my_date <- as.Date("%Y-%m-%d")  # yyyy-mm-dd
my_date <- as.Date("2012-19-01", format = "%Y-%d-%m")
format(my_date, "format_code")  # get formatted strings from Date objects
# http://www.rdocumentation.org/packages/base/versions/3.3.1/topics/strptime
# %Y: 4-digit year (1982)
# %y: 2-digit year (82)
# %m: 2-digit month (01)
# %d: 2-digit day of the month (13)
# %A: weekday (Wednesday)
# %a: abbreviated weekday (Wed)
# %B: month (January)
# %b: abbreviated month (Jan)
# %H: hours as a decimal number (00-23)
# %M: minutes as a decimal number
# %S: seconds as a decimal number
# %T: shorthand notation for the typical format %H:%M:%S

now <- Sys.time()
my_time <- as.POSIXct("%Y-%m-%d %H:%M:%s")
# #days since 01/01/1970, #seconds since the first second of this day
# a common offset for programming languages; expedites calculations
```

# Control flow

A `for` loop has three parts: the sequence, the body, and the output object. _Before you start the loop, you must always allocate sufficient space for the output ... This is very important for efficiency: if you grow the for loop at each iteration (e.g. using c()), your for loop will be very slow._

```R
if (condition1) {
..    expr1
.. } else if (condition2) {
..    next  # jump to next iteration of loop
.. } else { 
..    break  # exit the loop immediately and completely
.. }

while (condition) { 
..    expr 
.. }

# loop by element
for (v in list_or_vector) { 
..    expr
.. }

# loop by index
# prefer to 1:length(my_object); handles empty objects better
for (in in seq_along(list_or_vector) {
..    expr
.. }
```

# Functions & FP

```R
my_fun <- function(arg1, arg2 = default_value, ...) {
..    expr  # last-evaluated expr returned by default; and also specify return(expr)
.. }
# SCOPING:
# each function call creates a local environment for that function
# calling the same function multiple times --> multiple new environments
# functions should never depend on variables that are not arguments, i.e. global variables
# variables defined inside a function are not available globally unless they are returned

function(arg1, arg2 = default_value, ...) { expr }  # anonymous function

# MARGIN=1 means apply row-wise; MARGIN=2, column-wise; MARGIN=c(1,2) means both
apply(my_array, MARGIN=1, fcn)
lapply(my_iterable, fcn_to_apply, arg1="argument", ...)  # returns a list
lapply(my_mats_list,"[", i,j)  # select ith row, jth col of each matrix in list of matrices
lapply(my_list_of_lists, '[[', "name")
unlist(lapply(...))  # returns a vector
sapply (my_iterable, fcn, arg1="argument", ...)  # returns simplest possible datatype, e.g. vector
vapply(my_iterable, fcn, FUN.VALUE = template_for_return_value, ..., use.names = TRUE)
# safer than sapply because you can specify what the return value should look like;
# it will otherwise raise an error:
# numeric(3)  # a vector of numerics, length==3
# logical(1)  # a single logical
# character(), integer()

# if length(my_vec)==2, returns vec with 'a' copies of my_vec[1], 'b' copies of my_vec[2]
rep(my_vec, c(a,b, ...))  

aggregate()
sweeps()
```


# Sources

## References

- [Official R documentation](https://www.r-project.org/)
- [R cookbook](http://www.cookbook-r.com/)
- [R documentation search engine](https://www.rdocumentation.org/)
- [R packages for data access](http://blog.revolutionanalytics.com/2016/08/r-packages-data-access.html)
- [Software Carpentry's R cheatsheet](http://swcarpentry.github.io/r-novice-inflammation/reference/)
- [RStudio's R cheatsheets](https://www.rstudio.com/resources/cheatsheets/#515)
- [Hadley Wickham's R style guide](http://adv-r.had.co.nz/Style.html)
- [Google's R style guide](https://google.github.io/styleguide/Rguide.xml)
- [R code style guide](https://4dpiecharts.com/r-code-style-guide/)
- [An R introduction to statistics (many tutorials)](http://www.r-tutor.com/)

## Read

- [DataCamp - Data analysis & statistical inference in R](https://www.datacamp.com/community/open-courses/statistical-inference-and-data-analysis)
- [DataCamp - Introduction to statistics](https://www.datacamp.com/introduction-to-statistics)
- [DataCamp - Introduction to R](https://www.datacamp.com/courses/free-introduction-to-r)
- [DataCamp - Intermediate R](https://www.datacamp.com/courses/intermediate-r)
- [DataCamp - Intermediate R practice](https://campus.datacamp.com/courses/intermediate-r-practice)

## Unread

- [One page R](http://togaware.com/onepager/)
- [DataCamp - Writing functions in R](https://campus.datacamp.com/courses/writing-functions-in-r)
- [DataCamp - Importing data](https://www.datacamp.com/courses/importing-data-in-r-part-1)
- [DataCamp - Cleaning data](https://www.datacamp.com/courses/cleaning-data-in-r)
- [DataCamp - Data manipulation](https://www.datacamp.com/courses/dplyr-data-manipulation-r-tutorial)
- [DataCamp - Table data manipulation](https://www.datacamp.com/courses/data-table-data-manipulation-r-tutorial)
- [DataCamp - R, Yelp, and the search for good Indian food](https://www.datacamp.com/community/open-courses/r-yelp-and-the-search-for-good-indian-food)
- [DataCamp - The Apply family of functions](https://www.datacamp.com/community/tutorials/r-tutorial-apply-family#gs.mGQX8hE)
- [DataCamp - Big Data analysis with R Revolution](https://www.datacamp.com/community/open-courses/big-data-revolution-r-enterprise-tutorial)
- [Jared Knowles' R bootcamp](http://jaredknowles.com/r-bootcamp/)
- [Elementary statistics with R](http://www.r-tutor.com/elementary-statistics)
- [Scalable data science with R: Scaling up, scaling out, or using R as an abstraction layer](https://www.oreilly.com/ideas/scalable-data-science-with-r)
- [R Style. An Rchaeological Commentary.](https://cran.r-project.org/web/packages/rockchalk/vignettes/Rstyle.pdf)
# How do search engines work?

In general, search engines work by crawling and [automatically indexing](information-architecture.html#cataloging-&-indexing) content (thus creating metadata). This index may be fairly shallow, e.g. based on contents of the <meta> tag or headers; it may also be quite deep, using natural language process (NLP) techniques like grammatical stemming. User search terms are then matched to the index. 

In the early days, there was a strong distinction between search engines and library catalogs; increasingly, though, [KOSs from IA](information-architecture.html#koss-by-role-in-ir) may play a role. 

## Major search algorithms

### PageRank

### Hilltop

### Topic-Sensitive 

### Hypertext Induced Topic Selection

### Panda

### Penguin

### Hummingbird 

(semantic reasoning and query rewriting)  

### RankBrain 

(machine learning)




# Monetized applications of search engines

## Marketing

### SEO

### SEM 

## Enterprise IR





# Ethical & social dimensions of search

## Information overload

## Privacy vs. personalization

## Political & cultural influences on search

## Social networks as IR tools

## Adversarial IR (spam)



# Sources

Hedden, H. (2016). _The accidental taxonomist_ (2e). Medford, NJ: Information Today, Inc.

## Unread

- [Computational Advertising on Social Networks](http://www.datasciencecentral.com/profiles/blogs/computational-advertising-on-social-network)
- [SEO tutorial](http://www.afterhoursprogramming.com/tutorial/SEO/Introduction/)


# What is security? 


## What are major categories of security threats & defenses?


## What are security-relevant certifications?

- Microsoft has (obviously) product-specific certifications; the IT infrastructure track of their MTA certification covers security and networking, but might be too basic. 
- CompTIA has security-relevant product-neutral certifications: A+, Network+, Security+, Cybersecurity, Advanced Security Practitioner. These look like the best place to start, to understand the underlying hardware and concepts. 
- EC-Council has Network Defender, Ethical Hacker, Security Analyst, and Forensic Investigator.
- CyberSecurity Institute has a Forensic Analyst cert.
- ISACA is mostly focused on information systems security but also offers Cybersecurity.
- (ISC)<sup>2</sup> has various certs for info systems and cyber security.




# What is operational risk? 

## What are information assurance strategies?




# Individual privacy

## What are different degrees of privacy?

Metadata, content, anonymity.

## Threat modeling

Planning a security system is complex. Security is interdependent, so the weakest link matters. Asking and answering these questions is the foundation of effective security: 

- Understand that __you__ have information assets. What are they? Who would find them valuable? 
- What must be protected (sensitive data, _any_ data, metadata, personal identity), and from who? 
- Given this, what is the risk of surveillance, and what vectors are most likely? 
- Given this, what are relevant security measures, and what is their cost (both financial and in terms of usability)?

Some basic facts and definitions:

- 90% of hacks come from user behavior, not security flaws
- Malware is harmful software
- A virus is a malicious executable file that gets installed and, like a biological virus, can make copies of itself. A virus might be disguised as a legitimate program and installed by a user; or the virus might take advantage of a security flaw to install itself.
- Worms
- Ransomware
- Keylogger
- DOS is when a single computer overwhelms a server with its requests; DDOS is when many computers do this. DDOS can be friendly or malicious, coming from a botnet of computers infected with a virus.
- Phishing is an attempt to access private information by way of legitimate-appearing spam email.

## Account security

### Research the platforms and companies you use

Do they provide the government direct access to user data? Do they notify users when such data has been furnished in compliance with a court order? Do they have a track record of defending their user data in court? Do they sell your data to other companies? Are they data-greedy or do they collect only when needed? Do they have strong enterprise security practices?

### Logins, pass phrases, MFA/2FA, and password managers

Use fake names for accounts; use an email address specifically intended for accounts management; use different user names.

Passwords under 6 characters long are unsafe, even if alphanumeric. Use an impersonal and human-memorable pass phrase like `alphabet beginner eventually literate ultimately poetic`; this is too long for a computer to guess, and its impersonality will keep hackers from guessing based on your social media clues.

Enable two-factor authentication [when possible](https://twofactorauth.org/), using a phone or [Yubikey](https://www.yubico.com/products/why-yubikey-wins/) (buy two!).

Consider using a password manager (although this does create a juicy central target for hackers): _LastPass, Keepass, 1password_. Password managers make it easy to take basic account security steps, like generating strong passwords; never re-using a password; never re-using a login; setting passsword expiration dates; and resetting passwords when specifics sites are compromised. Cloud-based password managers make passwords accessible from all devices but introduce vulnerabilities. If not using a password manager, you can write down passwords and store them away from the computer; or you can store them in an encrypted document on your computer. Then you need to personally follow best practices, which will be taxing.

For shared systems, it is imperative that all users follow these practices.

### Security questions and account recovery

Answers to security questions should be nonsense, or shifted by one key; store answers in a password manager. When possible, write your own security questions rather than using defaults. Have an email address specifically intended for account recovery.

### 3p access

Review third party access you've granted to accounts like Facebook and Google Drive; remove inactive ones.


## Personal computer

- Protect your computer with a strong login plus hard drive encryption (_FileVault, BitLocker_, or some native option); install or live boot from an operating system that doesn't store data (_TAILS, Chrubix_). 
- Use special programs to thoroughly wipe electronics before discarding, because some hackers will dumpster dive for data. 
- Put tape over your webcam so it can't be used to record you visually (this doesn't help with audio). 
- Maintain an [air gapped](https://www.schneier.com/blog/archives/2013/10/air_gaps.html) laptop as a last resort---or, as the polar opposite, use an insecure and disposable device when traveling. 


## Email & messaging

### Risky emails

Set your email so that images don't display automatically, which reduces tracking. Be extremely cautious about downloading files; use Gmail's file previewer or a file scanner, and set risky filetypes (`.js, .jse, .wsp, .wsh`) to open in a text editor by default. Don't open links directly from an email; instead, navigate directly to sites you're familiar with, or copy the link URL and run it through a search engine in quotes: `"http://somefakebankname.com/scam-attempt.php"`.

### Email encryption

Use [Mailvelope](https://chrome.google.com/webstore/detail/mailvelope/kajibbejlbohfaggdiogboambcijhkke) for end-to-end encryption that integrates with existing email providers.

### Instant messaging

Try [CryptoCat](https://crypto.cat/) or [Pidgin](https://pidgin.im/).

## Browsing

### Tor and/or VPN for privacy

The Tor browser enables anonymous browsing (activities not associated with particular IP), but it's difficult to set up. One workaround is the TAILS operating system, which comes bundled with Tor. 

For extra security, Tor can be run from a VPN, or a VPN can be run through another VPN: _[Tunnel Bear](https://chrome.google.com/webstore/detail/tunnelbear-vpn/omdakjcmkglenbhjadbccaookpfjihpa?hl=en), Private Internet Access, TorGuard, WiTopia, Privacy10_. With VPN, it's important to:

- Know the [logging policy of VPN providers](https://torrentfreak.com/which-vpn-providers-really-take-anonymity-seriously-111007/). Do they retain data? For how long? Whose jurisdiction are they under?
- Know your VPN status: VPN failure or DNS leaks can unexpectedly expose your browsing.
- Patch the [PPTP/IPv2 security flaw](http://lifehacker.com/5902397/how-to-make-vpns-even-more-secure).

### Searching

Try [DuckDuckGo](https://duckduckgo.com/), although its search algorithms aren't as good as Google's.

### Purchasing online

Favor credit cards over PayPal or debit cards. Don't store payment information on websites.

### Apps for ads, malicious scripts, trackers, etc.

- [ScriptSafe](https://chrome.google.com/webstore/detail/scriptsafe/oiigbmnaadbkfbmpbfijlflahbdbdgdf/reviews?hl=en) and [NoScript](https://addons.mozilla.org/en-US/firefox/addon/noscript/) block malicious scripts; 
- [HTTPSEverywhere](https://chrome.google.com/webstore/detail/https-everywhere/gcbommkclmclpchllfjekcdonpmejbdp?hl=en) and [KB SSL Enforcer](https://chrome.google.com/webstore/detail/kb-ssl-enforcer/flcpelgcagfhfoegekianiofphddckof/related?hl=en) push many sites into HTTPS mode; 
- [Privacy Badger](https://chrome.google.com/webstore/detail/privacy-badger/pkehgijcmpdhfbdbbnkijodmdjhbjlgp?hl=en), [Disconnect](https://chrome.google.com/webstore/detail/disconnect/jeoacafpbcihiomhlakheieifhpjdfeo?hl=en), and [TrafficLight](https://chrome.google.com/webstore/detail/trafficlight/cfnpidifppmenkapgihekkeednfoenal?hl=en) track trackers and alert you; 
- [AdBlock](https://chrome.google.com/webstore/detail/adblock/gighmmpiobklfepjocnamgkkbiglidom?hl=en), [AdGuard](https://chrome.google.com/webstore/detail/adguard-adblocker/bgnkhhnnamicmpeenaelnjfhikgbkllg?hl=en), and [SuperBlock](https://chrome.google.com/webstore/detail/superblock-adblocker/miijbmhjndcihicbljlcieiajhemmdeb) reduce online ads.


## Mobile phone

### Discourage unauthorized access

Swipe and PIN codes are not robust protection; use a pass phrase to prevent people from opening the phone, and make sure your mobile account itself is secured with a different pass phrase. Sophisticated hackers don't _need_ to log in to your phone, though, so protect your data with encryption (most phones provide this option in "Settings"; you just need to activate it). Install _[Lookout](https://play.google.com/store/apps/details?id=com.lookout&hl=en)_ or _[Prey](https://play.google.com/store/apps/details?id=com.prey&hl=en)_ to lock/locate/wipe your phone in case of theft or loss. Swich off wi-fi, GPS, and Bluetooth when not in use.

### Keep text messages, calls, and browsing private

- Data sent from a phone can be easily intercepted by IMSI devices posing as part of the network.
- Use _[Signal](https://play.google.com/store/apps/details?id=org.thoughtcrime.securesms&hl=en)_ to send texts that are encrypted _if both parties are using the app_ (so urge your friends to join as well).
- Use _[Orbot](https://play.google.com/store/apps/details?id=org.torproject.android&hl=en)_ and _[OrFox](https://play.google.com/store/apps/details?id=info.guardianproject.orfox&hl=en)_ to encrypt browsing; OrFox includes HTTPSEverywhere and NoScript.
- Use _[CSipSimple](https://play.google.com/store/apps/details?id=com.csipsimple&hl=en)_ to encrypt VoIP calls.

### Phones as audio recorders and locators

Know that phones can be used to locate you via GPS and cell phone towers. If this is a concern, leave your phone behind. Phones can also be activated remotely and used to record you, _even if they are turned off_; if this is a concern, place the phone in a refrigerator to muffle sounds, or remove its battery. A "burner phone" is not really a good option.

### Phone scams and phishing

If you recieve a call from someone who requests your personal information, be very wary:

- Ask for a number to call them back, and Google or otherwise verify the number before calling back.
- Ask why they want your information and listen closely to their rationale. People have a strong psychological tendency to accept _any_ reason following the word "because"; don't succumb to this; __interrogate them__. 
- They might drop minor facts about you to seem legitimate and win your trust; scammers trade information for information until they get what they need, like the social experiment [One Red Paperclip](https://en.wikipedia.org/wiki/One_red_paperclip). Don't trust people who seem to know about you.
- For this same reason, don't give out seemingly minor information. They might use it to call a company and pose as you to gain additional information.
- Don't let them create a subconscious sense of indebtedness by giving you small gifts. A small gift followed by a waiting period followed by a request is a tricky and powerful tactic.
- Don't be charmed by humor or intimidated by confidence. Good scammers have these qualities in abundance.


## Harrassment

### Doxing

Preventatively stalk yourself to see what information comes up. [Delete old accounts](http://backgroundchecks.org/justdeleteme/); opt-out of data broker listings ([1](https://www.privacyrights.org/data-brokers), [2](https://abine.com/optouts.php)); change your cell number; make sure your personal information isn't published on [Whois.net](https://www.whois.net) as part of your domain registration.

### Social

In case of harrassment, know who your allies are: some lawyers with an interest in civil rights, and civil rights organizations; some journalists; some politicians; others in your personal network with the capacity to advocate for you, who wil be seen as credible by a mainstream audience.

### DDoS




# Sources

## References

- [SocialEngineer.org](http://SocialEngineer.org)
- [CSOonline](http://www.csoonline.com/)
- [Security In a Box](https://securityinabox.org/en)
- [MyShadow.org](https://myshadow.org/)
- [Freedom of the Press Foundation](https://freedom.press/)
- [IdentityTheft.gov](http://IdentityTheft.gov)
- [Electronic Freedom Foundation's Surveillance Self-Defense kit](https://ssd.eff.org/)
- [Hackblossom's Guide to Feminist Cybersecurity](https://hackblossom.org/cybersecurity/)

## Read

- _No Place to Hide: Edward Snowden, the NSA, and the U.S. Surveillance State._
- [Your iPhone knows where you’ve been](http://blog.chron.com/techblog/2013/10/your-iphone-knows-where-youve-been-puts-it-on-a-map/)
- [IBM distributes USB malware cocktail at AusCERT security conference&nbsp;](https://nakedsecurity.sophos.com/2010/05/21/ibm-distributes-usb-malware-cocktail-auscert-security-conference/)
- [Criminals May Be Using Covert Mobile Phone Surveillance Tech for Extortion](http://www.slate.com/blogs/future_tense/2012/08/22/imsi_catchers_criminals_law_enforcement_using_high_tech_portable_devices_to_intercept_communications_.html)
- [Met police using surveillance system to monitor mobile phones](https://www.theguardian.com/uk/2011/oct/30/metropolitan-police-mobile-phone-surveillance)
- [Threat modeling](https://source.opennews.org/en-US/learning/security-journalists-part-two-threat-modeling/)
- [Account Security 101](https://crashoverridenetwork.tumblr.com/post/109948061867/account-security-101-passwords-multifactor)
- [Protecting your digital life in 7 easy steps](http://mobile.nytimes.com/2016/11/17/technology/personaltech/encryption-privacy.html)
- [EFF's security starter pack](https://ssd.eff.org/en/playlist/want-security-starter-pack)
- [How can I protect against social engineering hacks?](http://lifehacker.com/5933296/how-can-i-protect-against-hackers-who-use-sneaky-social-engineering-techniques-to-get-into-my-accounts)
- [Social engineering: the basics](http://www.csoonline.com/article/2124681/leadership-management/security-awareness-social-engineering-the-basics.html)
- [The fine art of BS](http://www.csoonline.com/article/2112400/it-audit/social-engineering--the-fine-art-of-bs--face-to-face--includes-video-.html)
- [How social engineers win your confidence](http://www.csoonline.com/article/2124219/security-awareness/mind-games--how-social-engineers-win-your-confidence.html)
- [Snowden persuaded other NSA workers to give up passwords](http://www.reuters.com/article/net-us-usa-security-snowden-idUSBRE9A703020131108)
- [Protect yourself against online fraud and identity theft](http://lifehacker.com/5858197/how-to-protect-yourself-from-online-fraud)
- [What it's like to steal someone's identity](http://www.csoonline.com/article/2126146/identity-theft-prevention/what-it-s-like-to-steal-someone-s-identity.html)
- [Anti-phishing and email hygiene](https://freedom.press/training/email-security-tips/)
- [Preventing doxing](https://crashoverridenetwork.tumblr.com/post/108387569412/preventing-doxing)
- [Why you should use a VPN](http://lifehacker.com/5940565/why-you-should-start-using-a-vpn-and-how-to-choose-the-best-one-for-your-needs)
- [How VPNs work](http://computer.howstuffworks.com/vpn.htm)
- [How to make your VPN even more secure](http://lifehacker.com/5902397/how-to-make-vpns-even-more-secure)
- [Which VPN providers really take privacy seriously?](https://torrentfreak.com/which-vpn-providers-really-take-anonymity-seriously-111007/)
- [Rapid response for compromised phones](https://freedom.press/training/rapid-responses-compromised-phones/)
- [Mobile security prevention tips](https://freedom.press/training/rapid-responses-compromised-phones/)

## Unread

- [Khan Academy - Modern Cryptography](https://www.khanacademy.org/computing/computer-science/cryptography/modern-crypt/v/the-fundamental-theorem-of-arithmetic-1)
- [The DDoS attack survival guide](http://www.csoonline.com/article/2125101/data-protection/the-ddos-attack-survival-guide--2013-edition.html)
- [Air gaps](https://www.schneier.com/blog/archives/2013/10/air_gaps.html)
- [Digital security for journalists](https://susanemcg.gitbooks.io/digital-security-for-journalists/content/)
- [Digital First Aid Kit](https://www.digitaldefenders.org/digitalfirstaid/#DFAk/)
- [How to be anonymous online](https://www.wired.com/2014/06/be-anonymous-online/)
- [Things to know about malware](https://ciso.uw.edu/site/files/Things_to_know_about_malware.pdf)
- [Web Security Fundamentals](https://info.varonis.com/web-security-fundamentals?utm_campaign=web-security-fundamentals&amp;utm_medium=display&amp;utm_source=facebook.com)
- [A 70-Day Web Security Action Plan](https://medium.com/@TeacherC/90dayactionplan-ff86b1de6acb#.deqwp8xez)
- [How to encrypt your entire life in less than an hour](https://medium.freecodecamp.com/tor-signal-and-beyond-a-law-abiding-citizens-guide-to-privacy-1a593f2104c3#.50hv1jppg)
- [Wiki - UK/USA Agreement](https://en.wikipedia.org/wiki/UKUSA_Agreement)
- [Wiki - 2013 Global surveillance disclosure](https://en.wikipedia.org/wiki/2013_Global_surveillance_disclosure)
- [Your privacy is under attack from several terrifying new laws](https://medium.freecodecamp.com/your-privacy-is-disappearing-one-law-at-a-time-heres-what-you-can-do-about-it-b85c814035a4#.1dys0ywaa)
- [PRISM Break](https://prism-break.org/)
- [Persona based commsec training matrix](https://github.com/AnarchoTechNYC/meta/wiki/Persona-based-commsec-training-matrix)
- [Data Privacy Lab](http://dataprivacylab.org/)
- [Harvard's privacy tools](http://privacytools.seas.harvard.edu/)
- [Does Big Data change the privacy landscape?](http://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-041715-033453?journalCode=statistics&amp;)
- [Chatting in secret while we're all being watched](https://theintercept.com/2015/07/14/communicating-secret-watched/)
- [How the NSA Recruits in a Post-Snowden World](http://www.thedailybeast.com/articles/2014/01/17/how-the-nsa-recruits-in-a-post-snowden-world.html)
- [When Drones Come to America, What Happens Then?](http://www.thedailybeast.com/articles/2013/05/18/when-drones-come-to-america-what-happens-then.html)
- Dev Nambi's links on [privacy](https://github.com/DevNambi/blog-drafts/blob/master/9914%20-%20Privacy%20and%20Security.md) and [security])https://github.com/DevNambi/blog-drafts/blob/master/9906%20-%20Security%20Links.md)
- ["If I want to learn about infosec, where do I start?"<br>](https://twitter.com/lyzidiamond/status/581377651270553602)
- _Data and Goliath: The Hidden Battles to Collect Your Data and Control Your World._
- [Obama’s Disclosure About Russian Hacking Is A Cybersecurity Gold Mine](http://www.huffingtonpost.com/entry/the-disclosure-of-russias-hacking-is-a-gold-mine-for-cybersecurity_us_5866b4cfe4b0eb5864894ed6): _Public disclosures like this enable collective cyber defense through information sharing ..._


# Sources

## References

- [Washington Center's learning communities repository](http://wacenter.evergreen.edu/)

## Read 

## Unread

- [Transfer of learning](http://www.nwlink.com/~donclark/hrd/learning/transfer.html)
- [High-impact or engaged learning practices](https://uwaterloo.ca/centre-for-teaching-excellence/resources/integrative-learning/high-impact-practices-hips-or-engaged-learning-practices)
- [Building a better discussion](http://www.chronicle.com/article/Building-a-Better-Discussion/231685/)
- [Arizona State & Knewton's grand experiment with adaptive learning](https://www.insidehighered.com/news/2013/01/25/arizona-st-and-knewtons-grand-experiment-adaptive-learning)
- [Sharing slow ideas](http://www.newyorker.com/magazine/2013/07/29/slow-ideas)

# What is SPARQL?

SPARQL is a query language for [databases](databases.html) that use [RDF](RDF.html) as their data model.  

# Sources

## Cited

## References

## Read

## Not read

-# Spatial Analysis

## Quantifying space and place

- A note about [geographical distance](https://gis.stackexchange.com/questions/17638/how-to-cluster-spatial-data-in-r): unless very short, it differs from the Euclidean distance calculable from geographic coordinates alone.
- Coordinates
- Projections

## Categories of spatial analysis

- [Ch. 10: Intro to spatial point pattern analysis](http://www.columbia.edu/~cjd11/charles_dimaggio/DIRE/resources/spatialEpiBook.pdf)
    - Summary statistics
        - Ripley's K function
    - Assessments of randomness (number and location of points)
        - 'G function
        - Moran's I and Geary's I "measure the tendency of events to cluster or the extent to which points close together have similar values on average than those farther apart"; they are global autocorrelation statistics.
        - Local Moran's I and Gi*
    - Models of point processes for simulating points data

### Clusters, hotspots, heatmaps

Although both produce a smooth-ish gradient visualization from points data (darker=more dense, lighter=less), [a hotspot map is different from a heat map](https://www.gislounge.com/difference-heat-map-hot-spot-map/). For a [heat map](https://www.gislounge.com/heat-maps-in-gis/), the density surface is created using a point density or kernel density approach. In either case, the resulting map is highly subjective because it depends on your choice of (1) how many input rasters per output raster AKA density unit; (2) length of bandwidth AKA search radius; (3) how points are counted (raw count, inverse-distance weighting, etc.). Here's a [script for pretty heatmapping in Python](http://www.sethoscope.net/heatmap/).

- [POINT DENSITY](http://help.arcgis.com/en/arcgisdesktop/10.0/help/index.html#/How_Point_Density_works/009z00000013000000/): Set neighborhood size. Impute to each map raster the number or weighted number of points in its neighborhood, divided by the area of the neighborhood.
- [KERNEL DENSITY](http://help.arcgis.com/en/arcgisdesktop/10.0/help/index.html#/How_Kernel_Density_works/009z00000011000000/): By means of a kernel density function, each point is replaced by a circular surface that is centered on the point; extends some distance (the search radius); takes its maximum value at the center, and reaches zero at the border. The area of the surface may sum to one or some other value determined by the weight of the point. Finally, the value of each raster is the sum of all partial surfaces that overlap it. 

A hotspot map defines densities AND includes statistical tests for whether high densities (clusters) are nonrandom. Two primary methods: Gi* and KDE. 

Is it possible that the [Getis-Ord Gi* (G-i-star) statistic](http://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-statistics-toolbox/h-how-hot-spot-analysis-getis-ord-gi-spatial-stati.htm) is a recipe for how points are counted (3, above) and that it can be calculated on a point or kernel density basis?

- Gi* is a z-score; it can be interpreted relative to a critical z score or assigned a p-value. 
- [CDC source](https://www.cdc.gov/dhdsp/maps/GISX/training/module3/files/3_hotspot_analysis_module.PDF)
- [](https://ceprofs.civil.tamu.edu/dlord/Papers/Kuo_et_al._GIS_Guidelines.pdf)
- [Worked example using spdep](http://www.bias-project.org.uk/ASDARcourse/unit6_slides.pdf): a polygon-based analysis
- [Overview of hotspot detection methods](https://www.mailman.columbia.edu/research/population-health-methods/hot-spot-detection)
- [Calculate nearest neighbor distances in R](http://mapas.mma.gov.br/i3geo/pacotes/rlib/win/spatstat/html/nndist.html)


How does this relate to clustering algorithms used outside of geography?

[Textbook on spatial points analysis](https://research.csiro.au/software/wp-content/uploads/sites/6/2015/02/Rspatialcourse_CMIS_PDF-Standard.pdf):

- All these summary statistics assume stationarity, and are used to characterize a point process by comparing it with random Poisson process:
    - F function: "cumulative distribution function of the empty space distance"
    - G function: "cumulative distribution function of the nearest-neighbour distance for a typical point in the pattern" 
    - Ripley's K: "the expected number of other points of the process within a distance r of a typical point of the process ... weighted and renormalised empirical distribution functions of the pairwise distances"
- To go beyond this, you start thinking in terms of specific point processes which give rise to different kinds of clustering
- "Exploratory techniques for investigating localised features in a point pattern include LISA (Local Indicators of Spatial Association), nearest-neighbour cleaning, and data      sharpening"


```{r}
library(spdep)



library(spatstat)

# create ppp object
my.ppp <- ppp(x.coordinates, y.coordinates, x.range, y.range)

# use plot(as.ppp(my.ppp)) to truncate plot to specified range
plot(my.ppp) 

# create a density map
plot(density(my.ppp))

# plot Ripley's K, indicates if clustering is nonrandom
plot(Kest(my.ppp))

```

### Spatial dependence

- [Tutorials by Nick Eubank](http://www.nickeubank.com/gis-in-r/): 
    - how to install spatstat when install.packages() fails
    - instructions for calculating Moran's I, "one of the most common measures of spatial dependence"
- [Moran's I](http://stats.idre.ucla.edu/r/faq/how-can-i-calculate-morans-i-in-r/)
- [Moran's I is a summary statistic; Getis Ord is for hotspots](http://pro.arcgis.com/en/pro-app/tool-reference/spatial-statistics/h-how-spatial-autocorrelation-moran-s-i-spatial-st.htm)
- [Slidedeck that covers regression and point patterns](http://scc.stat.ucla.edu/page_attachments/0000/0094/spatial_R_1_09S.pdf)
- [Tutorials by V. Gómez-Rubio](http://www.uclm.es/profesorado/vgomez/useR2014/): slightly clarifying but not enough to work from
- [Tutorials by Claudia Engel](http://www.rpubs.com/cengel248): good but doesn't touch on clustering



## Resources

- [Overview of GIS-related R packages](https://cran.r-project.org/web/views/Spatial.html)
- [A whole textbook on finding geospatial patterns in points](https://research.csiro.au/software/wp-content/uploads/sites/6/2015/02/Rspatialcourse_CMIS_PDF-Standard.pdf)
- [Vignettes for spatstat](https://cran.r-project.org/web/packages/spatstat/):
    - __[Getting started](https://cran.r-project.org/web/packages/spatstat/vignettes/getstart.pdf)__
- [James Cheshire's tutorials for mapping in R](http://spatial.ly/r/)



## Install notes

### [Tips for troubleshooting R package installation problems](https://support.rstudio.com/hc/en-us/articles/200554786-Problem-Installing-Packages)

- Package available on CRAN or other repository
    - Use a different CRAN mirror
    - Use a different default repository, maybe not CRAN
- Package name spelled correctly
- Package available for your version of R
- Connectivity issues

### spdep

- /usr/share/R/share/make/shlib.mk:6: recipe for target 'expm.so' failed
- ERROR: compilation failed for package ‘expm’ (dependency for spdep)
- installed with v. 3.3.2 (launch from command line)

### spatstat

- ```install.packages("spatstat")``` fails because polyclip dependency fails because "unable to load shared object ... undefined symbol: __cxa_throw_bad_array_new_length"
- ```conda install --channel https://conda.anaconda.org/jsignell r-spatstat``` fails because dependencies in conflict with r-modelmetrics
    - too risky to uninstall r-modelmetrics; depends on r-essentials and ```conda remove r-modelmetrics --force``` is too risky
    - try exploring dependency conflict:
        - ```conda info r-spatstat``` throws an error, so add r-spatstat channel to conda: ```conda config --append channels https://conda.anaconda.org/jsignell```
            - r-abind
            - r-base 3.3.1*
            - r-deldir >=0.0_21
            - r-goftest
            - r-matrix
            - r-mgcv
            - r-nlme
            - r-polyclip >=1.5_0
            - r-rpart
            - r-tensor
        - ```conda info r-modelmetrics```:
            - r-base 3.3.2*
            - r-rcpp
        - looks like it's a conflict between versions of r-base, no idea how to fix that
- install R through Synaptic; this version of R should open when RStudio is launched from the menu
    - r-base == v.3.2.3, super out of date :/
    - r-cran-spatstat
    - VICTORY!!!

    
# Sources

## References

- [ColorBrewer generates map color palettes&nbsp;](http://colorbrewer2.org/#type=sequential&amp;scheme=BuGn&amp;n=3)
- [QGIS documentation](http://linfiniti.com/dla/)
- [ESRI data models](http://support.esri.com/technical-article/000011644)
- [NaturalEarth attractive map layers](http://www.naturalearthdata.com/)

## Read

## Unread

- [ESRI - Going places with spatial analysis](http://www.esri.com/mooc/going-places)
- [Spatial concepts](http://teachspatial.org/spatial-concepts/)
- [Wiki - Spatial analysis](https://en.wikipedia.org/wiki/Spatial_analysis)
- [Spatial statistics](http://giscollective.org/tutorials/gis-techniques/spatial-statistics/)
- [The nature of geographic information](https://www.e-education.psu.edu/natureofgeoinfo/)
- [Geography for GIS](http://www.ncgia.ucsb.edu/cctp/units/geog_for_GIS/GC_index.html)
# What is SQL?

SQL is the standardized language used to access a database. SQL language provides for: (1) data definition/DDL statements that help you define the database and its objects; (2) data manipulation/DML statements that allow you to update and query data; (3) data control, allows you to grant the permissions to a user to access a certain data in the database.


## Notation and style guide

- all commands end with semicolon
- by convention, commands are CAPITALIZED
- NULL means a cell is unpopulated; NULL = NULL → F
- ORDER is very important! e.g. WHERE must precede GROUP BY; GROUP BY must precede HAVING 
- notation: table (field1, field2, … , fieldn)
- comments: -- comment, /* comment comment */
- mathematical operators: +,-,/,* 
- SELECT GPA, GPA*(size_HS/1000) AS scaleGPA FROM … 
- gives row dimension of table: SELECT count(*) FROM tname;


## Data definition

### Manage databases

- UPDATE
- OUTPUT
- MERGE

### Manage tables

Datatypes: http://www.w3schools.com/sql/sql_datatypes.asp, but they will depend on DBMS

- int stores numbers from -217483648 to 217483647
- double & real allow scientific notation: 2.5e4
- decimal is good for money: DECIMAL(5,2) has 5 digits, of which 2 are after the decimal point
- varchar(max_length), char(fixed_length) , clob/text is unlimited length , boolean is TRUE/FALSE 
- date is ‘yyyy-MM-dd’, time is ‘hh:mm:ss’, datetime or timestamp is ‘yyyy-MM-dd hh:mm:ss’

```SQL
-- create table: 

CREATE TABLE t (id INT PRIMARY KEY, col1 TYPE(size), col2 TYPE constraint constraint, col3 FOREIGN KEY REFERENCES other_table(fieldname), col4 DEFAULT ‘value’, … ); 
-- constraints: NOT NULL, UNIQUE, CHECK, DEFAULT
-- autopopulate date: CREATE TABLE tname (order_date date DEFAULT getdate()); 
CREATE TABLE tname (… UNIQUE (col1, col2, …) … );
CREATE TABLE tname ( … year INT CHECK (year>1950 AND year<1980)) …)
CREATE TABLE tname (... CHECK (price/hours < 20)); 

-- composite PK: 
CREATE TABLE t (name int, date int, PRIMARY KEY(name,date));
-- foreign key:
… FOREIGN KEY(c1) REFERENCES t1(c1), FOREIGN KEY(c2) REFERENCES t2(c1));
… REFERENCES … ON DELETE [action] ON UPDATE [action]

-- CASCADE: when the referenced row is deleted or updated, the respective rows of the referencing table will be deleted or updated.
-- NO ACTION: do nothing to the the referenced row
-- SET NULL: the values of the affected rows are set to NULLs
-- SET DEFAULT: the values of the affected rows are set to their default values

-- multicolumn foreign key: 
… FOREIGN KEY(c1, c2) REFERENCES t2(c1, c2)); 

-- autoincrement PK:
… id INT IDENTITY(#start, #inc) PRIMARY KEY, ...
… id INT AUTO_INCREMENT(#start, #inc) PRIMARY KEY, …

-- create PK
CREATE SEQUENCE name START WITH # INCREMENT BY #; 
CREATE TABLE t ( … id DEFAULT nextval(‘name’) PRIMARY KEY … );

--- delete table: 
DROP TABLE tname;
DROP DATABASE dname;
TRUNCATE TABLE tname;

--- add records: when SELECT is used as an expression in these kinds of queries, it needs to return only one column; 
--- so it sometimes helps to create a new identifier, say c1*c2*c3
INSERT INTO tname VALUES (key#, ‘string_value’, NULL, ...);

--- if select returns same schema: 
INSERT INTO t  (c1, c2) SELECT c1, c2 … 

--- add fields to confom to schema: 
SELECT col, 12, NULL
INSERT INTO t (col1, colx) VALUES (val, val), (val, val), (val, val)

--- delete records: 
--- some implementations disallow condition statements with a subquery that includes the affected table
DELETE FROM tname WHERE … 

--- delete all records: 
DELETE FROM table;

--- update records:
UPDATE tname SET c1=value/subquery, c2=value/subquery WHERE …
```

### Manage fields

```SQL
UPDATE tname
SET [fname] = 'Value'
WHERE fname = 'Value';
```

### Manage views

view data is not stored physically; every time you retrieve data from the view, the database reruns the underlying query.  most databases don't allow inserting new data or updating existing data in views.

```SQL
create a view: CREATE VIEW vname AS [query];
use a view: SELECT * FROM vname;
DROP VIEW vname;
```



## Data manipulation

### Generic query form

Match the following clauses with its definition: 

- SELECT: Columns to appear in result-set
- FROM: Specifies table(s) holding desired columns
- WHERE: Allows user to filter rows returned
- GROUP BY: Clusters rows by common column value
- HAVING: Filters GROUP BY clause
- ORDER BY: Sorts the output of the results-set in a desired sequence

```SQL
SELECT c1,c2 FROM tname WHERE ... ; SELECT * FROM tname;
operators: <, >, <=, >=, !=, ==, <>, OR, AND, NOT, BETWEEN, IS, NULL, EXISTS, ANY, ALL
SELECT * FROM tname WHERE cname=numeric;
SELECT * FROM tname WHERE cname=‘string’;
SELECT c1,c2 FROM t WHERE c1>= value OR c2=value;
SELECT … FROM … WHERE (complex_condition1) AND (complex_condition2)
SELECT * FROM t WHERE c BETWEEN condition1 AND condition2;
dep. on implementation, may not include endpoints
… NOT BETWEEN …
IS NULL, IS NOT NULL
EXISTS checks whether a subquery is empty; its expressive power encompasses:
… = ALL (SELECT …), … != ALL (SELECT …)
… = ANY (SELECT …), … != ANY (SELECT …)
fuzzy match: 
multiple char: SELECT * FROM … WHERE c LIKE ‘%string%’;
single char: SELECT * FROM … WHERE c LIKE ‘_tringvalue’;
```

### SELECT and display

- SELECT, FROM, CAST, RANK(), DENSE_RANK()
- DATE functions
- remove duplicates

```SQL
-- concatenate strings: really depends on platform
SELECT CONCAT(c1,c2) FROM t
SELECT c1+c2+’ ‘+c3 ..
SELECT c1 || ‘ ‘ || c2 AS email_address FROM … 
SELECT LENGTH(col)  … 
SELECT MAX(LENGTH(col)) … 
change case: SELECT UPPER(c1), LOWER(c2) … 
SELECT SUBSTR(col,#from,#inc) … 
1-based indexing
SELECT REPLACE(col,’from_string’,’to_string’) …
date: yyyy-mm-d
time: hh:mm:ss
datetime: yyyy-mm-dd hh:mm:ss
DATE(<timestring>,<modifier>,<modifier>, …)
timestring: “now”, “yyyy-mm-dd”
modifier: “-7 days”, “+2 months”
DATE(“datetime_string”) → yyyy-mm-dd
TIME(“datetime_string”) → hh:mm:ss
STRFTIME(“format_string”, ”datetime_string”, <modifier>)
“%d/%m/%Y”
```

#### WITH conditional filtering

- EXCEPT

#### Aggregate and GROUP BY

- GROUP BY, HAVING, aggregate functions (SUM, MIN, MAX, AVG, COUNT) 

```SQL
skip: SELECT <c> FROM <t> LIMIT <# of rows> OFFSET <skipped rows>;
SELECT <c> FROM <t> LIMIT <skipped rows>, <# of rows>; 
SELECT <c> FROM <t> OFFSET <skipped rows> ROWS FETCH NEXT <#> ROWS ONLY;
summary statistics: SELECT max/min/avg/sum/abs(cname) FROM tname; 
SELECT c FROM t T1 WHERE NOT EXISTS (SELECT * FROM t T2 WHERE T2.val>T1.val);
SELECT c FROM t WHERE c >= ALL (SELECT c FROM t); 
SELECT c FROM t WHERE NOT c <  ANY (SELECT c FROM t); 
SELECT round(col,#digits) … 
sort (default=ASC): SELECT * FROM tname ORDER BY fieldname DESC;
select distinct: SELECT DISTINCT c FROM t; SELECT c FROM t GROUP BY c;
count distinct: SELECT count(DISTINCT cname) FROM tname;
count non-NULL values: SELECT count(cname) FROM tname;
sort groups: SELECT c1, sum(c3) FROM t GROUP BY c1 ORDER BY sum(c3);
display in groups: SELECT c1, c2, count(col3) FROM t GROUP BY c1, c2; 
filtering groups: SELECT c1, count(c3) FROM t GROUP BY c1 HAVING cndn;
```

### FROM 

#### JOINs

- http://www.vertabelo.com/blog/technical-articles/sql-joins
- http://thomaslarock.com/2012/04/real-world-sql-join-examples/
- http://sqlmag.com/t-sql/t-sql-join-types

Which of the following are characteristics of a JOIN?

- Data is automatically sorted by the first column
- Tables are JOINed via PK/FK relationships
- More than two tables can be JOINed in one query
- The keyword GROUP is mandatory

```SQL
returns, for two mxn tables, a table of dim (m1*m2)x(n1+n2): SELECT * FROM t1, t2
join via select: SELECT * FROM table1,table2 WHERE table1.col = table2.col;
for joins note that, in SQL classic versus the graphic: 
the default JOIN is inner join
FULL JOIN doesn’t need “outer” clause
NATURAL JOIN doesn’t need “on” clause; good for well-built db 
aliases enable self-joins: SELECT c1 AS ... FROM t1 AS ... JOIN t2 AS … ON …
SELECT * FROM Student S1, Student S2 WHERE S1.GPA = S2.GPA AND S1.sID > S2.sID
multiple tables: SELECT * FROM t1 JOIN t1 ON … JOIN t3 ON … WHERE … 
set operators: let you query n tables and show the results as one table, provided both tables have the same #cols of the same type
removes duplicates: SELECT … FROM … UNION SELECT … FROM …; 
multiset operator: SELECT … FROM … UNION ALL SELECT … FROM ...;
SELECT … FROM … INTERSECT SELECT … FROM ...;
SELECT * FROM <self_join> WHERE <join_cndn> AND <select_cndn>
except: SELECT ... FROM ... EXCEPT SELECT … FROM...;
SELECT * FROM ... WHERE * IN (subquery) AND * NOT IN (subquery) 
```

#### Subqueries

- simple
- correlated

```SQL
SUBQUERIES … the database will first check the subquery then check the final query, e.g.: SELECT name FROM city WHERE rating = (SELECT rating FROM city WHERE name = 'Paris');
subqueries can be in the WHERE clause …
in FROM clause, reducing need for calculation in SELECT and WHERE ..
and in the SELECT clause 
used as an expression, it's important the subquery return exactly one col 
compare with a set of values instead of a single value: … WHERE … IN (subquery);
correlated subqueries, i.e. dependent on the main query; subqueries can use tables from the main query, but the main query can't use tables from the subquery. 
good for debugging, e.g. SELECT * FROM country WHERE area <= (SELECT min(area) FROM city WHERE city.country_id = country.id);
use aliases for self-correlated subqueries: SELECT * FROM city as c1 WHERE c1.rating > (SELECT avg(c2.rating) FROM city AS c2 WHERE c1.country_id=c2.country_id);
exists operator: SELECT * FROM country WHERE EXISTS (SELECT * FROM mountain WHERE country.id = mountain.country_id);
```

### Set operations

Relational algebra is the formal math underlying SQL. Unlike SQL, it’s set-based, so it automatically removes duplicates from its ‘results’. RA operators are applied to expression, - trees, or assignment statements:

- select rows: σcondition ∧ condition  tname
- project columns: πcol_name, col_name
- compose:  πcol_name, col_name(σcondition ∧ condition )
- operators: 
- and, ∧; or, ∨; union, ∪
- cross product, x: union of schemas, every possible combo of tuples (ntuples(A)*ntuples(B))
- natural join, ⋈: match cols & drop duplicates; A⋈B ≡ πschema(A) ∪ πschema(B)(σA.col = B.col ∧ …  (AxB))
- theta join: A ⋈θ B where A⋈θB ≡ σθ(AxB); default join for many DBMS
- rename, ⍴Name(cname, …) (A): unifies schemas to satisfy conditions of set operators and to allow disambiguation in self-joins; ⍴Name(A), ⍴cname, … (A)
- difference, -
- intersection, ⋂ where A⋂B ≡ A - (A-B), A⋂B ≡ A⋈B 
- symmetric difference: (A-B)∪(B-A)





# T-SQL

- [https://docs.microsoft.com/en-us/sql/t-sql/functions/aggregate-functions-transact-sql](https://docs.microsoft.com/en-us/sql/t-sql/functions/aggregate-functions-transact-sql)
- Inclusive: SELECT … WHERE [date] BETWEEN ‘20120225’ AND ‘20120230’;
- Exclusive: SELECT … WHERE [date] > 2012-02-25’ AND [date] < ‘20120230’; 
    - Note that dates are given as strings
- Retrieve date, modify date & alias: SELECT DATEADD(DAY, 7, OrderDate) AS "EstimatedDeliveryDate" FROM Sales.SalesOrderHeader WHERE MONTH(OrderDate) = 6 AND YEAR(OrderDate) = 2007;
- Numeric functions, aliasing, conditional selection. date functions: SELECT COUNT(*) AS 'HowManyMarchOrders', SUM(TotalDue) AS 'TotalDueForMarch', AVG(TotalDue) AS 'AvgOrderTotal', 
    - SELECT COUNT(*) [HowManyMarchOrders] FROM ... 
- MIN(TotalDue) AS 'CheapestOrder', MAX(TotalDue) AS 'CostliestOrder'  FROM Sales.SalesOrderHeader WHERE MONTH(OrderDate) = 5 AND YEAR(OrderDate) = 2008;
- SELECT CustomerID, COUNT(TotalDue) AS '#orders', SUM(TotalDue) AS '$orders' FROM Sales.SalesOrderHeader WHERE YEAR(OrderDate) = 2007 GROUP BY CustomerID HAVING COUNT(TotalDue) > 1 - ORDER BY SUM(TotalDue) DESC;
- SELECT CONVERT(CHAR(20), DATEADD(DAY, 30, GETDATE()), 101) AS [30 Days From Today];
- SELECT DISTINCT SalesPersonID FROM Sales.SalesOrderHeader oh INNER JOIN Sales.SalesOrderDetail od ON oh.SalesOrderID = od.SalesOrderID WHERE ProductID = 777 ORDER BY SalesPersonID;




# Examples

```SQL
SELECT c.CustomerID, c.TerritoryID, COUNT(o.SalesOrderid) AS [Total Orders],
    DENSE_RANK() OVER (PARTITION BY c.TerritoryID ORDER BY COUNT(o.SalesOrderid)) AS [Rank]
FROM Sales.Customer c LEFT OUTER JOIN Sales.SalesOrderHeader o ON c.CustomerID = o.CustomerID
WHERE DATEPART(year, OrderDate) = 2007
GROUP BY c.TerritoryID, c.CustomerID;
```

Select product_id, name and selling start date for all products that started selling before 01/01/2006. Use the CAST function to display the date only. You need to work with the Production.Product table. The syntax for CAST is CAST(expression AS data_type), where expression is the column name we want to format and  we can use DATE as data_type for this question to display just the date.

```SQL
SELECT ProductID, Name, CAST(SellStartDate AS DATE) SellStartDate
FROM Production.Product
WHERE SellStartDate < '01/01/2006';
```

Select the product id, name, and list price for the product(s) that has the highest list price. You need to work with the Production.Product table. You’ll need to use a simple subquery to get the maximum list price and use it in the WHERE clause.

```SQL
SELECT ProductID, Name, ListPrice
FROM Production.Product
WHERE ListPrice = (SELECT MAX(ListPrice) FROM Production.Product);
```

Modify the following query to add a column that identifies the frequency of repeat customers and contains the following values based on the number of orders during 2007:

```SQL
SELECT c.CustomerID, c.TerritoryID, COUNT(o.SalesOrderid) AS 'Total Orders',
    CASE
        WHEN COUNT(o.SalesOrderid) = '0'
            THEN 'No Orders'
        WHEN COUNT(o.SalesOrderid) = '1'
            THEN 'One Time'
        WHEN COUNT(o.SalesOrderid) BETWEEN '2' AND '5'
            THEN 'Regular'
        WHEN COUNT(o.SalesOrderid) BETWEEN '6' AND '12'
            THEN 'Often'
        ELSE 'Very Often'
    END AS 'Order Frequency'
FROM Sales.Customer c LEFT OUTER JOIN Sales.SalesOrderHeader o ON c.CustomerID = o.CustomerID
WHERE YEAR(OrderDate) = 2007
GROUP BY c.TerritoryID, c.CustomerID
ORDER BY 'Order Frequency' DESC;
```

Write a SQL query to generate a list of customer ID's that have never placed an order before. Sort the list by CustomerID in the ascending order.

Solution with JOIN:

```SQL
SELECT CustomerID
FROM Sales.Customer c LEFT OUTER JOIN Sales.SalesOrderHeader h
ON c.CustomerID = h.CustomerID
WHERE h.CustomerID IS NULL
ORDER BY CustomerID ASC; 
```

Solution with subquery:

```SQL
SELECT CustomerID
FROM Sales.Customer
WHERE CustomerID NOT IN 
    (SELECT CustomerID FROM Sales.SalesOrderHeader)
ORDER BY CustomerID ASC;
```




# Sources

## References

- [w3schools SQL tutorial](http://www.w3schools.com/sql/default.asp)
- [Weller’s SQL cheatsheet](http://weller.engl.uw.edu/mysql/)
- [Treehouse - SQL basics cheatsheet](https://github.com/treehouse/cheatsheets/blob/master/sql_basics/cheatsheet.md)
- [Treehouse - SQL reporting cheatsheet](https://github.com/treehouse/cheatsheets/blob/master/reporting_with_sql/cheatsheet.md)
- [Treehouse - SQL to modify data cheatsheet](https://github.com/treehouse/cheatsheets/blob/master/modifying_data_with_sql/cheatsheet.md)
- [Codecademy - SQL Glossary](https://www.codecademy.com/articles/sql-commands)

## Archive

- [SQL queries](https://academy.vertabelo.com/course/sql-queries)
- [Operating on data in SQL](https://academy.vertabelo.com/course/operating-on-data-in-sql)
- [Creating tables](https://academy.vertabelo.com/course/creating-tables-in-sql)
- [SQL Style Guide](http://www.sqlstyle.guide/)
- [Treehouse - Reporting with SQL](https://teamtreehouse.com/library/reporting-with-sql)
- [SQL](https://lagunita.stanford.edu/courses/DB/SQL/SelfPaced/about)

## Inbox

- [7 steps to mastering SQL](http://www.kdnuggets.com/2016/06/seven-steps-mastering-sql-data-science.html)
- [Table Transformations](https://www.codecademy.com/learn/sql-table-transformation)
- [SQL Quiz](http://www.afterhoursprogramming.com/tutorial/SQL/SQL-Quiz/)
- [Learn SQL the Hard Way](http://sql.learncodethehardway.org/book/)


# SQL Server

## SQL Server Configuration Manager 

Shows every instance of a SQL Server product on a machine; lets you restart a process and enable protocols for communication between client & server.

## SQL Server Management Studio

- **Activity Monitor:** use for identifying bottlenecks & kill process if needed (but might cause data corruption!)
- **SQL Server Profiler:** lets you start a trace for ongoing tracking of activity; can specify activities of interest in detail




# T-SQL

## Legend

Per Buyham and Guyer (2017), the syntax for [syntax documentation of T-SQL statements](https://docs.microsoft.com/en-us/sql/t-sql/statements/statements) uses the following conventions:

- **KEYWORD**
- **\[** optional syntax item, brackets not typed **\]**
- use this syntax item **|** OR this syntax item
- **{**required syntax item, braces not typed**}**
- **\<syntax block label\> ::=** for documentation purposes only
- **[,...n]** indicates repeated elements with comma delimitation
- **[...n]** indicates repeated elements with space delimitation

```SQL
-- syntax documentation
CREATE TABLE
    [ database_name. [ schema_name ] . | schema_name. ] table_name   
    ( { <column_definition> } [ ,...n ] )   
[ ; ]  

-- example expression
CREATE TABLE dbo.tname
    (
    fname,
    fname
    );
```
    
    


## Manage objects
    
### Databases

```SQL
CREATE DATABASE dbname

USE SQLservername.databasename.schemaname.tablename
```

### Schemas

- [https://stackoverflow.com/questions/1062075/why-do-table-names-in-sql-server-start-with-dbo](https://stackoverflow.com/questions/1062075/why-do-table-names-in-sql-server-start-with-dbo)
- [http://www.sqlteam.com/article/understanding-the-difference-between-owners-and-schemas-in-sql-server](http://www.sqlteam.com/article/understanding-the-difference-between-owners-and-schemas-in-sql-server)

```SQL
CREATE SCHEMA sname AUTHORIZATION [dbo]
```

### Tables 

- **Filegroups:** A database has a default PRIMARY filegroup but you can create other filegroups, e.g. to hold tables that should be read-only.
- **Inspect relationships:**
    - [Database] > [Table] > Keys
    - [Database] > Database diagrams

```SQL
CREATE TABLE   
    [ database_name . [ schema_name ] . | schema_name . ] table_name   
    [ AS FileTable ]  
    ( {   <column_definition>   
        | <computed_column_definition>    
        | <column_set_definition>   
        | [ <table_constraint> ]   
        | [ <table_index> ] }  
          [ ,...n ]    
          [ PERIOD FOR SYSTEM_TIME ( system_start_time_column_name   
             , system_end_time_column_name ) ]  
      )  
    [ ON { partition_scheme_name ( partition_column_name )   
           | filegroup   
           | "default" } ]   
    [ TEXTIMAGE_ON { filegroup | "default" } ]   
    [ FILESTREAM_ON { partition_scheme_name   
           | filegroup   
           | "default" } ]  
    [ WITH ( <table_option> [ ,...n ] ) ]  
[ ; ]  

<column_definition> ::=  
column_name <data_type>  
    [ FILESTREAM ]  
    [ COLLATE collation_name ]   
    [ SPARSE ]  
    [ MASKED WITH ( FUNCTION = ' mask_function ') ]  
    [ CONSTRAINT constraint_name [ DEFAULT constant_expression ] ]   
    [ IDENTITY [ ( seed,increment ) ]  
    [ NOT FOR REPLICATION ]   
    [ GENERATED ALWAYS AS ROW { START | END } [ HIDDEN ] ]   
    [ NULL | NOT NULL ]  
    [ ROWGUIDCOL ]  
    [ ENCRYPTED WITH   
        ( COLUMN_ENCRYPTION_KEY = key_name ,  
          ENCRYPTION_TYPE = { DETERMINISTIC | RANDOMIZED } ,   
          ALGORITHM = 'AEAD_AES_256_CBC_HMAC_SHA_256'  
        ) ]  
    [ <column_constraint> [ ...n ] ]   
    [ <column_index> ]  

<data type> ::=   
[ type_schema_name . ] type_name   
    [ ( precision [ , scale ] | max |   
        [ { CONTENT | DOCUMENT } ] xml_schema_collection ) ]   

<column_constraint> ::=   
[ CONSTRAINT constraint_name ]   
{     { PRIMARY KEY | UNIQUE }   
        [ CLUSTERED | NONCLUSTERED ]   
        [   
            WITH FILLFACTOR = fillfactor    
          | WITH ( < index_option > [ , ...n ] )   
        ]   
        [ ON { partition_scheme_name ( partition_column_name )   
            | filegroup | "default" } ]  

  | [ FOREIGN KEY ]   
        REFERENCES [ schema_name . ] referenced_table_name [ ( ref_column ) ]   
        [ ON DELETE { NO ACTION | CASCADE | SET NULL | SET DEFAULT } ]   
        [ ON UPDATE { NO ACTION | CASCADE | SET NULL | SET DEFAULT } ]   
        [ NOT FOR REPLICATION ]   

  | CHECK [ NOT FOR REPLICATION ] ( logical_expression )   
}   

    
-- Add field to existing table
CREATE




```

#### Datatypes

[Details here;](https://docs.microsoft.com/en-us/sql/t-sql/data-types/data-types-transact-sql) also note that SQL Server's data types are [mapped to ISO standard data types.](https://docs.microsoft.com/en-us/sql/t-sql/data-types/data-type-synonyms-transact-sql)

- **Exact numeric:** NUMERIC, BIGINT, INT, SMALLINT, TINYINT, DECIMAL, BIT, MONEY, SMALLMONEY
- **Approximate numeric:** FLOAT, REAL 
- **Date and time:** DATE, TIME, DATETIME, DATETIME2, DATETIMEOFFSET, SMALLDATETIME
- **Character:** CHAR, VARCHAR, TEXT    
    - **Unicode character:** NCHAR, NVARCHAR, NTEXT
- **Binary:** BINARY, IMAGE, VARBINARY
- **Special purpose:** CURSOR, HIERARCHYID, SQL_VARIANT, TABLE, TIMESTAMP, UNIQUEIDENTIFIER, XML
    - [Spatial data types](https://docs.microsoft.com/en-us/sql/relational-databases/spatial/spatial-data-types-overview)

#### Temporal tables

Temporal tables (only SQL Server 2016) automatically maintain the history of the table, which can then be queried. The fields ValidFrom, ValidTo, and PERIOD FOR SYSTEM_TIME are required:

```SQL
CREATE TABLE Inventory ([InventoryID] int NOT NULL PRIMARY KEY CLUSTERED, 
[ItemName] nvarchar(100) NOT NULL, 
[ValidFrom] datetime2 (2) GENERATED ALWAYS AS ROW START, 
[ValidTo] datetime2 (2) GENERATED ALWAYS AS ROW END, 
PERIOD FOR SYSTEM_TIME (ValidFrom, ValidTo))    
WITH (SYSTEM_VERSIONING = ON); 

SELECT [StockItemName]
FROM [WideWorldImporters].[Warehouse].[StockItems]
FOR SYSTEM_TIME AS OF '2015-01-01'
WHERE StockItemName like '%shark%'
```

### Views

- **Create view:** [Database] > [Views] > right click. This is done to facilitate reporting, since data that is logically related (city and states) may be scattered across multiple tables; however, it create a penalty for writing data. 
- **Views can be made persistent to increase performance:** right click view > Script View as > ALTER To > New Query Editor Window > Add “WITH SCHEMABINDING” under “ALTER VIEW” line > Execute > Refresh Object Explorer pane > Right click on Indexes > Clustered index > Add columns.



## Manage performance

### Splitting the database

Installing logs and data on different drives gives a performance boost.

### Creating indexes

Recall that PKs are indexed automatically.

```SQL
-- Covering index
CREATE NONCLUSTERED INDEX IX_Address_PostalCode -- give index a name
ON Person.Address (PostalCode) -- specify table and key
INCLUDE (AddressLine1, AddressLine2, City, StateProvinceID); -- add other fields

-- Filtered index
CREATE NONCLUSTERED INDEX DesignEngineer
ON HumanResources.Employee (BusinessEntityID);
WHERE JobTitle = 'Design Engineer' -- filter

-- Columnstore index
CREATE NONCLUSTERED COLUMNSTORE INDEX csindx_simple 
ON SimpleTable (OrderDateKey, DueDateKey, ShipDateKey) 
WITH (DROP_EXISTING =  ON, -- drops and rebuilds an existing index of the same name 
    MAXDOP = 2) -- for parallel processing
    ON "default";
```

### Fixing index fragmentation

Fragmentation can be identified with [sys.dm_db_index_physical_stats,](https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-db-index-physical-stats-transact-sql) a SQL Server [dynamic management function](https://blogs.msdn.microsoft.com/sqlcan/2012/05/24/a-microsoft-sql-server-dmvdmf-cheat-sheet/) **(DMF):**

```SQL
SELECT * 
FROM sys.dm_db_index_physical_stats (   
    { database_id | NULL | 0 | DEFAULT }  
  , { object_id | NULL | 0 | DEFAULT }  
  , { index_id | NULL | 0 | -1 | DEFAULT }  
  , { partition_number | NULL | 0 | DEFAULT }  
  , { mode | NULL | DEFAULT | DETAILED | SAMPLE | LIMITED | }  
)

SELECT * 
FROM sys.dm_db_index_physical_stats (DB_ID(N'AdventureWorks'), 
    OBJECT_ID(N'Person.Address'), NULL, NULL , 'DETAILED'); 
-- Column **Avg_fragmentation_in_percent** depicts logical fragmentation
-- Column **Avg_page_space_used_in_percent** depicts internal fragmentation
```

Once identified, fragmentation can be repaired in the following ways:

```SQL
-- Recreate 
CREATE INDEX WITH DROP_EXISTING;

-- Rebuild 
ALTER INDEX ... REBUILD;

-- Reorganize 
ALTER INDEX ... REORGANIZE 
```


### Creating memory-optimized tables

In-memory AKA **memory-optimized tables** are used to improve performance of read-write tables. The keyword `GO` causes preceding commands to be submitted as a batch, and `USE` ensures that the table is created within the right database:

```SQL
ALTER DATABASE dbname
ADD FILEGROUP fgname
CONTAINS MEMORY_OPTIMIZED_DATA;

ALTER DATABASE dbname
ADD FILE (Name = ‘fname’, Filename ‘fpath/fname’)
TO FILEGROUP fgname;

GO

USE dbname
GO

CREATE TABLE tname (fdname INT IDENTITY(1,1) PRIMARY KEY NONCLUSTERED, fdname, fdname)
WITH (MEMORY-OPTIMIZED=ON)
```





# Sources

## Cited

Buyham, R. & Guyer, C. (2017, March 14). Transact-SQL Syntax Conventions-Transact-SQL. Microsoft. Retrieved from [https://docs.microsoft.com/en-us/sql/t-sql/language-elements/transact-sql-syntax-conventions-transact-sql](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/transact-sql-syntax-conventions-transact-sql)

## References

- [Syntax documentation of T-SQL commands](https://docs.microsoft.com/en-us/sql/t-sql/statements/statements)

## Read

## Unread

- [Core Concepts for SQL Server](http://www.lynda.com/SQL-Server-tutorials/Database-Fundamentals-Core-Concepts/385693-2.html)
- [Storage in SQL Server](http://www.lynda.com/SQL-Server-tutorials/Database-Fundamentals-Storage/385694-2.html)
- [Data Manipulation in SQL Server](http://www.lynda.com/SQL-Server-tutorials/Database-Fundamentals-Creating-Manipulating-Data/385697-2.html)# Probability & statistics

## Notes

- https://docs.google.com/document/d/1e0wdTbj6TfpLqfOfYjzqOX17L0pUmNGGfSIdr-n_0BE/edit?usp=drive_web
- https://docs.google.com/document/d/1akczbgY2f6M-y2FHY649C917KToSU8tj2Y1A7bCfq7k/edit?usp=drive_web
- https://docs.google.com/document/d/1gWr73U7uhIxzi8MOODNMafwDK7l-BDg3HsCEyc4xaGE/edit?usp=drive_web

## ... in R

```R
rowSums(my_mat)
colSums(my_mat)
mean(my_vec, trim = 0, na.rm = FALSE, ...)  
# trim is used to drop some observations from both end of the sorted vector; 
# When trim = 0.3, 3 values from each end will be dropped from the calculations to find mean. 
# na.rm is used to remove the missing values from the input vector. 
# If there are missing values, then the mean function returns NA; 
# To drop the missing values from the calculation use na.rm = TRUE.

# elementwise sum
my_vec1 + my_vec2  
# sum of vector elements: 
sum(my_vector)

mean(my_vector)
seq(a,b,length.out=n)
crossprod(x_vec,y_vec)

ceiling(x)
floor(x)
trunc(x, ...)
round(x, digits = 0)
signif(x, digits = 6)

# absolute value
abs()
# absolute difference
abs(x1-x2) 

mean()
nchar(my_vec)  
# counts number of characters in a vector of characters (strings)

max(my_data, na.rm = FALSE, ...)
min(my_data)
summary()
var()
median()

# confidence intervals: 
inference(my_dav, type="ci", method="simulation", conflevel=0.9, est="mean", boot_method="perc")
# method=simulation, theoretical
# boot_method=perc, se
# est=mean, median, proportion

# hypothesis testing: 
inference(y = nc$weight, x = nc$habit, est = "mean", type = "ht", null = 0, alternative = "twosided", method = "theoretical")
# alternative=”greater”
inference(us12$response, est = "proportion",  type = "ci", method = "theoretical",  success = "atheist")

# compare: 
by(numerical_dataset, categorical_dataset, mean/length/...)
cor(var1, var2)

# http://www.rdocumentation.org/packages/stats/versions/3.3.1/topics/sd?

# probability (in/dependent events and simulating event sequences)
# simulate: 
possible_values <- c("heads", "tails")
sample(possible_values, size=, replace=, prob=c())

# generate sample distribution: The sampling distribution is calculated by resampling from the population,
# the bootstrap distribution is calculated by resampling from the sample; 
# To construct the 95% bootstrap confidence interval using the percentile method, 
# we estimate the values of the 5th and 95th percentiles of the bootstrap distribution.
# Set up an empty vector of 5000 NAs to store sample means:
sample_means50 <- rep(NA, 5000)

# Take 5000 samples of size 50 of 'area' and store all of them in 'sample_means50'.
for (i in 1:5000) {
 samp <- sample(area, 50)
 sample_means50[i] <- mean(samp)
}

# View the result. If you want, you can increase the bin width to show more detail by changing the 'breaks' argument.
hist(sample_means50, breaks = 13)

# calculate 95 pct conf interval:
se <- sd(samp)/sqrt(60)
lower <- sample_mean - 1.96 * se
upper <- sample_mean + 1.96 * se
c(lower, upper)

# inference:
# ANOVA test: https://statistics.laerd.com/statistical-guides/one-way-anova-statistical-guide.php, https://explorable.com/anova 

# multivariate regression, looking for the best model:
# start with a full model: 
m_full <- lm(score ~ rank + ethnicity + gender + language + age + cls_perc_eval + cls_students + cls_level + cls_profs + cls_credits + bty_avg, data = evals)
# check p values for each coefficient; drop one with highest p-value
# drop the variable that, when dropped, leads to the best improvement in R2
```




## ... in Python

numpy has an arrays datatype, essential for analytic operations. arrays are like lists, but you can perform elementwise calculations on them and, unlike lists, 
they are cannot contain multiple types within one array; they are type coercive. 

```Python
# create an array: 
array_name = np.array(list_name)
np.array([list1],[list2])
np.column_stack((list1, list2))

# subsetting arrays is the same as for lists, plus:
# conditional selection: 
bmi[bmi>23]; 
# access specific cell: 
array_name[row,col]

# check dimensions: 
a_name.shape, a_name.size
# calculate average: 
np.mean(list_name)
# calculate median: 
np.median(list_name)
# calculate st dev: 
np.std(list_name)
# check correlation: 
np.corrcoef(list1,list2)
# add: 
np.sum(list_name)
# simulate random data: 
np.round(np.random.normal(mean, stdev, #obs), 2)

import random
random.choice(strname)
random.randint(a_inclusive,b_inclusive)
```

## Sources

### References

- [1,](http://www.dummies.com/how-to/content/statistics-for-dummies-cheat-sheet.html) [2,](http://web.mit.edu/~csvoss/Public/usabo/stats_handout.pdf) [3](https://drive.google.com/open?id=0B6XYyy1UbJ3XR2w5Snc2ck1BVFE)
- [CAUSEWeb statistics database](https://www.causeweb.org/cause/resources)
- [Andrew Gelman's round up of his stats writing](http://andrewgelman.com/2009/05/24/handy_statistic/)

### Read

- _The Seven Pillars of Statistical Wisdom_
- _Cartoon Guide to Statistics_ 
- _The Drunkard’s Walk_
- [Khan Academy -&nbsp;Probability &amp; Statistics](https://www.khanacademy.org/mission/probability)
- [What is a P-Value Anyway? 34 Stories](https://drive.google.com/open?id=1lNnLujwLNRt_dDBdl0S31bDPtw0-da9PXiImPjBusdE)
- [The normal distribution: A derivation from basic principles](http://courses.ncssm.edu/math/Talks/PDFS/normal.pdf)
- [Intuition behind normal distribution](http://math.stackexchange.com/questions/940189/intuition-behind-normal-distribution-forumula)
- [Intuition behind standard deviation](http://stats.stackexchange.com/questions/85387/intuition-behind-standard-deviation)
- [Mean absolute deviation versus standard deviation](http://stats.stackexchange.com/questions/81986/mean-absolute-deviation-vs-standard-deviation)
- [The advantages of the mean deviation](http://www.leeds.ac.uk/educol/documents/00003759.htm)
- [Difficult concepts in statistics](https://learnandteachstatistics.wordpress.com/2013/06/24/difficult-concepts/)

### Unread

- [A concrete introduction to probability using Python](http://nbviewer.jupyter.org/url/norvig.com/ipython/Probability.ipynb)
- [Statistical Rethinking: A Bayesian Course in R](http://xcelab.net/rm/statistical-rethinking/)
- [EdX - Intro. Probability](https://www.edx.org/course/mitx/mitx-6-041x-introduction-probability-1296#.U3yb762SzIo)
- [Probability theory: The logic of science](http://bayes.wustl.edu/etj/prob/book.pdf)
- [The Little Handbook of Statistical Practice](http://www.jerrydallal.com/LHSP/LHSP.HTM)
- [OpenIntro Statistics](https://www.openintro.org/stat/index.php)
- [Notation in probability in statistics](https://en.wikipedia.org/wiki/Notation_in_probability_and_statistics)
- Visualizations & interactives: [1, ](http://highered.mheducation.com/sites/0070000237/student_view0/visual_statistics.html) [2,](http://statpages.info/) [3,](http://www.math.uah.edu/stat/apps/index.html) [4](http://onlinestatbook.com/stat_sim/index.html)
- [British Medical Journal: Statistics notes](http://www.jerrydallal.com/LHSP/bmj.htm)
- [10 Modern Statistical Concepts Discovered by Data Scientists](http://www.datasciencecentral.com/profiles/blogs/10-modern-statistical-concepts-discovered-by-data-scientists)


# Linear Algebra

- [<i>Linear algebra</i>](http://joshua.smcvt.edu/linearalgebra/)
- [A first course in linear algebra](http://linear.ups.edu/)
- [PCA 4 Dummies: Eigenvectors/values &amp; dimension reduction](https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)

# Relationships & Trends

- [Contingency table calculator](http://www.physics.csbsju.edu/stats/contingency_NROW_NCOLUMN_form.html)
- [Understanding Multivariate Research](https://drive.google.com/open?id=1-3rUQMEKFaPoNtDHLfZGIqZkgpqvr9b_2Tx7HQQzrTg)
- [Intuition on the definition of covariance](http://stats.stackexchange.com/questions/99094/intuition-on-the-definition-of-the-covariance)
- [What is covariance in plain language](http://stats.stackexchange.com/questions/29713/what-is-covariance-in-plain-language)
- [How would you explain covariance](http://stats.stackexchange.com/questions/18058/how-would-you-explain-covariance-to-someone-who-understands-only-the-mean)
- [Correlation &amp; R<sup style="background-color:transparent">2</sup>](http://www.win-vector.com/blog/2011/11/correlation-and-r-squared/)
- [r and R<sup>2</sup>](http://prometheuswiki.publish.csiro.au/tiki-index.php?page=r+and+R2)
- [Is R<sup>2</sup> useful or dangerous?](http://stats.stackexchange.com/questions/13314/is-r2-useful-or-dangerous/)
- [Simple linear regression output interpretation](http://stats.stackexchange.com/questions/13266/simple-linear-regression-output-interpretation/13269#13269)
- [Khan Academy: Regression](https://www.khanacademy.org/math/probability/regression)
- [Statistical Model Specification and Validation](http://faculty.chicagobooth.edu/midwest.econometrics/papers/megspanos.pdf)
- [Specification Errors](http://ocw.uc3m.es/economia/econometrics/lecture-notes-1/Topic5_logo.pdf)
- [Notes on model specification](http://www.nyu.edu/classes/nagler/quant2/notes/model_specification_oh.pdf)
- [Overadjustment bias and unnecessary adjustment](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2744485/)
- [The Chicago Guide to Writing About Multivariate Analysis](https://drive.google.com/open?id=1rbtRgb84K1WhncEmz4rrgU11YHKtCYPpiaGofKrEWbQ)
- [Introductory Econometrics](http://www3.wabash.edu/econometrics/EconometricsBook/index.htm)
- [7 types of regression you should know](https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/)
- [10 types of regression: which to use?](http://www.datasciencecentral.com/profiles/blogs/10-types-of-regressions-which-one-to-use)
- [Going Deeper into Regression Analysis with Assumptions](https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/)
- [Plots &amp; Solutions](https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/)
- [How to detect spurious correlations, and how to find the real ones](http://www.datasciencecentral.com/profiles/blogs/tutorial-how-to-detect-spurious-correlations-and-how-to-find-the-)
- [The best kept secret about linear and logistic regression](http://www.datasciencecentral.com/profiles/blogs/the-best-kept-secret-about-linear-and-logistic-regression)
- [Jackknife logistic and linear regression for clustering and predictions](http://www.datasciencecentral.com/profiles/blogs/jackknife-logistic-and-linear-regression)
- [STAT 501: Regression Methods @ Penn State](https://onlinecourses.science.psu.edu/stat501/node/2)
- Regression tutorias:[1,](http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-tutorial-and-examples) [2,](http://denninginstitute.com/modules/dau/stat/regression/linregsn/linregsn_frm.html) [3,](http://illuminations.nctm.org/Search.aspx?view=search&amp;type=ac&amp;kw=regression) [4a,](https://www.causeweb.org/cause/archive/repository/StarLibrary/activities/miller2001/) [4b,](https://www.causeweb.org/cause/archive/repository/StarLibrary/activities/miller2001/Reg_Residuals.htm) [5](http://statpages.info/#Regression)
- [Rebuild OLS estimators in R](https://economictheoryblog.com/2016/02/20/rebuild-ols-estimator-manually-in-r/)
- [Fitting and interpreting linear models in R](http://blog.yhat.com/posts/r-lm-summary.html)
- [Understanding lm() output](http://www.montana.edu/screel/Webpages/conservation%20biology/Interpreting%20Regression%20Coefficients.html#/)
- [Residuals plot](http://www.r-tutor.com/elementary-statistics/simple-linear-regression/residual-plot)
- [Guide to interpreting R regression output](https://rstudio-pubs-static.s3.amazonaws.com/119859_a290e183ff2f46b2858db66c3bc9ed3a.html)
- [Don’t use correlation to track prediction performance](http://www.win-vector.com/blog/2013/02/dont-use-correlation-to-track-prediction-performance/)
- [Statistical Soup: ANOVA, ANCOVA, MANOVA, &amp; MANCOVA](http://www.statsmakemecry.com/smmctheblog/stats-soup-anova-ancova-manova-mancova)
- [Understanding beta binomial regression](http://varianceexplained.org/r/beta_binomial_baseball/)
- [Econometric analysis of panel data, class notes](http://people.stern.nyu.edu/wgreene/Econometrics/PanelDataNotes.htm)
- [Applied econometrics syllabus](http://courses.umass.edu/econ753/)


# Hypothesis & A/B Testing

## References

- Choosing the right test: [1,](http://www.graphpad.com/support/faqid/1790/) [2](http://www.ats.ucla.edu/stat/mult_pkg/whatstat/default.htm)

## Read

- [Khan Academy - Inferential Statistics](https://www.khanacademy.org/math/probability/statistics-inferential)
- [There is only one test](http://allendowney.blogspot.com/2011/05/there-is-only-one-test.html)
- [There is still only one test](http://allendowney.stfi.re/2016/06/there-is-still-only-one-test.html?sf=gezyvye)
- [Understanding statistical inference](https://learnandteachstatistics.wordpress.com/2015/11/09/understanding-statistical-inference/)
- [Statistical inference is only mostly wrong](http://allendowney.blogspot.com/2015/03/statistical-inference-is-only-mostly.html)
- [Statistical significance abuse](https://www.painscience.com/articles/statistical-significance.php)
- [Ignoring the ‘difference in differences’](https://www.theguardian.com/commentisfree/2011/sep/09/bad-science-research-error)
- [Hypothesis testing is only mostly useless](http://allendowney.blogspot.com/2015/05/hypothesis-testing-is-only-mostly.html)
- [Twelve p value misconceptions](http://www.perfendo.org/docs/BayesProbability/twelvePvaluemisconceptions.pdf)
- [p-hacking](http://blogs.discovermagazine.com/neuroskeptic/2015/05/18/p-hacking-a-talk-and-further-thoughts/#.V18pwRMrLC2)
- [The garden of forking paths](http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf)
- [Life after p-hacking](http://www.acrwebsite.org/volumes/v41/acr_v41_15833.pdf)
- [The correct interpretation of the confidence interval](https://www.reddit.com/r/statistics/comments/2xg0cs/the_correct_interpretation_of_the_confidence/)
- [Intuition for Chi-squared test](https://www.quora.com/What-is-the-most-intuitive-explanation-for-the-chi-square-test)
- [Why pseudoscientists like chi-square](http://bblais.blogspot.com/2010/09/why-pseudoscientists-like-chi-square.html)
- [Chi-square versus psi](http://bblais.blogspot.com/2010/08/orthodox-statistics-conducive-to-pseudo.html)
- [What is Chi-squared not for?](http://www.ling.upenn.edu/~clight/chisquared.htm)
- [How to understand degrees of freedom](http://stats.stackexchange.com/questions/16921/how-to-understand-degrees-of-freedom/17148#17148)
- DOF: [1, ](http://www.statsdirect.com/help/basics/degrees_of_freedom.htm) [2](http://www.jerrydallal.com/LHSP/dof.htm)
- [Deduction, induction, and abduction](http://web.bryant.edu/~bblais/deduction-induction-and-abduction-oh-my.html)
- [Bayes’ theorem with Legos](https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego)
- [Believe your friends’ allergies](https://www.countbayesie.com/blog/2016/1/22/why-you-should-believe-your-friends-claims-about-food-allergies)
- [Hans Solo and Bayesian priors](https://www.countbayesie.com/blog/2015/2/18/hans-solo-and-bayesian-priors)
- [Bayesian reasoning in the Twilight Zone](https://www.countbayesie.com/blog/2016/3/16/bayesian-reasoning-in-the-twilight-zone)
- [Bayes’ factor](https://www.countbayesie.com/blog/2015/2/27/building-a-bayesian-voight-kampff-test)

## Unread

- [Understanding the beta distribution](http://varianceexplained.org/statistics/beta_distribution_and_baseball/)
- [Parameter estimation](https://www.countbayesie.com/blog/2015/4/4/parameter-estimation-the-pdf-cdf-and-quantile-function)
- [Bayesian priors for parameter estimation](https://www.countbayesie.com/blog/2015/4/4/parameter-estimation-adding-bayesian-priors)
- [Understanding empirical Bayes estimation](http://varianceexplained.org/r/empirical_bayes_baseball/)
- [Confidence intervals vs. Bayesian intervals](http://bayes.wustl.edu/etj/articles/confidence.pdf)
- [Understanding credible intervals](http://varianceexplained.org/r/credible_intervals_baseball/)
- [Hypothesis testing that makes sense](https://www.countbayesie.com/blog/2015/4/25/bayesian-ab-testing)
- [How Bayes’ theorem, probability, logic and data intersect](https://www.springboard.com/blog/probability-bayes-theorem-data-science/)
- [Learning to love Bayesian statistics](http://www.oreilly.com/pub/e/3707)
- [The death of statistical tests of hypotheses](http://www.datasciencecentral.com/profiles/blogs/the-death-of-the-statistical-test-of-hypothesis)
- [Biased vs unbiased: debunking statistical myths](http://www.datasciencecentral.com/profiles/blogs/biased-vs-unbiased-debunking-statistical-myths)
- [Lognormal distributions across the sciences: Key and clues](http://stat.ethz.ch/~stahel/lognormal/bioscience.pdf)
- [<font size="2">Wired - AB testing</font>](http://www.wired.com/2012/04/ff_abtesting/)
- [Udacity - AB testing](https://www.udacity.com/courses/ab-testing--ud257)
- [Understanding Bayesian A/B testing](http://varianceexplained.org/r/bayesian_ab_baseball/)
- [Falsely reassuring: analyses of ALL p values](http://datacolada.org/41)

# Business Strategy

## What generic strategies can companies pursue?

## What are frameworks for a strategic analysis?

PESTEL, PORTER 5, SWOT.

## What is market research, and how is it done?


# Trends

## What technology trends matter?

Communication, automation, Big Data.

## What labor trends matter?

Overwork, precariousness, disemployment, loss of protection or violates of labor laws.

## What political trends matter?

Inequality and polarization.




# Sources

## References

- [Mike Bradley & Tiff Zhang's startup generator](http://tiffzhang.com/startup/)
- [Portal to Gartner's research methodologies](http://www.gartner.com/technology/research/methodologies/methodology.jsp)
- [McKinsey on "digital disruption"](http://www.mckinsey.com/global-themes/digital-disruption)
- [Forrester's industry reports](https://www.forrester.com/search?N=10001&amp;range=504005&amp;sort=3&amp;searchRefinement=reports)
- [Wilson Center global issues research](https://www.wilsoncenter.org/research)
- [CBBP's "policy futures" research](http://www.cbpp.org/about/special-projects/policy-futures)
- [SLU library guide to market research](http://libguides.slu.edu/c.php?g=185604&amp;p=1226028)

## Read


## Unread

- [Paul Graham's "Do things that don't scale"](http://paulgraham.com/ds.html)
- [Technology will replace many doctors, lawyers, and other professionals](https://hbr.org/2016/10/robots-will-replace-doctors-lawyers-and-other-professionals)
- [By a thousand cuts](https://hackernoon.com/by-a-thousand-cuts-a772ff1130f8#.5mxlejt5r)
- [Retailers discover labor isn't just a cost](https://www.bloomberg.com/view/articles/2015-11-16/retailers-discover-that-labor-isn-t-just-a-cost)
- [Nurse staffing, burnout linked to hospital infections](https://www.eurekalert.org/pub_releases/2012-07/afpi-nsb072612.php)
- ["Overworked America" in charts](http://www.motherjones.com/politics/2011/06/speedup-americans-working-harder-charts)
- [Broken laws, unprotected workers: A national report](http://www.unprotectedworkers.org/)
- [How the safety net is failing Americans and how to fix it](http://www.ips-dc.org/battered-by-the-storm/)
- [This tech bubble is different](http://webcache.googleusercontent.com/search?q=cache:3crTOeA1K3cJ:www.bloomberg.com/bw/stories/2011-04-13/this-tech-bubble-is-different&amp;num=1&amp;hl=en&amp;gl=us&amp;strip=1&amp;vwsrc=0)
- [The social media bubble is quietly deflating](https://www.bloomberg.com/news/articles/2013-07-16/the-social-media-bubble-is-quietly-deflating)
- [A Financial Start-Up That Provides the Illusion of a Salary](https://psmag.com/a-financial-start-up-that-provides-the-illusion-of-a-salary-f681edf174fa#.5jzqdol5u)
- [The disruptive potential of online learning](http://voxeu.org/article/disruptive-potential-online-learning)
- [The Gates effect](http://www.chronicle.com/article/The-Gates-Effect/140323/)
- [Big Firms Eyeing Profits From U.S. K-12 Market](http://www.huffingtonpost.com/2012/08/02/private-firms-eyeing-prof_n_1732856.html?view=print&amp;comm_ref=false)
- [A battle between education and business goals](http://www.nytimes.com/roomfordebate/2012/09/11/must-teachers-and-school-officials-be-foes/a-battle-between-education-and-business-goals)
- [Letter to Secretary of Education Arne Duncan Concerning Evaluation of Teachers and Principals](http://nepc.colorado.edu/publication/letter-to-Arne-Duncan)
- Predictions about Big Data: [1,](http://www.datasciencecentral.com/profiles/blogs/big-data-17-predictions-everyone-should-read-1) [2](http://www.datasciencecentral.com/profiles/blogs/23-predictions-about-the-future-of-big-data)
- [Big data: are we making a big mistake?](https://www.ft.com/content/21a6e7d8-b479-11e3-a09a-00144feabdc0#axzz2zQWDYEV3)
- [With income inequality comes violence](http://www.usnews.com/opinion/blogs/letters-to-the-editor/2012/07/18/with-income-inequality-comes-violence)
- [How the bursting of the consumer bubble continues to hold the economy back](http://www.nytimes.com/2011/07/17/sunday-review/17economic.html?_r=2)
- [America's incomplete thoughts about inequality and meritocracy](http://www.huffingtonpost.com/howard-steven-friedman/americas-incomplete-thoug_b_1696282.html)
- [Do artifacts have politics?](http://innovate.ucsb.edu/wp-content/uploads/2010/02/Winner-Do-Artifacts-Have-Politics-1980.pdf)
- [More than one-third of schoolchildren are homeless in shadow of Silicon Valley](https://www.theguardian.com/society/2016/dec/28/silicon-valley-homeless-east-palo-alto-california-schools?CMP=share_btn_fb)
- [Using Google Trends for marketing research](https://charlestonpr.com/video-tutorial-using-google-trends-for-marketing-research/)
- [Using NAICS codes for market research](http://library.royalroads.ca/infoquest-tutorials/market-research/using-naics-code-market-research)
- [Marketing plan tutorial](http://guides.lib.uw.edu/friendly.php?s=bothell/MarketPlanTutorial)
- [Understand your market<br>](https://www.sba.gov/starting-business/how-start-business/understand-your-market)
# Text analytics in Python


```Python
# reserve order of elements: 
list.reverse(), my_string[::-1] 
# selectively replace: 
str_name.replace(‘this’,’with this’) 
# find index of known element: 
list.index(‘str name’) 
# times element occurs: 
list.count(‘em_name’) makes tuple with (index,value): enumerate(my_list) 
```

## Manage punctuation, case, & whitespace

```Python
# remove punctuation
import string
line.translate(None, string.punctuation)

# modify case
my_string.lower()
my_string.upper()
my_string.capitalize()
my_string.title()

# remove whitespace by default, or remove specified characters
my_string.strip('chars')
my_string.lstrip()
my_string.rstrip()
```

## Search & regex

```Python
# search for substrings within string or subset of string (i inclusive to j exclusive)
str_index = my_string.find(x,i,j)
str_index = my_string.index(x,i,j)  # raises ValueError if not found
str.endswith(x,i,j)
str.startswith(x,i,j)
my_string.count(x,i,j)
```

- https://docs.python.org/3/library/re.html
- https://docs.python.org/3/howto/regex.html
- http://nbviewer.jupyter.org/github/ptwobrussell/Mining-the-Social-Web-2nd-Edition/tree/master/ipynb/

```Python
# match the beginning of a string:
re.match(pattern, text, flags)
re.match(r’Jac’, data) # the r denotes a raw string

# search anywhere in a string:
# first match only: 
re.search(pattern, text, flags)
# all nonoverlapping: 
re.findall(pattern, text, flags)

# phone number, note escaped parentheses:
re.search(r’\(\d\d\d\) \d\d\d-\d\d\d\d’, data)
# make parentheses, space, hyphen optional in phone number
r’\)?\d{3})?\s?-?\d\{3}-\d{4}’
```

flags:

- re.IGNORECASE or re.I will ignore word case
- re.VERBOSE or re.X let regexp span lines & contain (ignored) whitespace or comments
- re.MULTILINE or re.M to make a pattern regard lines in your text as the beginning or end of a string
- multiple flags: re.findall(pattern, data, flag|flag|flag)

```Python
# store regex for reuse: 
my_regex = re.compile(pattern, flags)
re.search(my_regex, data)
# OR 
my_regex.search(data)

# loop to obtain iterable of match objects:
for match in my_regex.finditer(data):
    print(‘{first} {last} <{email}>’.format(**match.groupdict())) 
```
    
- \\w = any Unicode word character,  \\W = anything not a Unicode word character
- \\s = any whitespace, \\S = anything not whitespace, \t = tab
- \\d = any number 0-9, \\D = any non-number
- \\b = word boundaries, \\B = not word boundaries

counts, for when something occurs multiple times:

- {3} = occurs 3 times, {,3} = 0-3 times, {3,} = 3 or more times, {3-5} = 3-5 times
- \\w? = 0-1 word characters, \\w* = 0-infinite word characters, \\w+ = 1-infinite word characters


sets let us combine explicit characters and escape patterns into pieces that can be repeated multiple time; they also let us specify pieces that should be left out of any matches:
[aple] finds apple and pale, [a-z] finds any lowercase letter, [A-Z] finds uppercase, [a-zA-Z] finds any case, [^2] finds anything not two, [0-9] finds any number, [\w.]+ finds any # of \w, .

```Python
# groups search for multiple conditions simultaneously; note that ^ marks the beginning of the string, and $ marks the end; unnamed groups returned as tuples, named groups as dicts:
my_var = re.findall (r’’’
    ^(?P<name>[-\w ]+,\s[-\w ]+)\t   # search for lastname, firstname
    (\)?\d{3})?\s?-?\d\{3}-\d{4})? # search for phone number, optional
    (?<email>[-\w\d.+]+ @[-\w\d.]+)\t$  # search for emails
    ‘’’, data, flags)

# groups addressing
my_var.groups()
my_var.group_dict() 
my_var.group(‘group_name’)
my_var.group(1)
```



# Sources

## References

- [NLTK cheatsheet](http://billchambers.me/tutorials/2015/01/14/python-nlp-cheatsheet-nltk-scikit-learn.html)
- [Corpus of Contemporary American English](http://corpus.byu.edu/coca/)
- [Corpus based language studies](http://cw.routledge.com/textbooks/0415286239/default.asp)
- [IBM Watson demo - Infer personality from unstructured text](https://personality-insights-livedemo.mybluemix.net/)

## Read

- [Lynda - Using Regex](http://www.lynda.com/Regular-Expressions-tutorials/Using-Regular-Expressions/85870-2.html)

## Unread

- Regex: [1](http://www.regular-expressions.info/), [2](https://regexone.com/)
- [CodeSchool - Regular Expressions](https://www.codeschool.com/courses/breaking-the-ice-with-regular-expressions)
- [4 steps to structure highly unstructured big data via automated indexation](http://www.datasciencecentral.com/profiles/blogs/5-easy-steps-to-structure-highly-unstructured-big-data)
- [Coursera - Natural Language Processing](https://www.coursera.org/learn/nlp)
- [Big Data U - Text Analytics](http://bigdatauniversity.com/courses/text-analytics-essentials/)
- [Language bias &amp; black sheep](http://nlpers.blogspot.co.uk/2016/06/language-bias-and-black-sheep.html)
- [Analyzing Text with NLTK](http://www.nltk.org/book/)
- [Donovan Rebbechi's grep tutorial](http://www.panix.com/~elflord/unix/grep.html)
- [Drew's grep tutorial](http://www.uccs.edu/~ahitchco/grep/)

# Elements of UML

UML is a graphical higher-level language used for database modeling and software design; see [notes on modeling.](modeling.html)

## Classes

Analogous to relations/tables.

- Name
- Attributes
- Methods/primary key

### Superclasses 

In/complete and disjoint/overlapping.

### Subclasses

Inherit attributes from super/parent class, but have own unique attributes and/or unique associations.  



## Associations

Captures relationships between objects of two classes. Self-association is possible.

### Types of associations

#### Composition

Objects in one class ‘belong’ to objects in another class. Denoted with a solid diamond on the association. Default multiplicity `1..1`. PK not required.

#### Aggregation

Objects might ‘belong’ to, at most, one object of another class. Denoted with an empty diamond on the association. PK required.

### Classes for associations

Add attributes to an association, e.g. “Date” and “Decision” to the association “Applied”.

### Multiplicity of associations 

- `0..n` 
- `0..*`
- `1..1`
- `m..n`
- `m..*` 




# Sources

- Ambler, S. W. (2005). _The elements of UML™ 2.0 style._ Cambridge, UK: Cambridge University Press.
- [https://praveenthomasln.wordpress.com/tag/class-diagrams-in-uml/](https://praveenthomasln.wordpress.com/tag/class-diagrams-in-uml/)
- [Unified Modeling Language](https://lagunita.stanford.edu/courses/DB/UML/SelfPaced/about)


# Sources

## References

- [w3schools CSS tutorial](http://www.w3schools.com/css/default.asp)
- Codecademy - [HTML glossary](https://www.codecademy.com/articles/glossary-html) & [HTML5 features](https://www.codecademy.com/articles/html5-features)
- Codecademy - [CSS glossary](https://www.codecademy.com/articles/glossary-css) & [CSS3 features](https://www.codecademy.com/articles/css3-features)

## Read

- [Khan Academy - Web 101](https://www.khanacademy.org/computing/computer-science/internet-intro)
- Mozilla - [Webpage, website, web server, search engine](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/Pages_sites_servers_and_search_engines), [How the Web works](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/How_the_Web_works)
- [The evolution of the web](http://www.evolutionoftheweb.com/) 
- [Web application development is different and better](https://www.oreilly.com/ideas/web-application-development-is-different-and-better)
- [How browsers work](https://www.html5rocks.com/en/tutorials/internals/howbrowserswork/)
- [Codecademy - HTML & CSS](https://www.codecademy.com/learn/web)
- [Codecademy - Ruby](https://www.codecademy.com/learn/ruby)
- [Why's poignant guide to Ruby](http://poignant.guide/)

## Unread

- [Codecademy - Use DevTools](https://www.codecademy.com/articles/use-devtools)
- [CodeSchool - Discover DevTools](https://www.codeschool.com/courses/discover-devtools?from_search=dev+tools)
- [Udacity - Configuring Linux Web Servers](https://www.udacity.com/course/configuring-linux-web-servers--ud299)
# Obtaining data AKA I/O

- [Treehouse - Using Databases in Python](https://teamtreehouse.com/library/using-databases-in-python)
- [https://docs.python.org/3/tutorial/inputoutput.html](https://docs.python.org/3/tutorial/inputoutput.html)

## ... in R


## ... in Python

```Python
# to read or write binary (raw 8-bit):
with open('filename', 'wb') as f: 
f.write(os.urandom(10))

with open('filename', 'rb') as f: 
f.read()

open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)
# https://docs.python.org/3/library/functions.html


# https://docs.python.org/3/howto/urllib2.html
# download from url: 
import urllib2
ftxt = urllib2.urlopen(“target_url”).read()

# store user input: 
# Python 2:
vname = raw_input('What is your name?\n')
age = int(input("What is your age?")
long_text = sys.stdin.read().strip()
# type ctrl+d for EOF
# Python 3:
vname = input('What is your name?')
age = int(input(‘What is your age? ‘)

# open text file, read single line, remove from memory: 
fhand = open(‘filename.txt’, encoding=’utf-8’)
fhand.readline() 
fhand.close()

# load small text file into string: 
str_name = fhand.read(#bytes)

# load large text file into list: 
l_name = fhand.readlines()

# load text file into pandas dataframe:
from pandas import DataFrame, Series
import pandas as pd
frame=DataFrame(open(‘/file/path.txt’))

# fetch through API: uses urllib2, json, pandas
# https://github.com/katychuang/python-data-sci-basics/blob/master/teachers_notes/api_example.py

# open csv with context manager pattern, load into list: 
import csv
with open(filename,encoding=’utf-8’) as file_in:
my_reader = csv.reader(file_in,delimiter=’\t’)
data = list(my_reader)

# load csv into dict & read dict: 
with open(filename,encoding=’utf-8’) as file_in:
my_reader = csv.DictReader(file_in,delimiter=’\t’) 
data[‘field name’] # retrieve data by column name

# load csv into array: 
my_array = np.genfromtxt(fname,delimiter=’\t’,dtypes=None)

# open JSON: 
import json
with open(‘filename.json’) as myfile:
file = json.load(myfile) # load chooses the closest Python object

# load file into list of dicts using json ’load string’ and a list comprehension:
records = [json.loads(line) for line in open(‘/file/path.txt’,’rb’)

print(*objects, sep=' ', end='\n', file=sys.stdout, flush=False)

# output to screen: 
print(vname) or print vname

# create reusable message: 
message = ’There are {it} items’ 
print(message.format(it=nr_items)

# print columns
print(state,’\t’,count)

# specify display of decimals: 
print(‘Price: ${:03.2f}’.format(price))

# add linebreak: 
print(‘Hello \n world!’)

# clear screen: 
os.system(‘cls’ if os_name == ‘nt’ else ‘clear’)

# output to text: 
# CAUTION: opening existing file with ‘w’ will erase its contents; instead, use ‘a’
fout = open('output.txt', 'w')

# add content to new file: 
fout.write(line)
# save and close file: 
fout.close()

# output to pdf:
from matplotlib.backends.backend_pdf import PdfPages
pp = PdfPages('foo.pdf')
pp.savefig()pp.close()

# https://openpyxl.readthedocs.io/en/default/
import openpyxl
output to excel: save_spreadsheet(“filename”, my_data)

# output to csv: see also unicodecsv
numpy.savetxt("filename", my_data, delimiter=",", fmt="%s")
with open(‘filename’, ‘a’) as myfile:
fnames = [l’A’, ‘B’, ‘C’]
filewriter = csv.DictWriter(myfile, fieldnames = fnames)
filewriter = writerheader()
filewriter = writerow({‘A’: 1, ‘B’:2, ‘C’:3})

# output to JSON:
# output to string: 
json.dumps(my_list)
# output to file:
with open(‘filename.json’, ‘a’) as my_file:
json.dump(list_of_dicts, my_file)
```


### ... integrate database via Peewee ORM

```Python
pip install peewee
from peewee import *

# create & connect to database:
db = SqliteDatabase(‘dbname.db’)
psql_db = PostgresqlDatabase('my_database', user='postgres')
mysql_db = MySQLDatabase('my_database')

# special Python classes called models correspond to tables; 
# specific models inherit from the Model class:
class Student(Model):
username = CharField(maxlength=255, unique=True)
points = IntegerField(default=0)
date = DateTimeField(datetime.datetime.now) # important to omit parantheses in this datetime function call

class Meta():
database = db
if __name__ == ‘__main__’: 
db.connect()
db.create_tables([Student], safe=True)
field types: CharField, IntegerField, TextField, DateTimeFieldfield characteristics: max_length, default, unique

# enter new data: 
Student.create(username=student[unm], points= student[pts])

# update existing data:
record = Student.get(username=student[unm]) → record.points = student[pts] → record.save()
Student.update(points=student['points']). where(Student.username == student['username']).execute()

# find/filter records in table: 
Student.select().order_by(Student.points.desc())
Entry.select().where(Entry.content.contains(search_query), Entry.Date == date)

# delete single record: 
Student.delete_instance(condition)
single_row.delete_instance()

# close connection: 
db.close()

# ordered dict can be useful:
from collections import OrderedDict
menu = OrderedDict([(‘a’, ‘add_entry’), (‘v’, ‘view_entries’)])
returns doc string for function: function_name.__doc__
```

# Cleaning data

## Dirty data typologies

Searching, counting, ranking, grouping, handling whitespace

## ... in R


- [Big Data U - Using R with Databases](http://bigdatauniversity.com/courses/using-r-with-databases/)
- [http://nbviewer.jupyter.org/urls/gist.githubusercontent.com/TomAugspurger/6e052140eaa5fdb6e8c0/raw/811585624e843f3f80b9b6fe89e18119d7d2d73c/dplyr_pandas.ipynb](http://nbviewer.jupyter.org/urls/gist.githubusercontent.com/TomAugspurger/6e052140eaa5fdb6e8c0/raw/811585624e843f3f80b9b6fe89e18119d7d2d73c/dplyr_pandas.ipynb)
- [http://www.alfredo.motta.name/data-manipulation-primitives-in-r-and-python/](http://www.alfredo.motta.name/data-manipulation-primitives-in-r-and-python/)
- [http://www.rdocumentation.org/packages/utils/versions/3.3.1/topics/read.table?](http://www.rdocumentation.org/packages/utils/versions/3.3.1/topics/read.table?)
- library for web scraping: [https://github.com/hadley/rvest](https://github.com/hadley/rvest)
- library: [https://github.com/smbache/magrittr](https://github.com/smbache/magrittr) 


```R
c(4,5,6) > 5  # returns vector of Booleans
my_selection <- my_vec > 5  # create new vector based on selection
# <, > can be used with strings, e.g. "Hello" > "Goodbye"; it compares based on alphabetical order, where A is smallest
# <, > can be used with Booleans, since TRUE == 1 and FALSE == 0 
# one trick: evaluate a vector to yield a Boolean vector, and then sum the Boolean vector: gives #TRUE

print("Your text here")
print(paste("", my_var, "")

seq(a,b,length.out=n)

seq(from = 1, to = 10, by = 1)
# Generate sequences, by specifying the from, to andby arguments.

rep()
# Replicate elements of vectors and lists.

sort()
# Sort a vector in ascending order. Works on numerics, but also on character strings and logicals.

rev()
# Reverse the elements in a data structures for which reversal is defined.

str()
# Display the structure of any R object.

append()
# Merge vectors or lists.

is.*()
# Check for the class of an R object.

as.*()
# Convert an R object from one class to another.

unlist()
# Flatten (possibly embedded) lists to produce a vector.

# load & clean data:
read.table(“url”)

# load custom function: 
load(url(“”))
na.omit(my_data)

# investigate structure of data frame object:
dim(my_df)
names(my_df)
head(my_df)
tail(my_df)
typeof(my_df$my_var)
levels(categorical_var)

sort(my_df)
order(my_df$my_var, decreasing=TRUE)
my_df[order(my_df),]

identical(thing1, thing2)

diff(my_vec)  
# vector of differences between elements in a numerics vector
unique(my_vec)
order() 
# is a function that gives you the ranked position of each element when it is 
# applied on a variable, such as a vector for example
# we can use the output of order(a) to reshuffle a: a[order(a)]

# retrieve data from data frame:
my_df$var_name
# row-and-column style: 
my_df[row#,col#], my_df[row#:range, col#:range], my_df[row#,]
# names style: 
my_df$var_name, mf_df$var_name[row#:range]

# conditional subsetting: 
subset[my_df, condition]
# ==, >, >=, !=, |, &
```


## ... in Python

- [https://docs.python.org/3.5/library/markup.html](https://docs.python.org/3.5/library/markup.html)
- [http://nbviewer.jupyter.org/urls/gist.githubusercontent.com/TomAugspurger/6e052140eaa5fdb6e8c0/raw/811585624e843f3f80b9b6fe89e18119d7d2d73c/dplyr_pandas.ipynb](http://nbviewer.jupyter.org/urls/gist.githubusercontent.com/TomAugspurger/6e052140eaa5fdb6e8c0/raw/811585624e843f3f80b9b6fe89e18119d7d2d73c/dplyr_pandas.ipynb)
- [http://www.alfredo.motta.name/data-manipulation-primitives-in-r-and-python/](http://www.alfredo.motta.name/data-manipulation-primitives-in-r-and-python/)
- [https://docs.python.org/3/howto/sorting.html](https://docs.python.org/3/howto/sorting.html)


```Python
# to check for an empty list, avoid (if len(my_list) == 0)
# prefer this approach:
def check_list_for_members(my_list):
    if not my_list:
        print("Sorry, that's an empty list")
    else: 
        pass

reversed(seq)
sorted(iterable[, key][, reverse])

zip(*iterables)
# Make an iterator that aggregates elements from each of the iterables.

# search file contents: 
for line in fhand:
line = line.rstrip()   # remove whitespace
if line.startswith('From:'): print line
# search more efficiently:
for line in fhand:
line = line.rstrip()
if not line.startswith('From:') : continue 
print line

# find specific string: 
for line in fhand:
line = line.rstrip()
if line.find('desired string') == -1 :  continue
print line

# troubleshooting  whitespace errors: print repr(strname)
inspect pandas dataframe: frame.info()

# count w/ standard library
from collections import defaultdict
def get_counts(data):
counts=defaultdict(int) # initialize to 0
for x in data:
counts[x]+=1
return counts

# rank w/ standard library
from collections import Counter
counts = Counter(data)
counts.most_common(10)

# count w/ pandas
item_counts = frame[‘item’].value_counts()

# clean w/ pandas
clean_field = frame[‘field’].fillna(‘Missing’)
clean_field[clean_field==’’] = ‘Unknown’
clean_frame = framep[frame.a.notnull()]

# plot
item_counts.plot(kind=’barh’,rot=0)

# numpy search for text in frame
result = np.where(clean_frame[field].str.contains(‘text’),’Group 1’, ‘ Group 2’)
# Binary labeling: Group 1 where contains text, Group 2 otherwise

# group & rank  results
results_by_group = clean_frame.groupby([field,result])
group_counts = results_by_group.size().unstack().fillna(0)
indexer=group_counts.sum(1).argsort()
count_subset=group_counts.take(indexer)[-10:]
```


# Sources

## Cited

## References

## Read

## Unread

- [XML data](https://lagunita.stanford.edu/courses/DB/XML/SelfPaced/about)
- [XPath &amp; XQuery](https://lagunita.stanford.edu/courses/DB/XPath/SelfPaced/about)
- [XSLT](https://lagunita.stanford.edu/courses/DB/XSLT/SelfPaced/about)