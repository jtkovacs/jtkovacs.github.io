<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
  <meta content="text/css" http-equiv="Content-Style-Type"/>
  <meta content="pandoc" name="generator"/>
  <title>
   jtck.github.io | big data
  </title>
  <link href="../assets/styles/main.css" rel="stylesheet" type="text/css"/>
  <link href="../assets/styles/refs.css" rel="stylesheet" type="text/css"/>
 </head>
 <body>
  <p class="path">
   <a href="../pkb.html">
    pkb contents
   </a>
   &gt; big data | just under 2456 words | updated 12/30/2017
  </p>
  <div class="TOC">
   <ul>
    <li>
     1.
     <a href="#what-is-big-data">
      What is Big Data?
     </a>
     <ul>
      <li>
       1.1.
       <a href="#sources-of-big-data">
        Sources of Big Data
       </a>
      </li>
      <li>
       1.2.
       <a href="#business-applications-of-big-data">
        Business applications of Big Data
       </a>
       <ul>
        <li>
         1.2.1.
         <a href="#business-applications-of-stream-analytics">
          Business applications of stream analytics
         </a>
        </li>
       </ul>
      </li>
      <li>
       1.3.
       <a href="#implementing-big-data-initiatives">
        Implementing Big Data initiatives
       </a>
       <ul>
        <li>
         1.3.1.
         <a href="#big-data-maturity-model">
          Big Data maturity model
         </a>
        </li>
        <li>
         1.3.2.
         <a href="#when-big-data-versus-data-warehousing">
          When Big Data versus data warehousing?
         </a>
        </li>
        <li>
         1.3.3.
         <a href="#success-factors-for-big-data-initiatives">
          Success factors for Big Data initiatives
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li>
     2.
     <a href="#big-data-technologies">
      Big Data technologies
     </a>
     <ul>
      <li>
       2.1.
       <a href="#high-performance-computing">
        High-performance computing
       </a>
      </li>
      <li>
       2.2.
       <a href="#generic-big-data-architectures">
        Generic Big Data architectures
       </a>
      </li>
      <li>
       2.3.
       <a href="#big-data-storage">
        Big Data storage
       </a>
       <ul>
        <li>
         2.3.1.
         <a href="#hadoop">
          Hadoop
         </a>
         <ul>
          <li>
           2.3.1.1.
           <a href="#hadoop-components">
            Hadoop components
           </a>
          </li>
          <li>
           2.3.1.2.
           <a href="#hadoop-suprojects">
            Hadoop suprojects
           </a>
          </li>
         </ul>
        </li>
        <li>
         2.3.2.
         <a href="#what-are-nosql-databases">
          What are NoSQL databases?
         </a>
         <ul>
          <li>
           2.3.2.1.
           <a href="#nosql-databases-versus-other-data-store-options">
            NoSQL databases versus other data store options
           </a>
          </li>
          <li>
           2.3.2.2.
           <a href="#nosql-database-software">
            NoSQL database software
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       2.4.
       <a href="#big-data-analytics">
        Big Data analytics
       </a>
       <ul>
        <li>
         2.4.1.
         <a href="#mapreduce">
          MapReduce
         </a>
        </li>
        <li>
         2.4.2.
         <a href="#data-stream-mining">
          Data stream mining
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li>
     3.
     <a href="#sources">
      Sources
     </a>
     <ul>
      <li>
       3.1.
       <a href="#cited">
        Cited
       </a>
      </li>
      <li>
       3.2.
       <a href="#references">
        References
       </a>
      </li>
      <li>
       3.3.
       <a href="#read">
        Read
       </a>
      </li>
      <li>
       3.4.
       <a href="#unread">
        Unread
       </a>
      </li>
     </ul>
    </li>
   </ul>
  </div>
  <h1 id="what-is-big-data">
   <a>
    1. What is Big Data?
   </a>
  </h1>
  <p>
   Per Sharda et al. (2014, pp. 280-282):
  </p>
  <p>
   <strong>
    Volume
   </strong>
  </p>
  <table>
   <thead>
    <tr class="header">
     <th align="left">
      Year
     </th>
     <th align="left">
      Estimated World Data
     </th>
    </tr>
   </thead>
   <tbody>
    <tr class="odd">
     <td align="left">
      2009
     </td>
     <td align="left">
      0.8 ZB
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      2010
     </td>
     <td align="left">
      &gt;1 ZB
     </td>
    </tr>
    <tr class="odd">
     <td align="left">
      2011
     </td>
     <td align="left">
      1.8 ZB
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      2020
     </td>
     <td align="left">
      35 ZB
     </td>
    </tr>
   </tbody>
  </table>
  <ul>
   <li>
    Kilobyte (kB) = 10
    <sup>
     3
    </sup>
   </li>
   <li>
    Megabyte (MB) = 10
    <sup>
     6
    </sup>
   </li>
   <li>
    Gigabyte (GB) = 10
    <sup>
     9
    </sup>
   </li>
   <li>
    Terabyte (TB) = 10
    <sup>
     12
    </sup>
   </li>
   <li>
    Petabyte (PB) = 10
    <sup>
     15
    </sup>
   </li>
   <li>
    Exabyte (EB) = 10
    <sup>
     18
    </sup>
   </li>
   <li>
    Zettabyte (ZB) = 10
    <sup>
     21
    </sup>
   </li>
   <li>
    Yottabyte (YB) = 10
    <sup>
     24
    </sup>
   </li>
  </ul>
  <p>
   <strong>
    Variety
   </strong>
   (in format; about 80-85% unstructured)
  </p>
  <ul>
   <li>
    RDBMS, hierachical data stores, OLAP
   </li>
   <li>
    text documents, email
   </li>
   <li>
    XML, HTML
   </li>
   <li>
    meter-collected, sensor-captured data
   </li>
   <li>
    video, audio
   </li>
   <li>
    stock ticker
   </li>
  </ul>
  <p>
   <strong>
    Velocity
   </strong>
  </p>
  <ul>
   <li>
    increased speed of data
    <strong>
     production
    </strong>
    ("RFID tags, automated sensors, GPS devices, and smart meters are driving an increasing need to deal with torrents of data in near-real time")
   </li>
   <li>
    <strong>
     demand
    </strong>
    for quicker data processing, i.e.
    <a href="#data-stream-mining">
     data stream mining
    </a>
   </li>
  </ul>
  <p>
   <strong>
    Veracity
   </strong>
  </p>
  <ul>
   <li>
    coined by IBM
   </li>
   <li>
    "conformity with facts: accuracy, quality, truthfulness, or trustiworthiness"
   </li>
  </ul>
  <p>
   <strong>
    Variability
   </strong>
   ("Daily, seasonal, and event-driven peak data loads")
  </p>
  <p>
   <strong>
    Value
   </strong>
   (one hopes)
  </p>
  <h2 id="sources-of-big-data">
   <a>
    1.1. Sources of Big Data
   </a>
  </h2>
  <p>
   "Web logs, RFID, GPS systems, sensor networks, social networks, Internet-based text documents, Internet search indexes, detail call records, astronomy, atmospheric science, biology, genomics, nuclear physics, biochemical experiments, medical records, scientific research, military surveillance, photography archives, video archives, and large-scalre e-commerce practices" (Sharda et al., 2014, pp. 278-280).
  </p>
  <h2 id="business-applications-of-big-data">
   <a>
    1.2. Business applications of Big Data
   </a>
  </h2>
  <p>
   Sharda et al. (2014, pp. 287):
  </p>
  <ul>
   <li>
    "Process efficiency and cost reduction
   </li>
   <li>
    Brand management
   </li>
   <li>
    Revenue maximization, cross-selling, and up-selling
   </li>
   <li>
    Enhanced customer experience
   </li>
   <li>
    Churn identification, customer recruiting
   </li>
   <li>
    Improved customer service
   </li>
   <li>
    Identifying new products and market opportunities
   </li>
   <li>
    Risk management
   </li>
   <li>
    Regulatory compliance
   </li>
   <li>
    Enhanced security capabilities"
   </li>
  </ul>
  <p>
   Per Zhu et al. (2014, pp. 16-17), there are four categories of business goals that companies may fruitfully pursue with Big Data:
  </p>
  <p>
   <em>
    REVENUE
   </em>
  </p>
  <ul>
   <li>
    <strong>
     Monetize big data:
    </strong>
    Design and execute big data analytics use cases that increase revenue, lower costs, or reduce risk.
   </li>
   <li>
    <strong>
     Manage big data at a low cost:
    </strong>
    Demonstrate cost savings of big data analytics styles for both MapReduce clusters and real-time analytics.
   </li>
   <li>
    <strong>
     Improve efficiency in business operations:
    </strong>
    Develop insight about the value of specific business processes, such as enterprise resource planning (ERP), supply chain management (SCM), and customer relationship management (CRM).
   </li>
  </ul>
  <p>
   <em>
    CUSTOMER SERVICES
   </em>
  </p>
  <ul>
   <li>
    <strong>
     Improve customer understanding (360-degree view of the customer):
    </strong>
    Mine all sources of client experience and interaction from additional unstructured and semi-structured data types using real-time and batch (Hadoop) analytics.
   </li>
   <li>
    <strong>
     Obtain behavioral insight into client transactions:
    </strong>
    | What led to a certain business transaction? Why did the client choose us? What else can we deduce about a clientâ€™s buying behavior?
   </li>
   <li>
    <strong>
     Attract and retain customers:
    </strong>
    Mine and apply insight toward marketing and sales effectiveness with clients, customers, and customer support personnel.
   </li>
   <li>
    <strong>
     Fraud detection and claims processing:
    </strong>
    Derive and exploit additional insight from data types not previously analyzed for anti-fraud and claims processing.
   </li>
  </ul>
  <p>
   <em>
    BUSINESS DEVELOPMENT
   </em>
  </p>
  <ul>
   <li>
    <strong>
     Introduce new products or services:
    </strong>
    Thanks to your new insight about target market preferences, new products and services will have higher adoption rates by the target clientele.
   </li>
   <li>
    <strong>
     Outsource non-core functions:
    </strong>
    Decide what to outsource without affecting the customer experience.
   </li>
   <li>
    <strong>
     Pursue mergers, acquisitions, and divestitures:
    </strong>
    Gather and consider marketplace insights about the potential impact of mergers, acquisitions, and divestitures.
   </li>
   <li>
    <strong>
     Gain new competitive insights:
    </strong>
    Mine all sources of information, even non-traditional sources of information, to learn about the brand perception of the company by its customers, its reputation, and its industry ranking. | Define metrics for improvement that are achievable if based on better insight.
   </li>
  </ul>
  <p>
   <em>
    BUSINESS AGILITY &amp; GOVERNANCE
   </em>
  </p>
  <ul>
   <li>
    <strong>
     Increase business agility:
    </strong>
    Mine real-time events for trends and apply the insight to transactions and interactions with customers.
   </li>
   <li>
    <strong>
     Plan with greater confidence:
    </strong>
    Build better scenario-based analysis models.
   </li>
   <li>
    <strong>
     Make better decisions faster:
    </strong>
    Harvest better insights from both batch (Hadoop) and real-time events and rapidly make them available to decision makers.
   </li>
   <li>
    <strong>
     Ensure regulatory compliance:
    </strong>
    Improve your understanding of the current regulatory climate and expectations of auditors.
   </li>
   <li>
    <strong>
     Lower risk:
    </strong>
    Improve the cost-benefit analysis of various risks (regulatory, market, credit, counter-party operational, and so on).
   </li>
  </ul>
  <h3 id="business-applications-of-stream-analytics">
   <a>
    1.2.1. Business applications of stream analytics
   </a>
  </h3>
  <p>
   Per Sharda et al. (2014, pp. 317-321):
  </p>
  <p>
   <em>
    e-COMMERCE
   </em>
   ("analysis of [clickstream] data can turn browsers into buyers and buyers into shopaholics")
  </p>
  <ul>
   <li>
    recommendations
   </li>
   <li>
    sales
   </li>
   <li>
    bundle offers
   </li>
  </ul>
  <p>
   <em>
    TELECOMMUNICATIONS
   </em>
  </p>
  <ul>
   <li>
    call detail records (CDRs); identify influencers via call patterns
    <ul>
     <li>
      act on this to retain and recruit own customers
     </li>
     <li>
      sell it?
     </li>
    </ul>
   </li>
   <li>
    IP detail records, "
   </li>
  </ul>
  <p>
   <em>
    LAW ENFORCEMENT &amp; CYBER SECURITY
   </em>
  </p>
  <ul>
   <li>
    real-time situational awareness
   </li>
   <li>
    multimodal surveillance
   </li>
   <li>
    cyber-security detection
   </li>
   <li>
    legal wire tapping
   </li>
   <li>
    face recognition
   </li>
  </ul>
  <p>
   <em>
    POWER INDUSTRY
   </em>
   (smart meters)
  </p>
  <ul>
   <li>
    forecast energy demand
   </li>
   <li>
    optimize supply chain
    <ul>
     <li>
      capacity adjustments
     </li>
     <li>
      distribution network options
     </li>
     <li>
      real-time buying &amp; selling
     </li>
    </ul>
   </li>
  </ul>
  <p>
   <em>
    FINANCIAL SERVICES
   </em>
  </p>
  <ul>
   <li>
    optimal buy/sell decisions
   </li>
   <li>
    detect fraud
   </li>
  </ul>
  <p>
   <em>
    HEALTH SCIENCES
   </em>
  </p>
  <ul>
   <li>
    detect medical emergencies from streaming vital signs
   </li>
  </ul>
  <p>
   <em>
    GOVERNMENT
   </em>
  </p>
  <ul>
   <li>
    surveil and respond to natural disasters
   </li>
   <li>
    monitor water and air quality
   </li>
   <li>
    traffic management
   </li>
  </ul>
  <h2 id="implementing-big-data-initiatives">
   <a>
    1.3. Implementing Big Data initiatives
   </a>
  </h2>
  <h3 id="big-data-maturity-model">
   <a>
    1.3.1. Big Data maturity model
   </a>
  </h3>
  <p>
   Per Zhu et al. (2014, p. 26):
  </p>
  <div class="figure">
   <img src="illos/big-data-maturity.png"/>
  </div>
  <h3 id="when-big-data-versus-data-warehousing">
   <a>
    1.3.2. When Big Data versus data warehousing?
   </a>
  </h3>
  <p>
   Use data warehouses for:
  </p>
  <ul>
   <li>
    <strong>
     Integration of data
    </strong>
    ("Data model designers and ETL architects armed with metadata, data-cleansing tools, and patience must rationalize data formats, source systems. and semantic meaning of the data to make it understandable and trustworthy")
   </li>
   <li>
    <strong>
     Backend for interactive BI tools
    </strong>
   </li>
  </ul>
  <p>
   Use Hadoop as:
  </p>
  <ul>
   <li>
    <strong>
     Repository and refinery for raw data
    </strong>
    (HDFS can capture hundreds of TB per day)
   </li>
   <li>
    <strong>
     Active archive
    </strong>
    (replacing magnetic tape archives)
   </li>
  </ul>
  <table>
   <thead>
    <tr class="header">
     <th align="left">
      Requirement
     </th>
     <th align="left">
      DW
     </th>
     <th align="left">
      Hadoop
     </th>
    </tr>
   </thead>
   <tbody>
    <tr class="odd">
     <td align="left">
      Low latency, interactive reports, and OLAP
     </td>
     <td align="left">
      X
     </td>
     <td align="left">
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      ANSI 2003 SQL compliance is required
     </td>
     <td align="left">
      X
     </td>
     <td align="left">
      X
     </td>
    </tr>
    <tr class="odd">
     <td align="left">
      Preprocessing or exploration of raw unstructured data
     </td>
     <td align="left">
     </td>
     <td align="left">
      X
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      Online archives alternative to tape
     </td>
     <td align="left">
     </td>
     <td align="left">
      X
     </td>
    </tr>
    <tr class="odd">
     <td align="left">
      High-quality cleansed and consistent data
     </td>
     <td align="left">
      X
     </td>
     <td align="left">
      ?
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      100s to 1,000s of concurrent users
     </td>
     <td align="left">
      ?
     </td>
     <td align="left">
      X
     </td>
    </tr>
    <tr class="odd">
     <td align="left">
      Discover unknown relationships in the data
     </td>
     <td align="left">
     </td>
     <td align="left">
      X
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      [Complex parallel] process logic
     </td>
     <td align="left">
      ?
     </td>
     <td align="left">
      X
     </td>
    </tr>
    <tr class="odd">
     <td align="left">
      CPU intense analysis
     </td>
     <td align="left">
      X
     </td>
     <td align="left">
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      System, users, and data governance
     </td>
     <td align="left">
     </td>
     <td align="left">
      X
     </td>
    </tr>
    <tr class="odd">
     <td align="left">
      Many flexible programming languages running in parallel
     </td>
     <td align="left">
     </td>
     <td align="left">
      X
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      Unrestricted, ungoverned sandbox explorations
     </td>
     <td align="left">
     </td>
     <td align="left">
      X
     </td>
    </tr>
    <tr class="odd">
     <td align="left">
      Analysis of provisional data
     </td>
     <td align="left">
      X
     </td>
     <td align="left">
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      Extensive security and regulatory compliance
     </td>
     <td align="left">
      X
     </td>
     <td align="left">
      ?
     </td>
    </tr>
   </tbody>
  </table>
  <h3 id="success-factors-for-big-data-initiatives">
   <a>
    1.3.3. Success factors for Big Data initiatives
   </a>
  </h3>
  <p>
   Sharda et al. (2014, pp. 285-286) cite Watson's (2012) "critical success factors" as follows:
  </p>
  <ul>
   <li>
    "A clear business need (alignment with the vision and the strategy)"
   </li>
   <li>
    "Strong, committed sponsorship (executive champion)"
   </li>
   <li>
    "Alignment between the business and IT strategy"
   </li>
   <li>
    "A fact-based decision-making culture ... also a culture of experimentation"
   </li>
   <li>
    "A strong data infrastructure," see
    <a href="#high-performance-computing">
     high-performance computing
    </a>
   </li>
  </ul>
  <p>
   They also synthesize Lampitt (2012) and a Tableau white paper (pp. 312-313):
  </p>
  <ul>
   <li>
    <strong>
     Simplify
    </strong>
    (the ecosystem is pretty complex)
   </li>
   <li>
    <strong>
     Coexist
    </strong>
    (blend legacy and new systems)
   </li>
   <li>
    <strong>
     Empower
    </strong>
    (support self-service)
   </li>
   <li>
    <strong>
     Integrate
    </strong>
    (more value from combining data across sources, despite the difficulty)
   </li>
   <li>
    <strong>
     Evangelize
    </strong>
   </li>
  </ul>
  <h1 id="big-data-technologies">
   <a>
    2. Big Data technologies
   </a>
  </h1>
  <h2 id="high-performance-computing">
   <a>
    2.1. High-performance computing
   </a>
  </h2>
  <ul>
   <li>
    In-memory analytics
   </li>
   <li>
    In-database analytics
   </li>
   <li>
    <strong>
     Grid computing:
    </strong>
    "Promotes efficiency, lower cost, and better performance by processing jobs in a shared, centrally-managed pool of IT resources"
   </li>
   <li>
    <strong>
     Appliances:
    </strong>
    "Brings together hardware and software in a physical unit that is not only fast but also scalable on an as-needed basis"
   </li>
  </ul>
  <h2 id="generic-big-data-architectures">
   <a>
    2.2. Generic Big Data architectures
   </a>
  </h2>
  <p>
   Per Zhu et al. (2014, p. 6):
  </p>
  <div class="figure">
   <img src="illos/big-data-arch1.png"/>
  </div>
  <p>
   Per Tetadata, their landscape of products AKA
   <a href="https://www.teradata.com/Solutions-and-Industries/unified-data-architecture">
    Unified Data Architecture:
   </a>
  </p>
  <div class="figure">
   <img src="illos/big-data-arch3.jpg"/>
  </div>
  <p>
   Per AsterData, cited in Sharda et al. (2014, p. 283):
  </p>
  <div class="figure">
   <img src="illos/big-data-arch2.jpg"/>
  </div>
  <h2 id="big-data-storage">
   <a>
    2.3. Big Data storage
   </a>
  </h2>
  <h3 id="hadoop">
   <a>
    2.3.1. Hadoop
   </a>
  </h3>
  <p>
   Per Sharda et al. (2014, pp. 291, 294), "Hadoop is an open source framework for processing, storing, and analyzing massive amounts of distibuted, unstructured data"; it is distributed storage plus distributed processing via the
   <a href="#mapreduce">
    MapReduce framework.
   </a>
  </p>
  <ul>
   <li>
    Hadoop's native language is Java
   </li>
   <li>
    "Hadoop consists of multiple products"
   </li>
   <li>
    "HDFS is a file system, not a database management system (DBMS)"
   </li>
   <li>
    "Hadoop is about data diversity, not just data volume. Theoretically, HDFS can manage the storage and access of any data type as long as you can put the data in a file and copy that file into HDFS."
   </li>
  </ul>
  <h4 id="hadoop-components">
   <a>
    2.3.1.1. Hadoop components
   </a>
  </h4>
  <ul>
   <li>
    <strong>
     HDFS:
    </strong>
    "The default storage layer in any given Hadoop cluster"
   </li>
   <li>
    <strong>
     Name node:
    </strong>
    "The node in a Hadoop cluster that provides the client information on where in the cluster particular data is stored and if any nodes fail"
   </li>
   <li>
    <strong>
     Secondary node:
    </strong>
    "A backup to the Name Node, it periodically replicates and stores data from the Name Node should it fail"
   </li>
   <li>
    <strong>
     Job tracker:
    </strong>
    "The node in a Hadoop cluster that initiates and coordinates MapReduce jobs or the processing of the data"
   </li>
   <li>
    <strong>
     Slave nodes:
    </strong>
    "The grunts of any Hadoop cluster, slave nodes store data and take direction to process it from the Job Tracker"
   </li>
  </ul>
  <h4 id="hadoop-suprojects">
   <a>
    2.3.1.2. Hadoop suprojects
   </a>
  </h4>
  <p>
   Per Sharda et al. (2014, pp. 292-293):
  </p>
  <ul>
   <li>
    <strong>
     Hive
    </strong>
    (from Facebook) converts HiveQL (SQL-like) queries to MapReduce jobs, thereby enabling Hadoop to function like a data warehouse in terms of interfacing with users and BI tools
   </li>
   <li>
    <strong>
     Pig
    </strong>
    (from Yahoo) is a Hadoop query language that's "adept at very deep, very long data pipelines"'
   </li>
   <li>
    <strong>
     Flume
    </strong>
    "is a framework for populating Hadoop with data"
   </li>
   <li>
    <strong>
     HBase
    </strong>
    "is a nonrelational database that ... adds transactional capabilities to Hadoop, allowing users to conduct updates, inserts, and deletes"
   </li>
   <li>
    <strong>
     Oozie
    </strong>
    "is a workflow processing system that lets users define a series of jobs written in multiple languages---such as MapReduce, Pig, and Hive---and then intelligently link them together"
   </li>
   <li>
    <strong>
     Ambari
    </strong>
    "is a Web-based set of tools for deploying, administering, and monitoring Apache Hadoop clusters"
   </li>
   <li>
    <strong>
     Avro
    </strong>
    "is a data serialization system that allows for encoding the schema of Hadoop files"
   </li>
   <li>
    <strong>
     Mahout
    </strong>
    "is a data mining library [that] takes the most popular data mining algorithms for performing clustering, regression testing, and statistical modeling and implements them using the MapReduce model"
   </li>
   <li>
    <strong>
     Sqoop
    </strong>
    "is a connectivity tool for moving data from non-Hadoop data stores ... into Hadoop"
   </li>
   <li>
    <strong>
     HCatalog
    </strong>
    "is a centralized metadata management and sharing service"
   </li>
  </ul>
  <h3 id="what-are-nosql-databases">
   <a>
    2.3.2. What are NoSQL databases?
   </a>
  </h3>
  <p>
   Per Connolly and Begg (2015):
  </p>
  <p>
   NoSQL databases use non-relational data models ...
  </p>
  <ul>
   <li>
    <strong>
     Key-value model,
    </strong>
    e.g. Dynamo, Riak, Basho:
    <em>
     [Key|Value|Timestamp].
    </em>
    Provides easy and fast storage for simple data.
   </li>
   <li>
    <strong>
     Columnar model,
    </strong>
    e.g. Googleâ€™s Bigtable, Apacheâ€™s HBase (part of Hadoop):
    <em>
     [Row Key|Value|Timestamp|Column Family|Column Name].
    </em>
    Good for retaining relationships (since columns can be grouped into families).
   </li>
   <li>
    <strong>
     Document model,
    </strong>
    e.g. MongoDB, JSON, XML. Good for storing complex hierarchical relationships.
   </li>
   <li>
    <strong>
     Graph/triple model,
    </strong>
    e.g. Neo4j. Good for capturing a web of relationships.
   </li>
  </ul>
  <p>
   ... plus some of these other features ...
  </p>
  <ul>
   <li>
    Open source &amp; less costly hardware
   </li>
   <li>
    Distributed storage and processing rather than client/server architecture
   </li>
   <li>
    Memory cache
   </li>
   <li>
    Batch processing (Google Map Reduce) or interactive AKA stream processing (Apache Tez Framework, Apache Spark, Facebook Presto)
   </li>
   <li>
    Proprietary and/or (for Presto, Hive QL, Pig, Cassandra Query Language (CQL), Cosmos/Scope) SQL-like interfaces
   </li>
   <li>
    Analytics integration (Hive, Amazonâ€™s Redshift, Facebookâ€™s Presto, Airbnbâ€™s Airpal)
   </li>
  </ul>
  <p>
   ... to store Big Data, achieving better performance by:
  </p>
  <ul>
   <li>
    <strong>
     appending
    </strong>
    rather than updating records, and
   </li>
   <li>
    <strong>
     denormalizing
    </strong>
    data upon input
   </li>
  </ul>
  <h4 id="nosql-databases-versus-other-data-store-options">
   <a>
    2.3.2.1. NoSQL databases versus other data store options
   </a>
  </h4>
  <p>
   Per Sharda et al. (2014, p. 295): "[W]hereas
   <strong>
    Hadoop
   </strong>
   is adept at supporting large-scale, batch-style historical analysis,
   <strong>
    NoSQL databases
   </strong>
   are aimed, for the most part (although there are some important exceptions), at serving up discrete data stored among large volumes of multi-structured data ... [a capability] sorely lacking in
   <strong>
    relational database technology
   </strong>
   ... the downside of most NoSQL databases today is that they trade ACID (atomicity, consistency, isolation, durability) compliance for performance and scalability. Many also lack mature management and monitoring tools".
  </p>
  <h4 id="nosql-database-software">
   <a>
    2.3.2.2. NoSQL database software
   </a>
  </h4>
  <ul>
   <li>
    HBase (a Hadoop subproject)
   </li>
   <li>
    Neo4j
   </li>
   <li>
    MongoDB
   </li>
   <li>
    Cassandra
   </li>
   <li>
    CouchDB
   </li>
  </ul>
  <h2 id="big-data-analytics">
   <a>
    2.4. Big Data analytics
   </a>
  </h2>
  <p>
   See
   <a href="https://jtkovacs.github.io/refs/data-science.html">
    notes on data science.
   </a>
  </p>
  <h3 id="mapreduce">
   <a>
    2.4.1. MapReduce
   </a>
  </h3>
  <p>
   Per Dean and Ghemawat's seminal paper (2004): "MapReduce is a programming model and an associated implementaiton for processing and generating large data sets. Programs written in this functional style are automatically patallelized and executed on a large cluster of commodity machines. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system."
  </p>
  <p>
   Another way of putting this, from Russom (2010) by way of Sharda et al. (2014, p. 295): "MapReduce provides control for analytics, not analytics per se. MapReduce is a general-purpose execution engine that handles the complexities of network communication, parallel programming, and fault-tolerance for any kind of application that you can hand code---not just analytics."
  </p>
  <div class="figure">
   <img src="illos/map-reduce.png"/>
  </div>
  <p>
   Per Sharda et al. (2014, p. 290):
  </p>
  <ul>
   <li>
    "The
    <strong>
     MapReduce system
    </strong>
    first reads the input file and splits it into multiple pieces ...
   </li>
   <li>
    These splits are then processed by
    <strong>
     multiple map programs
    </strong>
    running in parallel on the nodes of the cluster ...
   </li>
   <li>
    The
    <strong>
     MapReduce system
    </strong>
    then takes the ouput from each map program and merges (shuffle/sort) the results for input to
   </li>
   <li>
    the
    <strong>
     reduce program,
    </strong>
    which [aggregates the results and outputs them]"
   </li>
  </ul>
  <h3 id="data-stream-mining">
   <a>
    2.4.2. Data stream mining
   </a>
  </h3>
  <p>
   (AKA data-in-motion analytics, AKA in-motion analytics, AKA real-time data analytics)
  </p>
  <p>
   Per Sharda et al. (2014, p. 215), stream analytics began in the energy industry, and has become important because:
  </p>
  <ul>
   <li>
    Despite Big Data storage technologies, keeping everything is impossible; "current total storage capacity lags far behind the digital information being generated in the world"
   </li>
   <li>
    A turbulent business environment makes "real-time detection of meaningful changes in data as well as complex pattern variations within a short time
    <strong>
     window ...
    </strong>
    essential to come up with the actions that better fit with the new environment"
   </li>
  </ul>
  <p>
   Some related concepts:
  </p>
  <ul>
   <li>
    <strong>
     Perpetual analytics
    </strong>
    is different from data stream mining, since it "evaluates every incoming observation against all prior observations", i.e. does not use a window
   </li>
   <li>
    <strong>
     Critical event processing
    </strong>
    is the anticipation or extremely rapid detection of outlier events, to enable prevention or immediate response
   </li>
  </ul>
  <h1 id="sources">
   <a>
    3. Sources
   </a>
  </h1>
  <h2 id="cited">
   <a>
    3.1. Cited
   </a>
  </h2>
  <p>
   Connolly, T. &amp; Begg, C. (2015).
   <em>
    Database systems: A practical approach to design, implementation, and management
   </em>
   (6th ed.). New York City, NY: Pearson Education.
  </p>
  <p>
   Dean, J., &amp; Ghemawat, S. (2004). MapReduce: Simplified data processing on large clusters. Retrieved from
   <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf">
    https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf
   </a>
  </p>
  <p>
   Sharda, R., Delen, D., &amp; Turban, E. (2014).
   <em>
    Business intelligence: A managerial perspective on analytics
   </em>
   (3rd ed.). New York City, NY: Pearson.
  </p>
  <p>
   Zhu, W-D., Gupta, M., Kumar, V., Perepa, S., Sathi, A., &amp; Statchuk, C. (2014). Building Big Data and analytics solutions in the cloud. IBM Redpaper. Retrieved from
   <a href="https://www.redbooks.ibm.com/redpapers/pdfs/redp5085.pdf">
    https://www.redbooks.ibm.com/redpapers/pdfs/redp5085.pdf
   </a>
  </p>
  <h2 id="references">
   <a>
    3.2. References
   </a>
  </h2>
  <h2 id="read">
   <a>
    3.3. Read
   </a>
  </h2>
  <ul>
   <li>
    <a href="https://www.youtube.com/watch?v=43fqzaSH0CQ">
     What is MapReduce? (Youtube video)
    </a>
   </li>
   <li>
    <a href="https://www.youtube.com/watch?v=4DgTLaFNQq0">
     What is Hadoop? (Youtube video)
    </a>
   </li>
  </ul>
  <h2>
   <a name="3.4.-unread">
    3.4. Unread
   </a>
  </h2>
 </body>
</html>
