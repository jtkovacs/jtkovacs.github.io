<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
  <meta content="text/css" http-equiv="Content-Style-Type"/>
  <meta content="pandoc" name="generator"/>
  <title>
   jtck.github.io | big data
  </title>
  <link href="../assets/styles/main.css" rel="stylesheet" type="text/css"/>
  <link href="../assets/styles/refs.css" rel="stylesheet" type="text/css"/>
 </head>
 <body>
  <p class="path">
   <a href="../pkb.html">
    pkb contents
   </a>
   &gt; big data | just under 1505 words | updated 12/30/2017
  </p>
  <div class="TOC">
   <ul>
    <li>
     1.
     <a href="#what-is-big-data">
      What is Big Data?
     </a>
     <ul>
      <li>
       1.1.
       <a href="#sources-of-big-data">
        Sources of Big Data
       </a>
      </li>
      <li>
       1.2.
       <a href="#business-applications-of-big-data">
        Business applications of Big Data
       </a>
       <ul>
        <li>
         1.2.1.
         <a href="#business-applications-of-stream-analytics">
          Business applications of stream analytics
         </a>
        </li>
       </ul>
      </li>
      <li>
       1.3.
       <a href="#implementing-big-data-initiatives">
        Implementing Big Data initiatives
       </a>
       <ul>
        <li>
         1.3.1.
         <a href="#big-data-maturity-model">
          Big Data maturity model
         </a>
        </li>
        <li>
         1.3.2.
         <a href="#when-big-data-versus-data-warehousing">
          When Big Data versus data warehousing?
         </a>
        </li>
        <li>
         1.3.3.
         <a href="#success-factors-for-big-data-initiatives">
          Success factors for Big Data initiatives
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li>
     2.
     <a href="#big-data-technologies">
      Big Data technologies
     </a>
     <ul>
      <li>
       2.1.
       <a href="#high-performance-computing">
        High-performance computing
       </a>
      </li>
      <li>
       2.2.
       <a href="#generic-big-data-architectures">
        Generic Big Data architectures
       </a>
      </li>
      <li>
       2.3.
       <a href="#big-data-storage">
        Big Data storage
       </a>
       <ul>
        <li>
         2.3.1.
         <a href="#hadoop">
          Hadoop
         </a>
         <ul>
          <li>
           2.3.1.1.
           <a href="#why-use-hadoop">
            Why use Hadoop?
           </a>
          </li>
          <li>
           2.3.1.2.
           <a href="#hadoop-components">
            Hadoop components
           </a>
           <ul>
            <li>
             2.3.1.2.1.
             <a href="#hdfs">
              HDFS
             </a>
            </li>
            <li>
             2.3.1.2.2.
             <a href="#name-node">
              Name node
             </a>
            </li>
            <li>
             2.3.1.2.3.
             <a href="#secondary-node">
              Secondary node
             </a>
            </li>
            <li>
             2.3.1.2.4.
             <a href="#job-tracker">
              Job tracker
             </a>
            </li>
            <li>
             2.3.1.2.5.
             <a href="#slave-nodes">
              Slave nodes
             </a>
            </li>
           </ul>
          </li>
          <li>
           2.3.1.3.
           <a href="#hadoop-suprojects">
            Hadoop suprojects
           </a>
          </li>
         </ul>
        </li>
        <li>
         2.3.2.
         <a href="#what-are-nosql-databases">
          What are NoSQL databases?
         </a>
         <ul>
          <li>
           2.3.2.1.
           <a href="#nosql-database-software">
            NoSQL database software
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       2.4.
       <a href="#big-data-analytics">
        Big Data analytics
       </a>
       <ul>
        <li>
         2.4.1.
         <a href="#mapreduce">
          MapReduce
         </a>
        </li>
        <li>
         2.4.2.
         <a href="#data-stream-mining">
          Data stream mining
         </a>
         <ul>
          <li>
           2.4.2.1.
           <a href="#critical-event-processing">
            Critical event processing
           </a>
          </li>
          <li>
           2.4.2.2.
           <a href="#data-stream-mining-versus-perpetual-analytics">
            Data stream mining versus perpetual analytics
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li>
     3.
     <a href="#sources">
      Sources
     </a>
     <ul>
      <li>
       3.1.
       <a href="#cited">
        Cited
       </a>
      </li>
      <li>
       3.2.
       <a href="#references">
        References
       </a>
      </li>
      <li>
       3.3.
       <a href="#read">
        Read
       </a>
      </li>
      <li>
       3.4.
       <a href="#unread">
        Unread
       </a>
      </li>
     </ul>
    </li>
   </ul>
  </div>
  <h1 id="what-is-big-data">
   <a>
    1. What is Big Data?
   </a>
  </h1>
  <p>
   Per Sharda et al. (2014, pp. 280-282):
  </p>
  <p>
   <strong>
    Volume
   </strong>
  </p>
  <table>
   <thead>
    <tr class="header">
     <th align="left">
      Year
     </th>
     <th align="left">
      Estimated World Data
     </th>
    </tr>
   </thead>
   <tbody>
    <tr class="odd">
     <td align="left">
      2009
     </td>
     <td align="left">
      0.8 ZB
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      2010
     </td>
     <td align="left">
      &gt;1 ZB
     </td>
    </tr>
    <tr class="odd">
     <td align="left">
      2011
     </td>
     <td align="left">
      1.8 ZB
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      2020
     </td>
     <td align="left">
      35 ZB
     </td>
    </tr>
   </tbody>
  </table>
  <ul>
   <li>
    Kilobyte (kB) = 10
    <sup>
     3
    </sup>
   </li>
   <li>
    Megabyte (MB) = 10
    <sup>
     6
    </sup>
   </li>
   <li>
    Gigabyte (GB) = 10
    <sup>
     9
    </sup>
   </li>
   <li>
    Terabyte (TB) = 10
    <sup>
     12
    </sup>
   </li>
   <li>
    Petabyte (PB) = 10
    <sup>
     15
    </sup>
   </li>
   <li>
    Exabyte (EB) = 10
    <sup>
     18
    </sup>
   </li>
   <li>
    Zettabyte (ZB) = 10
    <sup>
     21
    </sup>
   </li>
   <li>
    Yottabyte (YB) = 10
    <sup>
     24
    </sup>
   </li>
  </ul>
  <p>
   <strong>
    Variety
   </strong>
   (in format; about 80-85% unstructured)
  </p>
  <ul>
   <li>
    RDBMS, hierachical data stores, OLAP
   </li>
   <li>
    text documents, email
   </li>
   <li>
    XML, HTML
   </li>
   <li>
    meter-collected, sensor-captured data
   </li>
   <li>
    video, audio
   </li>
   <li>
    stock ticker
   </li>
  </ul>
  <p>
   <strong>
    Velocity
   </strong>
  </p>
  <ul>
   <li>
    increased speed of data
    <strong>
     production
    </strong>
    ("RFID tags, automated sensors, GPS devices, and smart meters are driving an increasing need to deal with torrents of data in near-real time")
   </li>
   <li>
    <strong>
     demand
    </strong>
    for quicker data processing, i.e.
    <a href="#data-stream-mining">
     data stream mining
    </a>
   </li>
  </ul>
  <p>
   <strong>
    Veracity
   </strong>
  </p>
  <ul>
   <li>
    coined by IBM
   </li>
   <li>
    "conformity with facts: accuracy, quality, truthfulness, or trustiworthiness"
   </li>
  </ul>
  <p>
   <strong>
    Variability
   </strong>
   ("Daily, seasonal, and event-driven peak data loads")
  </p>
  <p>
   <strong>
    Value
   </strong>
   (one hopes)
  </p>
  <h2 id="sources-of-big-data">
   <a>
    1.1. Sources of Big Data
   </a>
  </h2>
  <p>
   "Web logs, RFID, GPS systems, sensor networks, social networks, Internet-based text documents, Internet search indexes, detail call records, astronomy, atmospheric science, biology, genomics, nuclear physics, biochemical experiments, medical records, scientific research, military surveillance, photography archives, video archives, and large-scalre e-commerce practices" (Sharda et al., 2014, pp. 278-280).
  </p>
  <h2 id="business-applications-of-big-data">
   <a>
    1.2. Business applications of Big Data
   </a>
  </h2>
  <p>
   Sharda et al. (2014, pp. 287):
  </p>
  <ul>
   <li>
    "Process efficiency and cost reduction
   </li>
   <li>
    Brand management
   </li>
   <li>
    Revenue maximization, cross-selling, and up-selling
   </li>
   <li>
    Enhanced customer experience
   </li>
   <li>
    Churn identification, customer recruiting
   </li>
   <li>
    Improved customer service
   </li>
   <li>
    Identifying new products and market opportunities
   </li>
   <li>
    Risk management
   </li>
   <li>
    Regulatory compliance
   </li>
   <li>
    Enhanced security capabilities"
   </li>
  </ul>
  <p>
   Per Zhu et al. (2014, pp. 16-17), there are four categories of business goals that companies may fruitfully pursue with Big Data:
  </p>
  <p>
   <em>
    REVENUE
   </em>
  </p>
  <ul>
   <li>
    <strong>
     Monetize big data:
    </strong>
    Design and execute big data analytics use cases that increase revenue, lower costs, or reduce risk.
   </li>
   <li>
    <strong>
     Manage big data at a low cost:
    </strong>
    Demonstrate cost savings of big data analytics styles for both MapReduce clusters and real-time analytics.
   </li>
   <li>
    <strong>
     Improve efficiency in business operations:
    </strong>
    Develop insight about the value of specific business processes, such as enterprise resource planning (ERP), supply chain management (SCM), and customer relationship management (CRM).
   </li>
  </ul>
  <p>
   <em>
    CUSTOMER SERVICES
   </em>
  </p>
  <ul>
   <li>
    <strong>
     Improve customer understanding (360-degree view of the customer):
    </strong>
    Mine all sources of client experience and interaction from additional unstructured and semi-structured data types using real-time and batch (Hadoop) analytics.
   </li>
   <li>
    <strong>
     Obtain behavioral insight into client transactions:
    </strong>
    | What led to a certain business transaction? Why did the client choose us? What else can we deduce about a client’s buying behavior?
   </li>
   <li>
    <strong>
     Attract and retain customers:
    </strong>
    Mine and apply insight toward marketing and sales effectiveness with clients, customers, and customer support personnel.
   </li>
   <li>
    <strong>
     Fraud detection and claims processing:
    </strong>
    Derive and exploit additional insight from data types not previously analyzed for anti-fraud and claims processing.
   </li>
  </ul>
  <p>
   <em>
    BUSINESS DEVELOPMENT
   </em>
  </p>
  <ul>
   <li>
    <strong>
     Introduce new products or services:
    </strong>
    Thanks to your new insight about target market preferences, new products and services will have higher adoption rates by the target clientele.
   </li>
   <li>
    <strong>
     Outsource non-core functions:
    </strong>
    Decide what to outsource without affecting the customer experience.
   </li>
   <li>
    <strong>
     Pursue mergers, acquisitions, and divestitures:
    </strong>
    Gather and consider marketplace insights about the potential impact of mergers, acquisitions, and divestitures.
   </li>
   <li>
    <strong>
     Gain new competitive insights:
    </strong>
    Mine all sources of information, even non-traditional sources of information, to learn about the brand perception of the company by its customers, its reputation, and its industry ranking. | Define metrics for improvement that are achievable if based on better insight.
   </li>
  </ul>
  <p>
   <em>
    BUSINESS AGILITY &amp; GOVERNANCE
   </em>
  </p>
  <ul>
   <li>
    <strong>
     Increase business agility:
    </strong>
    Mine real-time events for trends and apply the insight to transactions and interactions with customers.
   </li>
   <li>
    <strong>
     Plan with greater confidence:
    </strong>
    Build better scenario-based analysis models.
   </li>
   <li>
    <strong>
     Make better decisions faster:
    </strong>
    Harvest better insights from both batch (Hadoop) and real-time events and rapidly make them available to decision makers.
   </li>
   <li>
    <strong>
     Ensure regulatory compliance:
    </strong>
    Improve your understanding of the current regulatory climate and expectations of auditors.
   </li>
   <li>
    <strong>
     Lower risk:
    </strong>
    Improve the cost-benefit analysis of various risks (regulatory, market, credit, counter-party operational, and so on).
   </li>
  </ul>
  <h3 id="business-applications-of-stream-analytics">
   <a>
    1.2.1. Business applications of stream analytics
   </a>
  </h3>
  <p>
   Per Sharda et al. (2014, pp. 317-321):
  </p>
  <p>
   <em>
    e-COMMERCE
   </em>
   ("analysis of [clickstream] data can turn browsers into buyers and buyers into shopaholics")
  </p>
  <ul>
   <li>
    recommendations
   </li>
   <li>
    sales
   </li>
   <li>
    bundle offers
   </li>
  </ul>
  <p>
   <em>
    TELECOMMUNICATIONS
   </em>
  </p>
  <ul>
   <li>
    call detail records (CDRs); identify influencers via call patterns
    <ul>
     <li>
      act on this to retain and recruit own customers
     </li>
     <li>
      sell it?
     </li>
    </ul>
   </li>
   <li>
    IP detail records, "
   </li>
  </ul>
  <p>
   <em>
    LAW ENFORCEMENT &amp; CYBER SECURITY
   </em>
  </p>
  <ul>
   <li>
    real-time situational awareness
   </li>
   <li>
    multimodal surveillance
   </li>
   <li>
    cyber-security detection
   </li>
   <li>
    legal wire tapping
   </li>
   <li>
    face recognition
   </li>
  </ul>
  <p>
   <em>
    POWER INDUSTRY
   </em>
   (smart meters)
  </p>
  <ul>
   <li>
    forecast energy demand
   </li>
   <li>
    optimize supply chain
    <ul>
     <li>
      capacity adjustments
     </li>
     <li>
      distribution network options
     </li>
     <li>
      real-time buying &amp; selling
     </li>
    </ul>
   </li>
  </ul>
  <p>
   <em>
    FINANCIAL SERVICES
   </em>
  </p>
  <ul>
   <li>
    optimal buy/sell decisions
   </li>
   <li>
    detect fraud
   </li>
  </ul>
  <p>
   <em>
    HEALTH SCIENCES
   </em>
  </p>
  <ul>
   <li>
    detect medical emergencies from streaming vital signs
   </li>
  </ul>
  <p>
   <em>
    GOVERNMENT
   </em>
  </p>
  <ul>
   <li>
    surveil and respond to natural disasters
   </li>
   <li>
    monitor water and air quality
   </li>
   <li>
    traffic management
   </li>
  </ul>
  <h2 id="implementing-big-data-initiatives">
   <a>
    1.3. Implementing Big Data initiatives
   </a>
  </h2>
  <h3 id="big-data-maturity-model">
   <a>
    1.3.1. Big Data maturity model
   </a>
  </h3>
  <p>
   Per Zhu et al. (2014, p. 26):
  </p>
  <div class="figure">
   <img src="illos/big-data-maturity.png"/>
  </div>
  <h3 id="when-big-data-versus-data-warehousing">
   <a>
    1.3.2. When Big Data versus data warehousing?
   </a>
  </h3>
  <p>
   Use Hadoop as:
  </p>
  <ul>
   <li>
    Repository and refinery for raw data (HDFS can capture hundreds of TB per day)
   </li>
   <li>
    Active archive (replacing magnetic tape archives)
   </li>
  </ul>
  <p>
   Use data warehouses for:
  </p>
  <ul>
   <li>
    While performance is satisfactory
    <ul>
     <li>
      basic indexing
     </li>
     <li>
      advanced indexing
      <ul>
       <li>
        materialized views
       </li>
       <li>
        aggregate join indexes
       </li>
       <li>
        cube indexes
       </li>
       <li>
        spares join indexes
       </li>
      </ul>
     </li>
     <li>
      cost-based optimizer (analyzing SQL query, generates alternatives, compares cost)
     </li>
     <li>
      partitioning
     </li>
    </ul>
   </li>
   <li>
    Integration of data ("Data model designers and ETL architects armed with metadata, data-cleansing tools, and patience must rationalize data formats, source systems. and semantic meaning of the data to make it understandable and trustworthy")
   </li>
   <li>
    Backend for interactive BI tools
   </li>
  </ul>
  <h3 id="success-factors-for-big-data-initiatives">
   <a>
    1.3.3. Success factors for Big Data initiatives
   </a>
  </h3>
  <p>
   Sharda et al. (2014, pp. 285-286) cite Watson's (2012) "critical success factors" as follows:
  </p>
  <ul>
   <li>
    "A clear business need (alignment with the vision and the strategy)"
   </li>
   <li>
    "Strong, committed sponsorship (executive champion)"
   </li>
   <li>
    "Alignment between the business and IT strategy"
   </li>
   <li>
    "A fact-based decision-making culture ... also a culture of experimentation"
   </li>
   <li>
    "A strong data infrastructure," see
    <a href="#high-performance-computing">
     high-performance computing
    </a>
   </li>
  </ul>
  <p>
   They also synthesize Lampitt (2012) and a Tableau white paper (pp. 312-313):
  </p>
  <ul>
   <li>
    <strong>
     Simplify
    </strong>
    (the ecosystem is pretty complex)
   </li>
   <li>
    <strong>
     Coexist
    </strong>
    (blend legacy and new systems)
   </li>
   <li>
    <strong>
     Empower
    </strong>
    (support self-service)
   </li>
   <li>
    <strong>
     Integrate
    </strong>
    (more value from combining data across sources, despite the difficulty)
   </li>
   <li>
    <strong>
     Evangelize
    </strong>
   </li>
  </ul>
  <h1 id="big-data-technologies">
   <a>
    2. Big Data technologies
   </a>
  </h1>
  <h2 id="high-performance-computing">
   <a>
    2.1. High-performance computing
   </a>
  </h2>
  <ul>
   <li>
    In-memory analytics
   </li>
   <li>
    In-database analytics
   </li>
   <li>
    <strong>
     Grid computing:
    </strong>
    "Promotes efficiency, lower cost, and better performance by processing jobs in a shared, centrally-managed pool of IT resources"
   </li>
   <li>
    <strong>
     Appliances:
    </strong>
    "Brings together hardware and software in a physical unit that is not only fast but also scalable on an as-needed basis"
   </li>
  </ul>
  <h2 id="generic-big-data-architectures">
   <a>
    2.2. Generic Big Data architectures
   </a>
  </h2>
  <p>
   Per Zhu et al. (2014, p. 6):
  </p>
  <div class="figure">
   <img src="illos/big-data-arch1.png"/>
  </div>
  <p>
   Per AsterData, cited in Sharda et al. (2014, p. 283):
  </p>
  <div class="figure">
   <img src="illos/big-data-arch2.jpg"/>
  </div>
  <p>
   Per Tetadata, their landscape of products AKA
   <a href="https://www.teradata.com/Solutions-and-Industries/unified-data-architecture">
    Unified Data Architecture:
   </a>
  </p>
  <div class="figure">
   <img src="illos/big-data-arch3.jpg"/>
  </div>
  <h2 id="big-data-storage">
   <a>
    2.3. Big Data storage
   </a>
  </h2>
  <h3 id="hadoop">
   <a>
    2.3.1. Hadoop
   </a>
  </h3>
  <h4 id="why-use-hadoop">
   <a>
    2.3.1.1. Why use Hadoop?
   </a>
  </h4>
  <h4 id="hadoop-components">
   <a>
    2.3.1.2. Hadoop components
   </a>
  </h4>
  <h5 id="hdfs">
   <a>
    2.3.1.2.1. HDFS
   </a>
  </h5>
  <h5 id="name-node">
   <a>
    2.3.1.2.2. Name node
   </a>
  </h5>
  <h5 id="secondary-node">
   <a>
    2.3.1.2.3. Secondary node
   </a>
  </h5>
  <h5 id="job-tracker">
   <a>
    2.3.1.2.4. Job tracker
   </a>
  </h5>
  <h5 id="slave-nodes">
   <a>
    2.3.1.2.5. Slave nodes
   </a>
  </h5>
  <h4 id="hadoop-suprojects">
   <a>
    2.3.1.3. Hadoop suprojects
   </a>
  </h4>
  <ul>
   <li>
    Hive
   </li>
   <li>
    Pig
   </li>
   <li>
    ...
   </li>
  </ul>
  <h3 id="what-are-nosql-databases">
   <a>
    2.3.2. What are NoSQL databases?
   </a>
  </h3>
  <p>
   Per Connolly and Begg (2015):
  </p>
  <p>
   NoSQL databases use non-relational data models ...
  </p>
  <ul>
   <li>
    <strong>
     Key-value model,
    </strong>
    e.g. Dynamo, Riak, Basho:
    <em>
     [Key|Value|Timestamp].
    </em>
    Provides easy and fast storage for simple data.
   </li>
   <li>
    <strong>
     Columnar model,
    </strong>
    e.g. Google’s Bigtable, Apache’s HBase (part of Hadoop):
    <em>
     [Row Key|Value|Timestamp|Column Family|Column Name].
    </em>
    Good for retaining relationships (since columns can be grouped into families).
   </li>
   <li>
    <strong>
     Document model,
    </strong>
    e.g. MongoDB, JSON, XML. Good for storing complex hierarchical relationships.
   </li>
   <li>
    <strong>
     Graph/triple model,
    </strong>
    e.g. Neo4j. Good for capturing a web of relationships.
   </li>
  </ul>
  <p>
   ... plus some of these other features ...
  </p>
  <ul>
   <li>
    Open source &amp; less costly hardware
   </li>
   <li>
    Distributed storage and processing rather than client/server architecture
   </li>
   <li>
    Memory cache
   </li>
   <li>
    Batch processing (Google Map Reduce) or interactive AKA stream processing (Apache Tez Framework, Apache Spark, Facebook Presto)
   </li>
   <li>
    Proprietary and/or (for Presto, Hive QL, Pig, Cassandra Query Language (CQL), Cosmos/Scope) SQL-like interfaces
   </li>
   <li>
    Analytics integration (Hive, Amazon’s Redshift, Facebook’s Presto, Airbnb’s Airpal)
   </li>
  </ul>
  <p>
   ... to store Big Data, achieving better performance by:
  </p>
  <ul>
   <li>
    <strong>
     appending
    </strong>
    rather than updating records, and
   </li>
   <li>
    <strong>
     denormalizing
    </strong>
    data upon input
   </li>
  </ul>
  <h4 id="nosql-database-software">
   <a>
    2.3.2.1. NoSQL database software
   </a>
  </h4>
  <ul>
   <li>
    MongoDB
   </li>
   <li>
    Cassandra
   </li>
   <li>
    CouchDB
   </li>
  </ul>
  <h2 id="big-data-analytics">
   <a>
    2.4. Big Data analytics
   </a>
  </h2>
  <p>
   See
   <a href="https://jtkovacs.github.io/refs/data-science.html">
    notes on data science.
   </a>
  </p>
  <h3 id="mapreduce">
   <a>
    2.4.1. MapReduce
   </a>
  </h3>
  <h3 id="data-stream-mining">
   <a>
    2.4.2. Data stream mining
   </a>
  </h3>
  <p>
   (AKA in-motion analytics)
  </p>
  <h4 id="critical-event-processing">
   <a>
    2.4.2.1. Critical event processing
   </a>
  </h4>
  <h4 id="data-stream-mining-versus-perpetual-analytics">
   <a>
    2.4.2.2. Data stream mining versus perpetual analytics
   </a>
  </h4>
  <h1 id="sources">
   <a>
    3. Sources
   </a>
  </h1>
  <h2 id="cited">
   <a>
    3.1. Cited
   </a>
  </h2>
  <p>
   Connolly, T. &amp; Begg, C. (2015).
   <em>
    Database systems: A practical approach to design, implementation, and management
   </em>
   (6th ed.). New York City, NY: Pearson Education.
  </p>
  <p>
   Sharda, R., Delen, D., &amp; Turban, E. (2014).
   <em>
    Business intelligence: A managerial perspective on analytics
   </em>
   (3rd ed.). New York City, NY: Pearson.
  </p>
  <p>
   Zhu, W-D., Gupta, M., Kumar, V., Perepa, S., Sathi, A., &amp; Statchuk, C. (2014). Building Big Data and analytics solutions in the cloud. IBM Redpaper. Retrieved from
   <a href="https://www.redbooks.ibm.com/redpapers/pdfs/redp5085.pdf">
    https://www.redbooks.ibm.com/redpapers/pdfs/redp5085.pdf
   </a>
  </p>
  <h2 id="references">
   <a>
    3.2. References
   </a>
  </h2>
  <h2 id="read">
   <a>
    3.3. Read
   </a>
  </h2>
  <h2>
   <a name="3.4.-unread">
    3.4. Unread
   </a>
  </h2>
 </body>
</html>
