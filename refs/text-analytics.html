<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
  <meta content="text/css" http-equiv="Content-Style-Type"/>
  <meta content="pandoc" name="generator"/>
  <title>
   jtck.github.io | text analytics
  </title>
  <link href="../assets/styles/main.css" rel="stylesheet" type="text/css"/>
  <style type="text/css">
   table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <link href="../assets/styles/refs.css" rel="stylesheet" type="text/css"/>
 </head>
 <body>
  <p class="path">
   <a href="../pkb.html">
    pkb contents
   </a>
   &gt; text analytics | just under 3263 words | updated 12/29/2017
  </p>
  <div class="TOC">
   <ul>
    <li>
     1.
     <a href="#what-is-text-analytics">
      What is text analytics?
     </a>
     <ul>
      <li>
       1.1.
       <a href="#business-applications-of-text-analytics">
        Business applications of text analytics
       </a>
       <ul>
        <li>
         1.1.1.
         <a href="#applications-by-technique">
          Applications by technique
         </a>
        </li>
        <li>
         1.1.2.
         <a href="#applications-by-industry">
          Applications by industry
         </a>
         <ul>
          <li>
           1.1.2.1.
           <a href="#deception-detection">
            Deception detection
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li>
     2.
     <a href="#text-analytics-techniques">
      Text analytics techniques
     </a>
     <ul>
      <li>
       2.1.
       <a href="#text-mining">
        Text mining
       </a>
       <ul>
        <li>
         2.1.1.
         <a href="#establish-the-corpus">
          Establish the corpus
         </a>
        </li>
        <li>
         2.1.2.
         <a href="#create-term-by-document-matrix">
          Create term-by-document matrix
         </a>
        </li>
        <li>
         2.1.3.
         <a href="#analyze">
          Analyze
         </a>
         <ul>
          <li>
           2.1.3.1.
           <a href="#clustering">
            Clustering
           </a>
          </li>
          <li>
           2.1.3.2.
           <a href="#association">
            Association
           </a>
          </li>
          <li>
           2.1.3.3.
           <a href="#classification">
            Classification
           </a>
          </li>
          <li>
           2.1.3.4.
           <a href="#trend-analysis">
            Trend analysis
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       2.2.
       <a href="#natural-language-processing">
        Natural language processing
       </a>
       <ul>
        <li>
         2.2.1.
         <a href="#challenges-with-nlp">
          Challenges with NLP
         </a>
        </li>
        <li>
         2.2.2.
         <a href="#sentiment-analysis">
          Sentiment analysis
         </a>
         <ul>
          <li>
           2.2.2.1.
           <a href="#generic-sentiment-analysis-process">
            Generic sentiment analysis process
           </a>
          </li>
          <li>
           2.2.2.2.
           <a href="#challenges-with-sentiment-identification">
            Challenges with sentiment identification
           </a>
          </li>
          <li>
           2.2.2.3.
           <a href="#methods-for-sentiment-identification">
            Methods for sentiment identification
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       2.3.
       <a href="#web-mining">
        Web mining
       </a>
       <ul>
        <li>
         2.3.1.
         <a href="#challenges-with-web-mining">
          Challenges with web mining
         </a>
        </li>
        <li>
         2.3.2.
         <a href="#web-crawlers-structure-content-mining">
          Web crawlers (structure &amp; content mining)
         </a>
        </li>
        <li>
         2.3.3.
         <a href="#web-analytics-usage-mining">
          Web analytics (usage mining)
         </a>
         <ul>
          <li>
           2.3.3.1.
           <a href="#metrics-for-web-analytics">
            Metrics for web analytics
           </a>
          </li>
          <li>
           2.3.3.2.
           <a href="#technologies-for-web-analytics">
            Technologies for web analytics
           </a>
          </li>
         </ul>
        </li>
        <li>
         2.3.4.
         <a href="#social-analytics">
          Social analytics
         </a>
         <ul>
          <li>
           2.3.4.1.
           <a href="#social-network-analysis">
            Social network analysis
           </a>
          </li>
          <li>
           2.3.4.2.
           <a href="#social-media-analytics">
            Social media analytics
           </a>
           <ul>
            <li>
             2.3.4.2.1.
             <a href="#tools-for-social-media-analytics">
              Tools for social media analytics
             </a>
            </li>
           </ul>
          </li>
         </ul>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li>
     3.
     <a href="#text-analytics-tools">
      Text analytics tools
     </a>
     <ul>
      <li>
       3.1.
       <a href="#ibm-watson">
        IBM Watson
       </a>
      </li>
      <li>
       3.2.
       <a href="#attensity">
        Attensity
       </a>
      </li>
      <li>
       3.3.
       <a href="#python">
        Python
       </a>
       <ul>
        <li>
         3.3.1.
         <a href="#string-manipulation">
          String manipulation
         </a>
        </li>
        <li>
         3.3.2.
         <a href="#regex">
          Regex
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li>
     4.
     <a href="#sources">
      Sources
     </a>
     <ul>
      <li>
       4.1.
       <a href="#cited">
        Cited
       </a>
      </li>
      <li>
       4.2.
       <a href="#references">
        References
       </a>
      </li>
      <li>
       4.3.
       <a href="#read">
        Read
       </a>
      </li>
      <li>
       4.4.
       <a href="#unread">
        Unread
       </a>
      </li>
     </ul>
    </li>
   </ul>
  </div>
  <h1 id="what-is-text-analytics">
   <a>
    1. What is text analytics?
   </a>
  </h1>
  <p>
   Per Sharda et al. (2014, pp. 205-206),
   <strong>
    text analytics
   </strong>
   aims "to turn unstructured textual data into actionable information through the application of [techniques from] natural language processing (NLP) and analytics [i.e., [data mining]" ---](https://jtkovacs.github.io/refs/data-mining.html#what-is-data-mining) the latter taking a 'bag of words' approach and the former taking a much more sophisticated approach rooted in linguistics.
  </p>
  <p>
   Text analytics includes the core activities of:
  </p>
  <ul>
   <li>
    <strong>
     Information retrieval:
    </strong>
    "searching and identifying relevant documents for a given set of key terms"; see
    <a href="https://jtkovacs.github.io/refs/search-engines.html">
     notes on search engines
    </a>
    and
    <a href="https://jtkovacs.github.io/refs/information-architecture.html#what-is-information-retrieval">
     IA for information retrieval
    </a>
   </li>
   <li>
    <a href="https://jtkovacs.github.io/refs/text-analytics.html#text-mining">
     Text mining
    </a>
    AKA text data mining, AKA knowledge discovery in textual databases: "primarily focused on discovering new and useful relationships from the textual data sources"
    <ul>
     <li>
      <strong>
       Information extraction:
      </strong>
      "identification of key phrases and relationships within text by looking for predefined objects and sequences by way of pattern matching"
     </li>
     <li>
      <strong>
       Web mining
      </strong>
      <ul>
       <li>
        Search engines (overlaps with "information retrieval")
       </li>
       <li>
        Web analytics
       </li>
       <li>
        Social media analytics
       </li>
      </ul>
     </li>
     <li>
      <a href="https://jtkovacs.github.io/refs/data-mining.html">
       Data mining
      </a>
     </li>
    </ul>
   </li>
  </ul>
  <p>
   Text analytics is enabled by the foundational disciplines of:
  </p>
  <ul>
   <li>
    Statistics
   </li>
   <li>
    Computer Science
    <ul>
     <li>
      Artificial Intelligence
     </li>
     <li>
      Machine Learning
     </li>
    </ul>
   </li>
   <li>
    Linguistics
    <ul>
     <li>
      Natural Language Processing
     </li>
    </ul>
   </li>
   <li>
    Management Science
   </li>
  </ul>
  <h2 id="business-applications-of-text-analytics">
   <a>
    1.1. Business applications of text analytics
   </a>
  </h2>
  <h3 id="applications-by-technique">
   <a>
    1.1.1. Applications by technique
   </a>
  </h3>
  <p>
   Per Sharda et al., some applications of text analytics (2014, pp. 206-207):
  </p>
  <ul>
   <li>
    <strong>
     "Topic tracking.
    </strong>
    Based on a user profile and documents that a user views, text mining can predict other documents of interest to the user.
   </li>
   <li>
    <strong>
     Categorization.
    </strong>
    Identifying the main themes of a document and them placing the document into a predefined set of categories based on those themes.
   </li>
   <li>
    <strong>
     Clustering.
    </strong>
    Grouping similar documents without having a predefined set of categories.
   </li>
   <li>
    <strong>
     Concept-linking.
    </strong>
    Connects related documents by identifying their shared concepts and, by doing so, helps users find information that they perhaps would not have found using traditional search methods."
   </li>
  </ul>
  <p>
   ... and some applications specifically enabled by NLP (pp. 213, 225; for a great example, see
   <a href="https://textio.com/">
    Textio, The Augmented Writing Platform):
   </a>
  </p>
  <ul>
   <li>
    <strong>
     "Question-answering.
    </strong>
    ... producing a human language answer when given a human language question. ...
   </li>
   <li>
    <strong>
     Automatic summarization.
    </strong>
    The creation of a shortened version of a textual document by a computer program that contains the most important parts of the original document.
   </li>
   <li>
    <strong>
     Natural languge generation.
    </strong>
    Systems convert information from computer databases into readable human language.
   </li>
   <li>
    <strong>
     Natural language understanding.
    </strong>
    Systems convert samples of human language into more formal representations that are easier for computer programs to manipulate.
   </li>
   <li>
    <strong>
     Machine translation.
    </strong>
    Automatic translation of one human language to another.
   </li>
   <li>
    <strong>
     Foreign language reading.
    </strong>
    A computer program that assists a nonnative language speaker to read [or write, or speak] a foreign language ...
   </li>
   <li>
    <strong>
     Speech recognition.
    </strong>
    ... Given a sound clip of a person speaking, the system produces a text dictation.
   </li>
   <li>
    <strong>
     Text-to-speech.
    </strong>
    Also called
    <em>
     speech synthesis,
    </em>
    a computer program automatically converts normal language text into human speech.
   </li>
   <li>
    <strong>
     Text proofing.
    </strong>
    A computer program reads a proof copy of a text in order to detect and correct errors.
   </li>
   <li>
    <strong>
     Optical character recognition.
    </strong>
    The automatic translation of images of handwritten, typewritten, or printed text (usually captured by a scanner) into machine-editable textual documents"
   </li>
  </ul>
  <p>
   ... and some applications specifically enabled by sentiment analysis, part of NLP (p. 233):
  </p>
  <ul>
   <li>
    <strong>
     Financial system,
    </strong>
    trying to predict based on buzz
   </li>
   <li>
    Understanding the 'voices' of employees (VOE), customers (VOC) and the market (VOM)
   </li>
   <li>
    Politics &amp; surveillance
   </li>
  </ul>
  <h3 id="applications-by-industry">
   <a>
    1.1.2. Applications by industry
   </a>
  </h3>
  <p>
   Per Sharda et al. (2014, pp. 213-220):
  </p>
  <ul>
   <li>
    <strong>
     Information management
    </strong>
    <ul>
     <li>
      quarterly reports
     </li>
     <li>
      manage search engines
     </li>
     <li>
      manage websites
     </li>
     <li>
      email
      <ul>
       <li>
        classify
       </li>
       <li>
        filter junk
       </li>
       <li>
        prioritize
       </li>
       <li>
        generate automatic responses
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li>
    <strong>
     Marketing
    </strong>
    <ul>
     <li>
      <em>
       Data sources
      </em>
      <ul>
       <li>
        call centers (notes and transcriptions)
       </li>
       <li>
        blogs
       </li>
       <li>
        user reviews
       </li>
       <li>
        discussion boards &amp; comment sections
       </li>
      </ul>
     </li>
     <li>
      <em>
       Information sought
      </em>
      <ul>
       <li>
        customer perceptions in the market at-large
       </li>
       <li>
        CRM system-based insights about churn, perceptions, purchasing behavior
       </li>
       <li>
        improve customer service performance by providing granular feedback on writing (e.g. email to customers)
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li>
    <strong>
     Legal
    </strong>
    <ul>
     <li>
      court orders
     </li>
     <li>
      patent files
     </li>
    </ul>
   </li>
   <li>
    <strong>
     Security
    </strong>
    <ul>
     <li>
      ECHELON, "assumed to be capable of identifying the content of telephone calls, faxes, emails, and other types of data, intercepting information sent via satellites, publics-switched telephone networks, and microwave links"
     </li>
     <li>
      FBI &amp; CIA joint database development
     </li>
     <li>
      <a href="#deception-detection">
       deception detection
      </a>
     </li>
    </ul>
   </li>
   <li>
    <strong>
     Academic &amp; biomedical
    </strong>
    <ul>
     <li>
      citation analysis
     </li>
     <li>
      research articles
     </li>
     <li>
      medical records
     </li>
     <li>
      molecular interactions
     </li>
    </ul>
   </li>
  </ul>
  <h4 id="deception-detection">
   <a>
    1.1.2.1. Deception detection
   </a>
  </h4>
  <p>
   Per Sharda et al. (2014, p. 216):
  </p>
  <p>
   "Applying text mining to a large set of real-world criminal (person-of-interest) statements, Fuller et al. (2008) developed prediction models to differentiate deceptive statements from truthful ones. Using a rich set of cues extracted from the textual statements, the model predicted the holdout samples with 70 percent accuracy, which is believed to be a significant success considering that the cues are extracted only from textual statementss (no verbal or visual cues are present). Furthermore, compared to other deception-detection techniques, such as polygraph, this method is nonintrusive and widely applicable to not only textual data, but also (potentially) to transcriptions of voice recordings."
  </p>
  <table>
   <thead>
    <tr class="header">
     <th align="left">
      Construct
     </th>
     <th align="left">
      Example Cues
     </th>
    </tr>
   </thead>
   <tbody>
    <tr class="odd">
     <td align="left">
      Quantity
     </td>
     <td align="left">
      Verb count, noun-phrase count
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      Complexity
     </td>
     <td align="left">
      Average number of clauses, average sentence length
     </td>
    </tr>
    <tr class="odd">
     <td align="left">
      Uncertainty
     </td>
     <td align="left">
      Modifiers, modal verbs
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      Nonimmediacy
     </td>
     <td align="left">
      Passive voice, objectification
     </td>
    </tr>
    <tr class="odd">
     <td align="left">
      Expressivity
     </td>
     <td align="left">
      Emotiveness
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      Diversity
     </td>
     <td align="left">
      Lexical diversity, redundancy
     </td>
    </tr>
    <tr class="odd">
     <td align="left">
      Informality
     </td>
     <td align="left">
      Typographical error ratio
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      Specificity
     </td>
     <td align="left">
      Spatiotemporal information, perceptual information
     </td>
    </tr>
    <tr class="odd">
     <td align="left">
      Affect
     </td>
     <td align="left">
      Positive affect, negative affect
     </td>
    </tr>
   </tbody>
  </table>
  <h1 id="text-analytics-techniques">
   <a>
    2. Text analytics techniques
   </a>
  </h1>
  <h2 id="text-mining">
   <a>
    2.1. Text mining
   </a>
  </h2>
  <p>
   Per Sharda et al., text mining can be undertaken through the three-step process elaborated below (2014; I follow and mostly quote from pp. 220-226, but some term definitions are taken from pp. 206-207).
  </p>
  <p>
   Delen and Crossland (2008, cited in Sharda et al., 2014) place the 'black box' of this data mining process into the following context, which they represent graphically:
  </p>
  <ul>
   <li>
    <strong>
     Input
    </strong>
    <ul>
     <li>
      structured data
     </li>
     <li>
      unstructured data
     </li>
    </ul>
   </li>
   <li>
    <strong>
     Constraints
    </strong>
    <ul>
     <li>
      software/hardware limitations
     </li>
     <li>
      privacy issues
     </li>
     <li>
      linguistic limitations
     </li>
    </ul>
   </li>
   <li>
    <strong>
     Mechanisms
    </strong>
    <ul>
     <li>
      domain expertise
     </li>
     <li>
      tools &amp; techniques
     </li>
    </ul>
   </li>
   <li>
    <strong>
     Output
    </strong>
    <ul>
     <li>
      context-specific knowledge
     </li>
    </ul>
   </li>
  </ul>
  <h3 id="establish-the-corpus">
   <a>
    2.1.1. Establish the corpus
   </a>
  </h3>
  <p>
   ("large and structured set of texts ... prepared for the purpose of conducting knowledge discovery")
  </p>
  <ul>
   <li>
    <strong>
     "Collect
    </strong>
    all documents related to the context (domain of interest) being studied", which may include:
    <ul>
     <li>
      XML files
     </li>
     <li>
      emails
     </li>
     <li>
      web pages
     </li>
     <li>
      notes
     </li>
     <li>
      memos
     </li>
     <li>
      transcriptions of audio
     </li>
    </ul>
   </li>
   <li>
    <strong>
     Organize
    </strong>
    (often into a flat text file with consistent character encoding)
   </li>
  </ul>
  <h3 id="create-term-by-document-matrix">
   <a>
    2.1.2. Create term-by-document matrix
   </a>
  </h3>
  <ul>
   <li>
    <strong>
     Count raw frequencies
    </strong>
    in each document:
    <ul>
     <li>
      <strong>
       Tokenize
      </strong>
      raw input (a token is "a categorized block of text in a sentence ... assignment of meaning to blocks of text is known as tokenizing")
     </li>
     <li>
      <strong>
       Filter out
      </strong>
      stop words
      <strong>
       OR filter in
      </strong>
      include terms
      <ul>
       <li>
        <strong>
         stop words
        </strong>
        "(or noise words) ... are filtered out prior to or after processing of natural language data ... there is no universally accepted list of stop words, [but] most natural language processing tools use a list that includes articles
        <em>
         (a, an, the, of, etc.),
        </em>
        auxiliary verbs
        <em>
         (is, are, was, were, etc.),
        </em>
        and context-specific words that are deemed not to have any differentiating value"
       </li>
       <li>
        <strong>
         include terms
        </strong>
        AKA term dictionary
       </li>
      </ul>
     </li>
     <li>
      <strong>
       Reckon with linguistic ambiguities
      </strong>
      e.g. typos, synonyms, etc.
     </li>
     <li>
      <strong>
       Perform stemming
      </strong>
      to "[reduce] inflected words to their stem (or base or root) form"
     </li>
    </ul>
   </li>
   <li>
    <strong>
     Normalize frequencies
    </strong>
    (e.g., to account for different document lengths or to assign different weights to different documents; can use log frequencies, binary frequencies, inverse document frequencies, etc.; "text mining research and practice have clearly indicated that the best weighting may come from the use of
    <em>
     term-frequency
    </em>
    divided by
    <em>
     inverse-document-frequency
    </em>
    ... "; p. 245)
   </li>
   <li>
    <strong>
     Construct the term-by-document-matrix
    </strong>
    AKA occurrence matrix (example below) --- a "common representation schema of the frequency-based relationship between the terms and documents in a tabular format where terms are listed in rows, documents are listed in columns, and the frequency between the terms and documents is listed in cells as integer values"
    <ul>
     <li>
      <strong>
       Latent semantic indexing
      </strong>
      by single-value decomposition (SVD) "dimensionality reduction method to transform the term-by-document matrix to a manageable size by generating an intermediate representation of the frequencies using a matrix manipulation method similar to principal component analysis"; through SVD, "the analyst might identify the two or three most salient dimensions that account for most of the variability (differences) between the words and documents, thus identifying the latent semantic space that organizes the words and documents in the analysis. Once such dimensions are identified, the underlying 'meaning' of what is contained (discussed or described) in the documents has been extracted."
     </li>
    </ul>
   </li>
  </ul>
  <table>
   <thead>
    <tr class="header">
     <th align="left">
     </th>
     <th align="left">
      Term1
     </th>
     <th align="left">
      Term2
     </th>
     <th align="left">
      Term3
     </th>
     <th align="left">
      Term4
     </th>
     <th align="left">
      Term4
     </th>
     <th align="left">
      Term5
     </th>
    </tr>
   </thead>
   <tbody>
    <tr class="odd">
     <td align="left">
      Doc1
     </td>
     <td align="left">
      1
     </td>
     <td align="left">
     </td>
     <td align="left">
     </td>
     <td align="left">
      1
     </td>
     <td align="left">
     </td>
     <td align="left">
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      Doc2
     </td>
     <td align="left">
     </td>
     <td align="left">
      1
     </td>
     <td align="left">
     </td>
     <td align="left">
     </td>
     <td align="left">
     </td>
     <td align="left">
     </td>
    </tr>
    <tr class="odd">
     <td align="left">
      Doc3
     </td>
     <td align="left">
     </td>
     <td align="left">
     </td>
     <td align="left">
      3
     </td>
     <td align="left">
     </td>
     <td align="left">
      1
     </td>
     <td align="left">
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      Doc4
     </td>
     <td align="left">
     </td>
     <td align="left">
      2
     </td>
     <td align="left">
      1
     </td>
     <td align="left">
     </td>
     <td align="left">
     </td>
     <td align="left">
      1
     </td>
    </tr>
   </tbody>
  </table>
  <h3 id="analyze">
   <a>
    2.1.3. Analyze
   </a>
  </h3>
  <p>
   See
   <a href="https://jtkovacs.github.io/refs/data-mining.html">
    notes on data mining.
   </a>
  </p>
  <h4 id="clustering">
   <a>
    2.1.3.1. Clustering
   </a>
  </h4>
  <p>
   Per Sharda et al. (2014, pp. 224-225), clustering is often used to
   <strong>
    improve search recall
   </strong>
   ("when a query matches a document its whole cluster is returned") and
   <strong>
    precision
   </strong>
   ("grouping the documents into a number of much smaller groups of related documents, ordering them by relevance, and returning only the documents from the most relevant group or groups"). The most common clustering methods:
  </p>
  <ul>
   <li>
    <strong>
     Scatter/gather
    </strong>
    "dynamically generates a table of contents for the collection and adapts and modifies it in response to the user selection"
   </li>
   <li>
    <strong>
     Query-specific clustering
    </strong>
    "a hierarchical clustering approach where the most relevant documents to the posed query appear in small tight clusters that are nested in larger clusters"
   </li>
  </ul>
  <h4 id="association">
   <a>
    2.1.3.2. Association
   </a>
  </h4>
  <p>
   Sharda et al. (2014, pp. 225): "In text mining, associations specifically refer to the direct relationships between concepts (terms) or set of concepts ... [For
   <em>
    A
   </em>
   ==&gt; _C],_ confidence is the percentage of documents that include all the concepts in
   <em>
    C
   </em>
   within the same subset of those documents that include all the concepts in
   <em>
    A.
   </em>
   Support is the percentage (or number) of documents that include all the concepts in
   <em>
    A
   </em>
   and
   <em>
    C."
   </em>
  </p>
  <h4 id="classification">
   <a>
    2.1.3.3. Classification
   </a>
  </h4>
  <p>
   (AKA automatic text categorization, a form of prediction)
  </p>
  <p>
   Per Sharda et al. (2014, pp. 224), some applications:
  </p>
  <ul>
   <li>
    indexing text (semi/automatic)
   </li>
   <li>
    filtering spam
   </li>
   <li>
    cataloging web pages
   </li>
   <li>
    generating metadata
   </li>
   <li>
    genre detection
   </li>
  </ul>
  <h4 id="trend-analysis">
   <a>
    2.1.3.4. Trend analysis
   </a>
  </h4>
  <p>
   Comparing the distribution of concepts across different subcollections, e.g. from the same source but at different points in time.
  </p>
  <h2 id="natural-language-processing">
   <a>
    2.2. Natural language processing
   </a>
  </h2>
  <p>
   With its two parent disciplines---artificial intelligence and computational linguistics---NLP extracts more meaning from textual data because it goes beyond the 'bag of words' approach to account for syntax, and, beyond that, "grammatical and semantic constraints as well as the context" (Sharda et al., 2014, p. 210).
  </p>
  <h3 id="challenges-with-nlp">
   <a>
    2.2.1. Challenges with NLP
   </a>
  </h3>
  <p>
   Per Sharda et al. (2014, p. 210), NLP faces major challenges:
  </p>
  <ul>
   <li>
    <strong>
     part-of-speech tagging
    </strong>
   </li>
   <li>
    <strong>
     text segmentation
    </strong>
    (identifying word boundaries in spoken language as well as written Chinese, Japanese, Thai, etc.)
   </li>
   <li>
    <strong>
     word sense disambiguation
    </strong>
    (see
    <a href="https://jtkovacs.github.io/refs/information-architecture.html#what-are-controlled-vocabularies">
     notes on controlled vocabularies)
    </a>
   </li>
   <li>
    <strong>
     syntatic ambiguity
    </strong>
    ("multiple possible sentence structures often need to be considered")
   </li>
   <li>
    <strong>
     irregular input
    </strong>
    (e.g. typos, accents)
   </li>
   <li>
    <strong>
     identifying speech acts,
    </strong>
    speech that is meant to provoke an action
   </li>
  </ul>
  <h3 id="sentiment-analysis">
   <a>
    2.2.2. Sentiment analysis
   </a>
  </h3>
  <p>
   "Often we want to categorize text by topic, which may involve dealing with whole taxonomies of topics. Sentiment classification, on the other hand, usually deals with two classes (positive versus negative), a range of polarity (e.g., star ratings for movies), or even a range in strength of opinion" (Sharda et al., 2014, p. 229).
  </p>
  <h4 id="generic-sentiment-analysis-process">
   <a>
    2.2.2.1. Generic sentiment analysis process
   </a>
  </h4>
  <p>
   Per Sharda et al. (2014, pp. 234-237):
  </p>
  <ul>
   <li>
    <strong>
     Sentiment detection:
    </strong>
    determine whether a given passage is 'sentimentful', perhaps by calculating its Objectivity-Subjectivity (O-S) polarity
   </li>
   <li>
    <strong>
     N-P polarity classification:
    </strong>
    "classify the opinion as falling under one of two opposing sentiment polarities, or locate its position on the continuum between these two polarities"
   </li>
   <li>
    <strong>
     Target identification:
    </strong>
    identify what---explicit or implicit in the sentence (or other unit of analysis)---the expressed sentiment is directed towards (its target)
    <ul>
     <li>
      the challenge posed by this step varies greatly by domain
     </li>
     <li>
      can be multiple valid or invalid targets in a sentence
     </li>
    </ul>
   </li>
   <li>
    <strong>
     Collection and aggregation:
    </strong>
    polarity is calculated at the word level, which can then be aggregated to the sentence/phrase and document levels through simple summing; weighted averaging; or "as complex as using one or more machine-learning techniques to create a predictive relationship between the words (and their polarity values) and phrases or sentences"
   </li>
  </ul>
  <h4 id="challenges-with-sentiment-identification">
   <a>
    2.2.2.2. Challenges with sentiment identification
   </a>
  </h4>
  <p>
   Per Sharda et al. (2014):
  </p>
  <ul>
   <li>
    Sentiments can be explicit or implicit, "where the text implies an opinion"; the latter is much more difficult to detect
   </li>
   <li>
    "A document containing several opinionated statements would have a mixed polarity overall, which is different from not having a polarity at all"
   </li>
   <li>
    "an article may contain negative news without explictly using any subjective words or terms"
   </li>
  </ul>
  <h4 id="methods-for-sentiment-identification">
   <a>
    2.2.2.3. Methods for sentiment identification
   </a>
  </h4>
  <p>
   Per Sharda et al. (2014, pp. 236-237):
  </p>
  <ul>
   <li>
    <strong>
     Lexicon
    </strong>
    <ul>
     <li>
      <a href="http://sentiwordnet.isti.cnr.it/">
       SentiWordNet
      </a>
     </li>
     <li>
      <a href="http://wndomains.fbk.eu/wnaffect.html">
       WordNet-Affect
      </a>
     </li>
    </ul>
   </li>
   <li>
    <strong>
     Training documents
    </strong>
    <ul>
     <li>
      <strong>
       <em>
        data
       </em>
      </strong>
      "Product-review Web sites like Amazon, C-NET, eBay, RottenTomatoes, and the Internet Movie Database (IMDB) have all been extensively used as sources of annotated data. The star (or tomato, as it were) system provides an explicit label of the overall polarity of the review, and it is often taken as the gold standard in algorithm evaluation"
     </li>
     <li>
      <strong>
       <em>
        algorithms
       </em>
      </strong>
      artificial neural networks, support vector machines, k-nearest neighbor, naive Bayes, decision trees, expectation maximization-based clustering
     </li>
    </ul>
   </li>
  </ul>
  <h2 id="web-mining">
   <a>
    2.3. Web mining
   </a>
  </h2>
  <p>
   Per Sharda et al. (2014, pp. 240-241), web mining, AKA web data mining, "is essentially the same as data mining that uses data generated over the web". They contrast two common terms, noting that Web analytics has a narrower meaning but is replacing its parent term in popular discussion:
  </p>
  <table>
   <thead>
    <tr class="header">
     <th align="left">
      Web mining
     </th>
     <th align="left">
      Web analytics
     </th>
    </tr>
   </thead>
   <tbody>
    <tr class="odd">
     <td align="left">
      "all data generated via the Internet, including transaction, social, and usage data"
     </td>
     <td align="left">
      "Web site usage data"
     </td>
    </tr>
    <tr class="even">
     <td align="left">
      "discover previously unknown patterns and relationships"
     </td>
     <td align="left">
      "describe what happened on a website"
     </td>
    </tr>
    <tr class="odd">
     <td align="left">
      "predictive or prescriptive analytics methodology"
     </td>
     <td align="left">
      "predefined, metrics-driven descriptive analysis"
     </td>
    </tr>
   </tbody>
  </table>
  <h3 id="challenges-with-web-mining">
   <a>
    2.3.1. Challenges with web mining
   </a>
  </h3>
  <p>
   Per Sharda et al. (2014, p. 239) --- the Web is:
  </p>
  <ul>
   <li>
    Big, growing, and constantly updated
   </li>
   <li>
    Complex, e.g. authoring style, content variation, lack of unified structure, not specific to a domain
   </li>
  </ul>
  <h3 id="web-crawlers-structure-content-mining">
   <a>
    2.3.2. Web crawlers (structure &amp; content mining)
   </a>
  </h3>
  <p>
   Web content and metadata can be scraped and mined by web crawlers, to:
  </p>
  <ul>
   <li>
    reveal the
    <strong>
     structure
    </strong>
    of the Web, for example identifying
    <strong>
     authoritative pages
    </strong>
    and
    <strong>
     hubs
    </strong>
    on the basis of hyperlinks
   </li>
   <li>
    build a corpus of
    <strong>
     content
    </strong>
    for knowledge discovery through text mining.
   </li>
  </ul>
  <p>
   See
   <a href="https://jtkovacs.github.io/refs/search-engines.html">
    notes on search engines
   </a>
   for a discussion of how web crawlers are used in that application.
  </p>
  <h3 id="web-analytics-usage-mining">
   <a>
    2.3.3. Web analytics (usage mining)
   </a>
  </h3>
  <h4 id="metrics-for-web-analytics">
   <a>
    2.3.3.1. Metrics for web analytics
   </a>
  </h4>
  <ul>
   <li>
    visitor profiles
   </li>
   <li>
    traffic
   </li>
   <li>
    usability
   </li>
   <li>
    conversion
   </li>
  </ul>
  <h4 id="technologies-for-web-analytics">
   <a>
    2.3.3.2. Technologies for web analytics
   </a>
  </h4>
  <h3 id="social-analytics">
   <a>
    2.3.4. Social analytics
   </a>
  </h3>
  <h4 id="social-network-analysis">
   <a>
    2.3.4.1. Social network analysis
   </a>
  </h4>
  <ul>
   <li>
    types of networks
   </li>
   <li>
    network metrics
   </li>
   <li>
    connections
   </li>
   <li>
    distributions
   </li>
   <li>
    segmentation
   </li>
  </ul>
  <h4 id="social-media-analytics">
   <a>
    2.3.4.2. Social media analytics
   </a>
  </h4>
  <ul>
   <li>
    social media vs traditional media
   </li>
   <li>
    types of social media users
   </li>
   <li>
    measuring social media impact
   </li>
  </ul>
  <h5 id="tools-for-social-media-analytics">
   <a>
    2.3.4.2.1. Tools for social media analytics
   </a>
  </h5>
  <ul>
   <li>
    <a href="https://blog.bufferapp.com/social-media-analytics-tools">
     https://blog.bufferapp.com/social-media-analytics-tools
    </a>
   </li>
   <li>
    <a href="http://venturebeat.com/2013/12/20/top-10-social-media-analytics-tools-the-venturebeat-index/">
     http://venturebeat.com/2013/12/20/top-10-social-media-analytics-tools-the-venturebeat-index/
    </a>
   </li>
   <li>
    <a href="http://www.entrepreneur.com/article/239029">
     http://www.entrepreneur.com/article/239029
    </a>
   </li>
   <li>
    <a href="http://www.wordstream.com/home-a-may2015">
     http://www.wordstream.com/home-a-may2015
    </a>
   </li>
   <li>
    <a href="https://keen.io/">
     https://keen.io/
    </a>
   </li>
   <li>
    kissmetrics
   </li>
  </ul>
  <h1 id="text-analytics-tools">
   <a>
    3. Text analytics tools
   </a>
  </h1>
  <h2 id="ibm-watson">
   <a>
    3.1. IBM Watson
   </a>
  </h2>
  <p>
   IBM Watson's DeepQA is a "massively parallel, text mining-focused, probabilistic evidence-based computational architecture ... [using] more than 100 different techniques for analyzing natural language, identifying sources, finding and generating hypotheses, finding and scoring evidence, and merging and ranking hypotheses" (Sharda et al., 2014, pp. 203-204):
  </p>
  <p>
   <img src="illos/DeepQA.png" width="600"/>
  </p>
  <h2 id="attensity">
   <a>
    3.2. Attensity
   </a>
  </h2>
  <p>
   <a href="https://en.wikipedia.org/wiki/Attensity">
    https://en.wikipedia.org/wiki/Attensity
   </a>
  </p>
  <h2 id="python">
   <a>
    3.3. Python
   </a>
  </h2>
  <pre class="sourceCode Python"><code class="sourceCode python"><span class="co"># reverse order of elements:</span>
<span class="dt">list</span>.reverse(), my_string[::-<span class="dv">1</span>]
<span class="co"># selectively replace:</span>
str_name.replace(‘this’,’with this’)
<span class="co"># find index of known element:</span>
<span class="dt">list</span>.index(‘str name’)
<span class="co"># times element occurs:</span>
<span class="dt">list</span>.count(‘em_name’) makes <span class="dt">tuple</span> <span class="kw">with</span> (index,value): <span class="dt">enumerate</span>(my_list)</code></pre>
  <h3 id="string-manipulation">
   <a>
    3.3.1. String manipulation
   </a>
  </h3>
  <pre><code># remove punctuation
import string
line.translate(None, string.punctuation)

# modify case
my_string.lower()
my_string.upper()
my_string.capitalize()
my_string.title()

# remove whitespace by default, or remove specified characters
my_string.strip('chars')
my_string.lstrip()
my_string.rstrip()</code></pre>
  <h3 id="regex">
   <a>
    3.3.2. Regex
   </a>
  </h3>
  <pre><code># search for substrings within string or subset of string (i inclusive to j exclusive)
str_index = my_string.find(x,i,j)
str_index = my_string.index(x,i,j)  # raises ValueError if not found
str.endswith(x,i,j)
str.startswith(x,i,j)
my_string.count(x,i,j)</code></pre>
  <ul>
   <li>
    https://docs.python.org/3/library/re.html
   </li>
   <li>
    https://docs.python.org/3/howto/regex.html
   </li>
   <li>
    http://nbviewer.jupyter.org/github/ptwobrussell/Mining-the-Social-Web-2nd-Edition/tree/master/ipynb/
   </li>
  </ul>
  <pre><code># match the beginning of a string:
re.match(pattern, text, flags)
re.match(r’Jac’, data) # the r denotes a raw string

# search anywhere in a string:
# first match only:
re.search(pattern, text, flags)
# all nonoverlapping:
re.findall(pattern, text, flags)

# phone number, note escaped parentheses:
re.search(r’\(\d\d\d\) \d\d\d-\d\d\d\d’, data)
# make parentheses, space, hyphen optional in phone number
r’\)?\d{3})?\s?-?\d\{3}-\d{4}’</code></pre>
  <p>
   flags:
  </p>
  <ul>
   <li>
    re.IGNORECASE or re.I will ignore word case
   </li>
   <li>
    re.VERBOSE or re.X let regexp span lines &amp; contain (ignored) whitespace or comments
   </li>
   <li>
    re.MULTILINE or re.M to make a pattern regard lines in your text as the beginning or end of a string
   </li>
   <li>
    multiple flags: re.findall(pattern, data, flag|flag|flag)
   </li>
  </ul>
  <pre><code># store regex for reuse:
my_regex = re.compile(pattern, flags)
re.search(my_regex, data)
# OR
my_regex.search(data)

# loop to obtain iterable of match objects:
for match in my_regex.finditer(data):
    print(‘{first} {last} &lt;{email}&gt;’.format(**match.groupdict()))</code></pre>
  <ul>
   <li>
    \w = any Unicode word character, \W = anything not a Unicode word character
   </li>
   <li>
    \s = any whitespace, \S = anything not whitespace, = tab
   </li>
   <li>
    \d = any number 0-9, \D = any non-number
   </li>
   <li>
    \b = word boundaries, \B = not word boundaries
   </li>
  </ul>
  <p>
   counts, for when something occurs multiple times:
  </p>
  <ul>
   <li>
    {3} = occurs 3 times, {,3} = 0-3 times, {3,} = 3 or more times, {3-5} = 3-5 times
   </li>
   <li>
    \w? = 0-1 word characters, \w* = 0-infinite word characters, \w+ = 1-infinite word characters
   </li>
  </ul>
  <p>
   sets let us combine explicit characters and escape patterns into pieces that can be repeated multiple time; they also let us specify pieces that should be left out of any matches: [aple] finds apple and pale, [a-z] finds any lowercase letter, [A-Z] finds uppercase, [a-zA-Z] finds any case, [^2] finds anything not two, [0-9] finds any number, [.]+ finds any # of , .
  </p>
  <pre><code># groups search for multiple conditions simultaneously; note that ^ marks the beginning of the string, and $ marks the end; unnamed groups returned as tuples, named groups as dicts:
my_var = re.findall (r’’’
    ^(?P&lt;name&gt;[-\w ]+,\s[-\w ]+)\t   # search for lastname, firstname
    (\)?\d{3})?\s?-?\d\{3}-\d{4})? # search for phone number, optional
    (?&lt;email&gt;[-\w\d.+]+ @[-\w\d.]+)\t$  # search for emails
    ‘’’, data, flags)

# groups addressing
my_var.groups()
my_var.group_dict()
my_var.group(‘group_name’)
my_var.group(1)</code></pre>
  <h1 id="sources">
   <a>
    4. Sources
   </a>
  </h1>
  <h2 id="cited">
   <a>
    4.1. Cited
   </a>
  </h2>
  <p>
   Sharda, R., Delen, D., &amp; Turban, E. (2014).
   <em>
    Business intelligence: A managerial perspective on analytics
   </em>
   (3rd ed.). New York City, NY: Pearson.
  </p>
  <h2 id="references">
   <a>
    4.2. References
   </a>
  </h2>
  <ul>
   <li>
    <a href="https://wordnet.princeton.edu/">
     WordNet:
    </a>
    <em>
     WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. The resulting network of meaningfully related words and concepts can be navigated with the browser. WordNet is also freely and publicly available for download. WordNet's structure makes it a useful tool for computational linguistics and natural language processing.
    </em>
    <ul>
     <li>
      <a href="http://sentiwordnet.isti.cnr.it/">
       SentiWordNet
      </a>
     </li>
     <li>
      <a href="http://wndomains.fbk.eu/wnaffect.html">
       WordNet-Affect
      </a>
     </li>
    </ul>
   </li>
   <li>
    <a href="https://stanfordnlp.github.io/CoreNLP/">
     Stanford NLP Lab software
    </a>
   </li>
   <li>
    <a href="http://billchambers.me/tutorials/2015/01/14/python-nlp-cheatsheet-nltk-scikit-learn.html">
     NLTK cheatsheet
    </a>
   </li>
   <li>
    <a href="http://corpus.byu.edu/coca/">
     Corpus of Contemporary American English
    </a>
   </li>
   <li>
    <a href="http://cw.routledge.com/textbooks/0415286239/default.asp">
     Corpus based language studies
    </a>
   </li>
   <li>
    <a href="https://personality-insights-livedemo.mybluemix.net/">
     IBM Watson demo - Infer personality from unstructured text
    </a>
   </li>
  </ul>
  <h2 id="read">
   <a>
    4.3. Read
   </a>
  </h2>
  <ul>
   <li>
    <a href="http://www.lynda.com/Regular-Expressions-tutorials/Using-Regular-Expressions/85870-2.html">
     Lynda - Using Regex
    </a>
   </li>
  </ul>
  <h2>
   <a name="4.4.-unread">
    4.4. Unread
   </a>
  </h2>
  <ul>
   <li>
    Regex:
    <a href="http://www.regular-expressions.info/">
     1
    </a>
    ,
    <a href="https://regexone.com/">
     2
    </a>
   </li>
   <li>
    <a href="https://www.codeschool.com/courses/breaking-the-ice-with-regular-expressions">
     CodeSchool - Regular Expressions
    </a>
   </li>
   <li>
    <a href="http://www.datasciencecentral.com/profiles/blogs/5-easy-steps-to-structure-highly-unstructured-big-data">
     4 steps to structure highly unstructured big data via automated indexation
    </a>
   </li>
   <li>
    <a href="https://www.coursera.org/learn/nlp">
     Coursera - Natural Language Processing
    </a>
   </li>
   <li>
    <a href="http://bigdatauniversity.com/courses/text-analytics-essentials/">
     Big Data U - Text Analytics
    </a>
   </li>
   <li>
    <a href="http://nlpers.blogspot.co.uk/2016/06/language-bias-and-black-sheep.html">
     Language bias &amp; black sheep
    </a>
   </li>
   <li>
    <a href="http://www.nltk.org/book/">
     Analyzing Text with NLTK
    </a>
   </li>
   <li>
    <a href="http://www.panix.com/~elflord/unix/grep.html">
     Donovan Rebbechi's grep tutorial
    </a>
   </li>
   <li>
    <a href="http://www.uccs.edu/~ahitchco/grep/">
     Drew's grep tutorial
    </a>
   </li>
  </ul>
 </body>
</html>
